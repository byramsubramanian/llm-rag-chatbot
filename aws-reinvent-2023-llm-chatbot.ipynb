{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107219"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://aws.amazon.com/blogs/aws/top-announcements-of-aws-reinvent-2023/\"\n",
    "page = requests.get(url)\n",
    "\n",
    "len(page.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introducing Amazon Q Generative SQL in Amazon Redshift (Preview)\n",
      "Introducing new AI-driven scaling and optimizations in Amazon Redshift Serverless (Preview)\n",
      "Announcing Amazon Aurora PostgreSQL zero-ETL integration with Amazon Redshift (Preview)\n",
      "Announcing Amazon RDS for MySQL zero-ETL integration with Amazon Redshift (Preview)\n",
      "Announcing Amazon DynamoDB zero-ETL integration with Amazon Redshift (Preview)\n",
      "Introducing highly durable Amazon OpenSearch Service clusters with 30% price/performance improvement\n",
      "AWS Clean Rooms Differential Privacy enhances privacy protection of your users’ data (preview)\n",
      "AWS Clean Rooms ML helps customers and partners apply ML models without sharing raw data (preview)\n",
      "Announcing Amazon OpenSearch Service zero-ETL integration with Amazon S3 (preview)\n",
      "New generative AI capabilities for Amazon DataZone further simplify data cataloging and discovery (preview)\n",
      "Analyze large amounts of graph data to get insights and find trends with Amazon Neptune Analytics\n",
      "New Amazon Q in QuickSight uses generative AI assistance for quicker, easier data insights (preview)\n",
      "Use anomaly detection with AWS Glue to improve data quality (preview)\n",
      "Amazon CloudWatch Logs now offers automated pattern analytics and anomaly detection\n",
      "AWS Step Functions Workflow Studio is now available in AWS Application Composer\n",
      "Announcing throughput increase and dead letter queue redrive support for Amazon SQS FIFO queues\n",
      "Manage EDI at scale with new AWS B2B Data Interchange\n",
      "Easily deploy SaaS products with new Quick Launch in AWS Marketplace\n",
      "Check your AWS Free Tier usage programmatically with a new API\n",
      "New Cost Optimization Hub centralizes recommended actions to save you money\n",
      "Join the preview for new memory-optimized, AWS Graviton4-powered Amazon EC2 instances (R8g)\n",
      "Introducing Amazon EC2 high memory U7i Instances for large in-memory databases (preview)\n",
      "Amazon Managed Service for Prometheus collector provides agentless metric collection for Amazon EKS\n",
      "Amazon EKS Pod Identity simplifies IAM permissions for applications on Amazon EKS clusters\n",
      "Increase collaboration and securely share cloud knowledge with AWS re:Post Private\n",
      "Amazon Redshift adds new AI capabilities, including Amazon Q, to boost efficiency and productivity\n",
      "Vector search for Amazon DocumentDB (with MongoDB compatibility) is now generally available\n",
      "Amazon DynamoDB zero-ETL integration with Amazon OpenSearch Service is now available\n",
      "Amazon ElastiCache Serverless for Redis and Memcached is now available\n",
      "Join the preview of Amazon Aurora Limitless Database\n",
      "Getting started with new Amazon RDS for Db2\n",
      "Use AWS Fault Injection Service to demonstrate multi-region and multi-AZ application resilience\n",
      "IDE extension for AWS Application Composer enhances visual modern applications development with AI-generated IaC\n",
      "Upgrade your Java applications with Amazon Q Code Transformation (preview)\n",
      "Improve developer productivity with generative-AI powered Amazon Q in Amazon CodeCatalyst (preview)\n",
      "New Amazon WorkSpaces Thin Client provides cost-effective, secure access to virtual desktops\n",
      "Announcing cross-region data replication for Amazon WorkSpaces\n",
      "Amazon SageMaker Studio adds web-based interface, Code Editor, flexible workspaces, and streamlines user onboarding\n",
      "Package and deploy models faster with new tools and guided workflows in Amazon SageMaker\n",
      "Use natural language to explore and prepare data with a new capability of Amazon SageMaker Canvas\n",
      "Amazon SageMaker adds new inference capabilities to help reduce foundation model deployment costs and latency\n",
      "Leverage foundation models for business analysis at scale with Amazon SageMaker Canvas\n",
      "Amazon SageMaker Clarify makes it easier to evaluate and select foundation models (preview)\n",
      "Evaluate, compare, and select the best foundation models for your use case in Amazon Bedrock (preview)\n",
      "Introducing Amazon SageMaker HyperPod, a purpose-built infrastructure for distributed training at scale\n",
      "Amazon Titan Image Generator, Multimodal Embeddings, and Text models are now available in Amazon Bedrock\n",
      "Amazon Bedrock now provides access to Anthropic’s latest model, Claude 2.1\n",
      "Introducing Amazon Q, a new generative AI-powered assistant (preview)\n",
      "Amazon Q brings generative AI-powered assistance to IT pros and developers (preview)\n",
      "Guardrails for Amazon Bedrock helps implement safeguards customized to your use cases and responsible AI policies (preview)\n",
      "Agents for Amazon Bedrock is now available with improved control of orchestration and visibility into reasoning\n",
      "Customize models in Amazon Bedrock with your own data using fine-tuning and continued pre-training\n",
      "Knowledge Bases now delivers fully managed RAG experience in Amazon Bedrock\n",
      "Amazon Transcribe Call Analytics adds new generative AI-powered call summaries (preview)\n",
      "Build generative AI apps using AWS Step Functions and Amazon Bedrock\n",
      "Amazon CodeWhisperer offers new AI-powered code remediation, IaC support, and integration with Visual Studio\n",
      "Amazon CloudWatch Application Signals for automatic instrumentation of your applications (preview)\n",
      "New myApplications in the AWS Management Console simplifies managing your application resources\n",
      "Use Amazon CloudWatch to consolidate hybrid, multicloud, and on-premises metrics\n",
      "Use natural language to query Amazon CloudWatch logs and metrics (preview)\n",
      "New Amazon CloudWatch log class for infrequent access logs at a reduced price\n",
      "Zonal autoshift automatically shifts your traffic away from Availability Zones when we detect potential issues\n",
      "Mutual authentication for Application Load Balancer reliably verifies certificate-based client identities\n",
      "External endpoints and testing of task states now available in AWS Step Functions\n",
      "Announcing new diagnostic tools for AWS Partner-Led Support (PLS) participants\n",
      "Three new capabilities for Amazon Inspector broaden the realm of vulnerability scanning for workloads\n",
      "AWS Control Tower adds 65 new controls\n",
      "Amazon Detective adds new capabilities to accelerate and improve your cloud security investigations\n",
      "Detect runtime security threats in Amazon ECS and AWS Fargate, new in Amazon GuardDuty\n",
      "Vector engine for Amazon OpenSearch Serverless is now available\n",
      "AWS Lambda functions now scale 12 times faster when handling high-volume requests\n",
      "Announcing the new Amazon S3 Express One Zone high performance storage class\n",
      "Amazon EBS Snapshots Archive is now available with AWS Backup\n",
      "Replication failback and increased IOPS are new for Amazon EFS\n",
      "Automatic restore testing and validation now available in AWS Backup\n",
      "Optimize your storage costs for rarely-accessed files with Amazon EFS Archive\n",
      "Announcing on-demand data replication for Amazon FSx for OpenZFS\n",
      "Introducing shared VPC support for Amazon FSx for NetApp ONTAP\n",
      "New – Scale-out file systems for Amazon FSx for NetApp ONTAP\n",
      "FlexGroup Volume Management for Amazon FSx for NetApp ONTAP is now available\n",
      "Reserve quantum computers, get guidance and cutting-edge capabilities with Amazon Braket Direct\n",
      "No of announcements: 81\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "announcements = []\n",
    "results = soup.find_all(\"p\")\n",
    "for result in results:  # iterate through each announcement\n",
    "    a = result.find(\"a\", href=True)\n",
    "    if a and a['href'].startswith(\"https://aws.amazon.com\"):\n",
    "        announcement_title = a.get_text(strip=True)\n",
    "        print(announcement_title)\n",
    "        announcement_summary = result.get_text(separator=\"#@#\", strip=True).split(\"#@#\")[1]\n",
    "        announcement_link = a['href']\n",
    "        \n",
    "        announcements.append({\n",
    "            \"title\": announcement_title,\n",
    "            \"summary\": announcement_summary,\n",
    "            \"link\": announcement_link\n",
    "        })\n",
    "\n",
    "print(f\"No of announcements: {len(announcements)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for announcement in announcements:\n",
    "    announcement_link = announcement[\"link\"]\n",
    "    r = requests.get(announcement_link)\n",
    "    s = BeautifulSoup(r.text, \"html.parser\")\n",
    "    sections = s.find_all(\"section\")\n",
    "    announcement_content = \"\"\n",
    "    for section in sections:\n",
    "        announcement_content = announcement_content + section.get_text(strip=True, separator=\" \")\n",
    "    announcement[\"content\"] = announcement_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235 chunks after splitting\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "text_parser = SentenceSplitter(\n",
    "    chunk_size=512,\n",
    "    separator=\".\",\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "chunk_count = 0\n",
    "for announcement in announcements:\n",
    "    chunks = text_parser.split_text(announcement[\"content\"])\n",
    "    announcement[\"chunks\"] = chunks\n",
    "    chunk_count = chunk_count + len(chunks)\n",
    "\n",
    "print(f\"{chunk_count} chunks after splitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id_='08c9afc2-52e4-44d8-bc59-7308fb033dcd', embedding=None, metadata={'title': 'Introducing Amazon Q Generative SQL in Amazon Redshift (Preview)', 'summary': 'Simplify query authoring and increase your productivity by using natural language to receive SQL code recommendations without extensive knowledge of your organization’s complex database metadata.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"Amazon Q generative SQL is now available in Amazon Redshift Query Editor (preview) Posted On: Nov 29, 2023 Amazon Redshift introduces Amazon Q generative SQL in Amazon Redshift Query Editor, an out-of-the-box web-based SQL editor for Redshift, to simplify query authoring and increase your productivity by allowing you to express queries in natural language and receive SQL code recommendations. Furthermore, it allows you to get insights faster without extensive knowledge of your organization’s complex database metadata. Amazon Q generative SQL uses generative AI to analyze user intent, query patterns, and schema metadata to identify common SQL query patterns directly within Amazon Redshift, accelerating the query authoring process for users, and reducing the time required to derive actionable data insights. Amazon Q generative SQL provides a conversational interface where users can submit queries in natural language, within the scope of their current data permissions. For example, when you submit a question such as 'Find total revenue by region,' Amazon Q generative SQL will recognize and suggest the appropriate SQL code for this frequent query pattern by joining multiple tables, thus saving time and decreasing the likelihood of errors. You can either accept the query or enhance your prior query by asking additional questions. Amazon Q generative SQL in Amazon Redshift Query Editor is available for public preview in the following regions: US East (N. Virginia), US West (Oregon). You can try Amazon Q generative SQL at no cost during preview. See the documentation to get started. »\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='bbd37d90-a3a2-4f04-93c7-d8c45ab05512', embedding=None, metadata={'title': 'Introducing new AI-driven scaling and optimizations in Amazon Redshift Serverless (Preview)', 'summary': 'Amazon Redshift Serverless uses AI techniques to scale automatically with workload changes across all key dimensions—such as data volume changes, concurrent users, and query complexity—to meet and maintain your price performance targets.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Announcing Amazon Redshift Serverless with AI-driven scaling and optimizations (Preview) Posted On: Nov 27, 2023 Today, Amazon Redshift Serverless introduces a preview of the next generation of artificial intelligence (AI)–driven scaling and optimization in cloud data warehousing. Amazon Redshift Serverless uses AI techniques to scale automatically with workload changes across all key dimensions—such as data volume changes, concurrent users, and query complexity—to meet and maintain your price performance targets.\\xa0\\xa0Internal tests demonstrate that these optimizations can give you up to 10x better price performance for variable workloads without manual intervention. With these new AI-driven scaling and optimizations, Amazon Redshift Serverless learns your workload patterns based on dimensions like query complexity and data volumes. It continually adjusts resources throughout the day to apply tailored performance optimizations, and it automatically and proactively adjusts the capacity based on actual workload needs. Furthermore, Amazon Redshift Serverless introduces system-wide new AI-enhanced optimizations and forecasting that go beyond the current, already leading self-tuning capabilities of Amazon Redshift, such as automatic materialized views and sort orders. For instance, it has an ML-enhanced sorting technique that automatically organizes data beyond what traditional encodings can achieve today. You can use a price-performance slider to set your desired price-performance target for your workload. AI-driven scaling and optimization is available in preview in the following AWS Regions: US East (Ohio), US East (N. Virginia), US West (Oregon), Asia Pacific (Tokyo), Europe (Ireland), and Europe (Stockholm). To get started, create Amazon Redshift Serverless using the Create preview workgroup under Amazon Redshift Serverless.\\xa0Note that preview features are provided primarily for evaluation and testing purposes and should not be used in production systems. For preview terms and conditions, see AWS Service Terms: Beta Service Participation . For more information, see the following list of resources: Amazon Redshift Serverless documentation AWS News Blog »', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e9586bd6-aae5-483a-bb5e-12456059e7b6', embedding=None, metadata={'title': 'Announcing Amazon Aurora PostgreSQL zero-ETL integration with Amazon Redshift (Preview)', 'summary': 'This capability enables near real-time analytics and machine learning (ML) using Amazon Redshift on petabytes of transactional data from Amazon Aurora'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='AWS announces Amazon Aurora PostgreSQL zero-ETL integration with Amazon Redshift (Public Preview) Posted On: Nov 28, 2023 Amazon Aurora zero-ETL integration with Amazon Redshift enables near real-time analytics and machine learning (ML) using Amazon Redshift on petabytes of transactional data from Amazon Aurora. Amazon Aurora PostgreSQL-Compatible Edition database clusters can now be used (in public preview) as a source for zero-ETL integrations. Within seconds of transactional data being written into Aurora, the data is available in Amazon Redshift. You don’t have to build and maintain complex data pipelines to perform extract, transform, and load (ETL) operations. Aurora zero-ETL integration with Amazon Redshift helps you derive holistic insights across many applications, and break data silos in your organization. Quickly analyze data from an Aurora database cluster in an Amazon Redshift warehouse. Enhance data analysis with the rich analytics capabilities of Amazon Redshift, such as high-performance SQL, built-in ML and Spark integrations, materialized views, data sharing, and direct access to multiple data stores and data lakes. Aurora PostgreSQL zero-ETL integration with Amazon Redshift is now available in public preview for Aurora PostgreSQL 15.4 in the US East (Ohio) Region for Amazon Aurora provisioned as well as Amazon Redshift Serverless and RA3 instance types. To learn more and get started with a zero-ETL integration, visit the getting started guides for Aurora and Amazon Redshift . »', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='38c8c35d-4073-4f47-8cb3-6044d29fc954', embedding=None, metadata={'title': 'Announcing Amazon RDS for MySQL zero-ETL integration with Amazon Redshift (Preview)', 'summary': 'The new capability allows you to access transactional data from Amazon RDS for MySQL to run analytics and machine learning (ML) on petabytes of data in Amazon Redshift.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='AWS announces Amazon RDS for MySQL zero-ETL integration with Amazon Redshift (Public Preview) Posted On: Nov 28, 2023 Amazon Relational Database Service (Amazon RDS) for MySQL zero-ETL integration with Amazon Redshift allows you to access transactional data from Amazon RDS for MySQL to run analytics and machine learning (ML) on petabytes of data in Amazon Redshift. With the zero-ETL integration, you don’t need to build and maintain complex data pipelines to perform extract, transform, and load (ETL) operations. The Amazon RDS for MySQL zero-ETL integration with Amazon Redshift is now available in public preview for Amazon Redshift Serverless and Amazon Redshift RA3 instance types. An Amazon RDS for MySQL zero-ETL integration with Amazon Redshift\\xa0helps derive holistic insights across many applications and break data silos in your organization, making it simpler to analyze data from one or multiple Amazon RDS for MySQL instances in Amazon Redshift. Additionally, you can consolidate that data into the same Amazon Redshift cluster with data from other zero-ETL integrations, such as from Amazon Aurora and Amazon DynamoDB. Enhance data analysis with the rich analytics capabilities of Amazon Redshift, such as high-performance SQL, built-in ML and Spark integrations, materialized views, data sharing, and direct access to multiple data stores and data lakes. Amazon RDS for MySQL zero-ETL integration with Amazon Redshift is now available in public preview for Amazon RDS for MySQL version 8.0.28 and higher in the US East (Ohio), US East (N. Virginia), US West (Oregon), Asia Pacific (Tokyo), and Europe (Ireland) AWS Regions. To learn more, refer to the Amazon RDS User Guide and the Amazon Redshift Management Guide . »', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='685a39ff-d1ce-49bd-9e95-f7d2c1ba19ce', embedding=None, metadata={'title': 'Announcing Amazon DynamoDB zero-ETL integration with Amazon Redshift (Preview)', 'summary': 'This enables customers to run high performance analytics on their DynamoDB data.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='AWS announces Amazon DynamoDB zero-ETL integration with Amazon Redshift Posted On: Nov 28, 2023 Amazon DynamoDB now supports zero-ETL integration with Amazon Redshift, enabling customers to run high performance analytics on their DynamoDB data. This zero-ETL integration has no impact on production workloads running on DynamoDB. As data is written into a DynamoDB table, it is seamlessly made available in Amazon Redshift, eliminating the need for customers to build and maintain complex data pipelines for performing extract, transform, and load (ETL) operations. The DynamoDB zero-ETL integration with Amazon Redshift helps you derive holistic insights across many applications, break data silos in your organization, and gain significant cost savings and operational efficiencies. Now you can run enhanced analysis on your DynamoDB data with the rich capabilities of Amazon Redshift, such as high performance SQL, built-in ML and Spark integrations, materialized views, data sharing, and ability to join data across multiple data stores and data lakes. Amazon DynamoDB zero-ETL integration with Amazon Redshift is now available in limited preview in the US East (Ohio) region. To request access to the limited preview, visit the Preview Page . To learn more about each service, visit the Amazon DynamoDB or Amazon Redshift webpages. »', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='5bdb0058-f5f7-46d4-9e4a-137015dc9442', embedding=None, metadata={'title': 'Introducing highly durable Amazon OpenSearch Service clusters with 30% price/performance improvement', 'summary': 'Ingest, store, index, and access just about any amount of data, while also enjoying a 30% price/performance improvement over existing instance types, eleven nines of data durability, and a zero-time Recovery Point Objective (RPO).'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='You can use the new OR1 instances to create Amazon OpenSearch Service clusters that use Amazon Simple Storage Service (Amazon S3) for primary storage. You can ingest, store, index, and access just about any imaginable amount of data, while also enjoying a 30% price/performance improvement over existing instance types, eleven nines of data durability, and a zero-time Recovery Point Objective (RPO). You can use this to perform interactive log analytics, monitor application in real time, and more. New OR1 Instances These benefits are all made possible by the new OR1 instances, which are available in eight sizes and used for the data nodes of the cluster: Instance Name vCPUs Memory EBS Storage Max (gp3) or1.medium.search 1 8 GiB 400 GiB or1.large.search 2 16 GiB 800 GiB or1.xlarge.search 4 32 GiB 1.5 TiB or1.2xlarge.search 8 64 GiB 3 TiB or1.4xlarge.search 16 128 GiB 6 TiB or1.8xlarge.search 32 256 GiB 12 TiB or1.12xlarge.search 48 384 GiB 18 TiB or1.16xlarge.search 64 512 GiB 24 TiB To choose a suitable instance size, read Sizing Amazon OpenSearch Service domains . The Amazon Elastic Block Store (Amazon EBS) volumes are used for primary storage, with data copied synchronously to S3 as it arrives. The data in S3 is used to create replicas and to rehydrate EBS after shards are moved between instances as a result of a node failure or a routine rebalancing operation. This is made possible by the remote-backed storage and segment replication features that were recently released for OpenSearch. Creating a Domain To create a domain I open the Amazon OpenSearch Service Console, select Managed clusters , and click Create domain : I enter a name for my domain ( my-domain ), select Standard create , and use the Production template: Then I choose the Domain with standby deployment option. This option will create active data nodes in two Availability Zones and a standby one in a third.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='fdffa20a-0e8a-462b-a4f4-f57168c0a92e', embedding=None, metadata={'title': 'Introducing highly durable Amazon OpenSearch Service clusters with 30% price/performance improvement', 'summary': 'Ingest, store, index, and access just about any amount of data, while also enjoying a 30% price/performance improvement over existing instance types, eleven nines of data durability, and a zero-time Recovery Point Objective (RPO).'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='I also choose the latest engine version: Then I select the OR1 instance family and (for my use case) configure 500 GiB of EBS storage per data node: I set the other settings as needed, and click Create to proceed: I take a quick lunch break and when i come back my domain is ready: Things to Know Here are a couple of things to know about this new storage option: Engine Versions – Amazon OpenSearch Service engines version 2.11 and above support OR1 instances. Regions – The OR1 instance family is available for use with OpenSearch in the US East (Ohio, N. Virginia), US West (N. California, Oregon), Asia Pacific (Mumbai, Singapore, Sydney, Tokyo), and Europe (Frankfurt, Ireland, Spain, Stockholm) AWS Regions. Pricing – You pay On-Demand or Reserved prices for data nodes, and you also pay for EBS storage. See the Amazon OpenSearch Service Pricing page for more information. — Jeff ;', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='c0a8fb63-a425-42af-9190-6145ddfe508e', embedding=None, metadata={'title': 'AWS Clean Rooms Differential Privacy enhances privacy protection of your users’ data (preview)', 'summary': 'Help protect the privacy of your users with mathematically backed and intuitive controls in a few steps.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Starting today, you can use AWS Clean Rooms Diﬀerential Privacy (preview) to help protect the privacy of your users with mathematically backed and intuitive controls in a few steps. As a fully managed capability of AWS Clean Rooms , no prior differential privacy experience is needed to help you prevent the reidentification of your users. AWS Clean Rooms Differential Privacy obfuscates the contribution of any individual’s data in generating aggregate insights in collaborations so that you can run a broad range of SQL queries to generate insights about advertising campaigns, investment decisions, clinical research, and more. Quick overview on differential privacy Differential privacy is not new. It is a strong, mathematical definition of privacy compatible with statistical and machine learning based analysis, and has been used by the United States Census Bureau as well as companies with vast amounts of data. Differential privacy helps with a wide variety of use cases involving large datasets, where adding or removing a few individuals has a small impact on the overall result, such as population analyses using count queries, histograms, benchmarking, A/B testing, and machine learning. The following illustration shows how differential privacy works when it is applied to SQL queries. When an analyst runs a query, differential privacy adds a carefully calibrated amount of error (also referred to as noise) to query results at run-time, masking the contribution of individuals while still keeping the query results accurate enough to provide meaningful insights. The noise is carefully fine-tuned to mask the presence or absence of any possible individual in the dataset. Differential privacy also has another component called privacy budget. The privacy budget is a finite resource consumed each time a query is run and thus controls the number of queries that can be run on your datasets, helping ensure that the noise cannot be averaged out to reveal any private information about an individual. When the privacy budget is fully exhausted, no more queries can be run on your tables until it is increased or refreshed. However, differential privacy is not easy to implement because this technique requires an in-depth understanding of mathematically rigorous formulas and theories to apply it effectively. Configuring differential privacy is also a complex task because customers need to calculate the right level of noise in order to preserve the privacy of their users without negatively impacting the utility of query results. Customers also want to enable their partners to conduct a wide variety of analyses including highly complex and customized queries on their data.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='a21265b5-43c9-4af9-bead-bd1ca17461a3', embedding=None, metadata={'title': 'AWS Clean Rooms Differential Privacy enhances privacy protection of your users’ data (preview)', 'summary': 'Help protect the privacy of your users with mathematically backed and intuitive controls in a few steps.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='This requirement is hard to support with differential privacy because of the intricate nature of the calculations involved in calibrating the noise while processing various query components such as aggregations, joins, and transformations. We created AWS Clean Rooms Differential Privacy to help you protect the privacy of your users with mathematically backed controls in a few clicks. How differential privacy works in AWS Clean Rooms While differential privacy is quite a sophisticated technique, AWS Clean Rooms Differential Privacy makes it easy for you to apply it and protect the privacy of your users with mathematically backed, flexible, and intuitive controls. You can begin using it with just a few steps after starting or joining an AWS Clean Rooms collaboration as a member with abilities to contribute data. You create a configured table, which is a reference to your table in the AWS Glue Data Catalog, and choose to turn on differential privacy while adding a custom analysis rule to the configured table. Next, you associate the configured table to your AWS Clean Rooms collaboration and configure a differential privacy policy in the collaboration to make your table available for querying. You can use a default policy to quickly complete the setup or customize it to meet your specific requirements. As part of this step, you will configure the following: Privacy budget Quantified as a value that we call epsilon , the privacy budget controls the level of privacy protection. It is a common, ﬁnite resource that is applied for all of your tables protected with differential privacy in the collaboration because the goal is to preserve the privacy of your users whose information can be present in multiple tables. The privacy budget is consumed every time a query is run on your tables. You have the flexibility to increase the privacy budget value any time during the collaboration and automatically refresh it each calendar month. Noise added per query Measured in terms of the number of users whose contributions you want to obscure, this input parameter governs the rate at which the privacy budget is depleted. In general, you need to balance your privacy needs against the number of queries you want to permit and the accuracy of those queries. AWS Clean Rooms makes it easy for you to complete this step by helping you understand the resulting utility you are providing to your collaboration partner. You can also use the interactive examples to understand how your chosen settings would impact the results for different types of SQL queries. Now that you have successfully enabled differential privacy protection for your data, let’s see AWS Clean Rooms Differential Privacy in action.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e6357bb8-ee0e-4d76-a168-394867af68b8', embedding=None, metadata={'title': 'AWS Clean Rooms Differential Privacy enhances privacy protection of your users’ data (preview)', 'summary': 'Help protect the privacy of your users with mathematically backed and intuitive controls in a few steps.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='For this demo, let’s assume I am your partner in the AWS Clean Rooms collaboration. Here, I’m running a query to count the number of overlapping customers and the result shows there are 3,227,643 values for tv.customer_id . Now, if I run the same query again after removing records about an individual from coffee_customers table, it shows a diﬀerent result, 3,227,604 tv.customer_id . This variability in the query results prevents me from identifying the individuals from observing the diﬀerence in query results. I can also see the impact of differential privacy, including the remaining queries I can run. Available for preview Join this preview and start protecting the privacy of your users with AWS Clean Rooms Differential Privacy. During this preview period, you can use AWS Clean Rooms Differential Privacy wherever AWS Clean Rooms is available. To learn more on how to get started, visit the AWS Clean Rooms Differential Privacy page. Happy collaborating! — Donnie', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='48719bcf-9a75-4303-8422-4032d7b0909a', embedding=None, metadata={'title': 'AWS Clean Rooms ML helps customers and partners apply ML models without sharing raw data (preview)', 'summary': 'This capability helps you and your partners apply machine learning (ML) models on your collective data without copying or sharing raw data with each other.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, we’re introducing AWS Clean Rooms ML (preview), a new capability of AWS Clean Rooms that helps you and your partners apply machine learning (ML) models on your collective data without copying or sharing raw data with each other. With this new capability, you can generate predictive insights using ML models while continuing to protect your sensitive data. During this preview, AWS Clean Rooms ML introduces its ﬁrst model specialized to help companies create lookalike segments for marketing use cases. With AWS Clean Rooms ML lookalike, you can train your own custom model, and you can invite partners to bring a small sample of their records to collaborate and generate an expanded set of similar records while protecting everyone’s underlying data. In the coming months, AWS Clean Rooms ML will release a healthcare model. This will be the first of many models that AWS Clean Rooms ML will support next year. AWS Clean Rooms ML helps you to unlock various opportunities for you to generate insights. For example: Airlines can take signals about loyal customers, collaborate with online booking services, and offer promotions to users with similar characteristics. Auto lenders and car insurers can identify prospective auto insurance customers who share characteristics with a set of existing lease owners. Brands and publishers can model lookalike segments of in-market customers and deliver highly relevant advertising experiences. Research institutions and hospital networks can find candidates similar to existing clinical trial participants to accelerate clinical studies (coming soon). AWS Clean Rooms ML lookalike modeling helps you apply an AWS managed, ready-to-use model that is trained in each collaboration to generate lookalike datasets in a few clicks, saving months of development work to build, train, tune, and deploy your own model. How to use AWS Clean Rooms ML to generate predictive insights Today I will show you how to use lookalike modeling in AWS Clean Rooms ML and assume you have already set up a data collaboration with your partner. If you want to learn how to do that, check out\\xa0the AWS Clean Rooms Now Generally Available — Collaborate with Your Partners without Sharing Raw Data post. With your collective data in the AWS Clean Rooms collaboration, you can work with your partners to apply ML lookalike modeling to generate a lookalike segment. It works by taking a small sample of representative records from your data, creating a machine learning (ML) model, then applying the particular model to identify an expanded set of similar records from your business partner’s data.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='d62de173-760a-457d-b307-d899b6b65f14', embedding=None, metadata={'title': 'AWS Clean Rooms ML helps customers and partners apply ML models without sharing raw data (preview)', 'summary': 'This capability helps you and your partners apply machine learning (ML) models on your collective data without copying or sharing raw data with each other.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='The following screenshot shows the overall workflow for using AWS Clean Rooms ML. By using AWS Clean Rooms ML, you don’t need to build complex and time-consuming ML models on your own. AWS Clean Rooms ML trains a custom, private ML model, which saves months of your time while still protecting your data. Eliminating the need to share data As ML models are natively built within the service, AWS Clean Rooms ML helps you protect your dataset and customer’s information because you don’t need to share your data to build your ML model. You can specify the training dataset using the AWS Glue Data Catalog table, which contains user-item interactions. Under Additional columns to train , you can define numerical and categorical data. This is useful if you need to add more features to your dataset, such as the number of seconds spent watching a video, the topic of an article, or the product category of an e-commerce item. Applying custom-trained AWS-built models Once you have defined your training dataset, you can now create a lookalike model. A lookalike model is a machine learning model used to find similar profiles in your partner’s dataset without either party having to share their underlying data with each other. When creating a lookalike model, you need to specify the training dataset. From a single training dataset, you can create many lookalike models. You also have the flexibility to define the date window in your training dataset using Relative range or Absolute range . This is useful when you have data that is constantly updated within AWS Glue, such as articles read by users. Easy-to-tune ML models After you create a lookalike model, you need to configure it to use in AWS Clean Rooms collaboration. AWS Clean Rooms ML provides flexible controls that enable you and your partners to tune the results of the applied ML model to garner predictive insights. On the Configure lookalike model page, you can choose which Lookalike model you want to use and define the Minimum matching seed size you need. This seed size deﬁnes the minimum number of profiles in your seed data that overlap with profiles in the training data. You also have the flexibility to choose whether the partner in your collaboration receives metrics in Metrics to share with other members . With your lookalike models properly configured, you can now make the ML models available for your partners by associating the configured lookalike model with a collaboration.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='3a307a25-0857-486c-8a56-50308118454a', embedding=None, metadata={'title': 'AWS Clean Rooms ML helps customers and partners apply ML models without sharing raw data (preview)', 'summary': 'This capability helps you and your partners apply machine learning (ML) models on your collective data without copying or sharing raw data with each other.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Creating lookalike segments Once the lookalike models have been associated, your partners can now start generating insights by selecting Create lookalike segment and choosing the associated lookalike model for your collaboration. Here on the Create lookalike segment page, your partners need to provide the Seed profiles . Examples of seed profiles include your top customers or all customers who purchased a specific product. The resulting lookalike segment will contain profiles from the training data that are most similar to the profiles from the seed. Lastly, your partner will get the Relevance metrics as the result of the lookalike segment using the ML models. At this stage, you can use the Score to make a decision. Export data and use programmatic API You also have the option to export the lookalike segment data. Once it’s exported, the data is available in JSON format and you can process this output by integrating with AWS Clean Rooms API and your applications. Join the preview AWS Clean Rooms ML is now in preview and available via AWS Clean Rooms in US East (Ohio, N. Virginia), US West (Oregon), Asia Pacific (Seoul, Singapore, Sydney, Tokyo), and Europe (Frankfurt, Ireland, London). Support for additional models is in the works. Learn how to apply machine learning with your partners without sharing underlying data on the AWS Clean Rooms ML page. Happy collaborating! — Donnie', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='3ed27048-4644-47b2-a2a1-edaa634be253', embedding=None, metadata={'title': 'Announcing Amazon OpenSearch Service zero-ETL integration with Amazon S3 (preview)', 'summary': 'This is a new way to query operational logs in Amazon S3 and S3-based data lakes without needing to switch between services.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today we are announcing a preview of Amazon OpenSearch Service zero-ETL integration with Amazon S3 , a new way to query operational logs in Amazon S3 and S3-based data lakes without needing to switch between services. You can now analyze infrequently queried data in cloud object stores and simultaneously use the operational analytics and visualization capabilities of OpenSearch Service. Amazon OpenSearch Service direct queries with Amazon S3 provides a zero-ETL integration to reduce the operational complexity of duplicating data or managing multiple analytics tools by enabling customers to directly query their operational data, reducing costs and time to action. This zero-ETL integration will be configurable within OpenSearch Service, where you can take advantage of various log type templates, including predefined dashboards, and configure data accelerations tailored to that log type. Templates include VPC Flow Logs , Elastic Load Balancing logs, and NGINX logs, and accelerations include skipping indexes, materialized views, and covered indexes. With direct queries with Amazon S3, you can perform complex queries critical to security forensic and threat analysis that correlate data across multiple data sources, which aids teams in investigating service downtime and security events. After creating an integration, you can start querying their data directly from the OpenSearch Dashboards or OpenSearch API. You can easily audit connections to ensure that they are set up in a scalable, cost-efficient, and secure way. Getting started with direct queries with Amazon S3 You can easily get started by creating a new Amazon S3 direct query data source for OpenSearch Service through the AWS Management Console or the API. Each new data source uses AWS Glue Data Catalog to manage tables that represent S3 buckets. Once you create a data source, you can configure Amazon S3 tables and data indexing and query data in OpenSearch Dashboards. 1. Create a data source in OpenSearch Service Before you create a data source, you should have an OpenSearch Service domain with version 2.11 or later and a target Amazon S3 table in AWS Glue Data Catalog with the appropriate IAM permissions. IAM will need access to the desired S3 bucket(s) and read and write access to AWS Glue Data Catalog. To learn more about IAM prerequisites, see Creating a data source in the AWS documentation. Go to the OpenSearch Service console and choose the domain you want to set up a new data source for.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='916fb324-c327-4a7d-a47a-0444c87e7c31', embedding=None, metadata={'title': 'Announcing Amazon OpenSearch Service zero-ETL integration with Amazon S3 (preview)', 'summary': 'This is a new way to query operational logs in Amazon S3 and S3-based data lakes without needing to switch between services.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"In the domain details page, choose the Connections tab below the general information and see the Direct Query section. To create a new data source, choose Create , input the name of your new data source, select the data source type as Amazon S3 with AWS Glue Data Catalog , and choose the IAM role for your data source. Once you create a data source, you can go to the OpenSearch Dashboards of the domain, which you use to configure access control, define tables, set up log type–based dashboards for popular log types, and query your data. 2. Configuring your data source in OpenSearch Dashboards To configure data source in OpenSearch Dashboards, choose Configure in the console and go to OpenSearch Dashboards. In the left-hand navigation of OpenSearch Dashboards, under Management , choose Data sources . Under Manage data sources , choose the name of the data source you created in the console. Direct queries from OpenSearch Service to Amazon S3 use Spark tables within AWS Glue Data Catalog. To create a new table you want to direct query, go to the Query Workbench in the Open Search Plugins menu. Now run as in the following SQL statement to create http_logs table and run MSCK REPAIR TABLE mys3.default.http_logs command to update the metadata in the catalog CREATE EXTERNAL TABLE IF NOT EXISTS mys3.default.http_logs (\\n   `@timestamp` TIMESTAMP,\\n    clientip STRING,\\n    request STRING, \\n    status INT, \\n    size INT, \\n    year INT, \\n    month INT, \\n    day INT) \\nUSING json PARTITIONED BY(year, month, day) OPTIONS (path 's3://mys3/data/http_log/http_logs_partitioned_json_bz2/', compression 'bzip2') To ensure a fast experience with your data in Amazon S3, you can set up any of three different types of accelerations to index data into OpenSearch Service, such as skipping indexes, materialized views, and covering indexes. To create OpenSearch indexes from external data connections for better performance, choose the Accelerate Table . Skipping indexes allow you to index only the metadata of the data stored in Amazon S3. Skipping indexes help quickly identify data stored by narrowing down a specific location of where the data is stored. Materialized views enable you to use complex queries such as aggregations, which can be used for querying or powering dashboard visualizations.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='701cb4af-c35e-4f54-90a5-e8b31026645c', embedding=None, metadata={'title': 'Announcing Amazon OpenSearch Service zero-ETL integration with Amazon S3 (preview)', 'summary': 'This is a new way to query operational logs in Amazon S3 and S3-based data lakes without needing to switch between services.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Materialized views ingest data into OpenSearch Service for anomaly detection or geospatial capabilities. Covering indexes will ingest all the data from the specified table column. Covering indexes are the most performant of the three indexing types. 3. Query your data source in OpenSearch Dashboards After you set up your tables, you can query your data using Discover . You can run a sample SQL query for the http_logs table you created in AWS Glue Data Catalog tables. To learn more, see Working with Amazon OpenSearch Service direct queries with Amazon S3 in the AWS documentation. Join the preview Amazon OpenSearch Service zero-ETL integration with Amazon S3 is now previewed in the AWS US East (Ohio), US East (N. Virginia), US West (Oregon), Asia Pacific (Tokyo), Europe (Frankfurt), and Europe (Ireland) Regions. OpenSearch Service separately charges for only the compute needed as OpenSearch Compute Units to query your external data as well as maintain indexes in OpenSearch Service. For more information, see Amazon OpenSearch Service Pricing . Give it a try and send feedback to the AWS re:Post for Amazon OpenSearch Service or through your usual AWS Support contacts. — Channy', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e834c951-0fc4-4205-9cbe-eedf8b86d50d', embedding=None, metadata={'title': 'New generative AI capabilities for Amazon DataZone further simplify data cataloging and discovery (preview)', 'summary': 'This new feature can automate the traditionally labor-intensive process of data cataloging and dramatically decrease the amount of time needed to provide context for organizational data.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, we are announcing a preview of an automation feature backed by generative artificial intelligence (AI) for Amazon DataZone that will dramatically decrease the amount of time needed to provide context for organizational data. The new feature can automate the traditionally labor-intensive process of data cataloging. Powered by the large language models (LLMs) of Amazon Bedrock , it generates detailed descriptions of data assets and their schemas, and suggests analytical use cases. You can generate a comprehensive business context with a single click. We heard from customers that data consumers such as data analysts, scientists, and engineers in organizations struggle to understand the data’s relevance with little metadata. As a result, they either spend more time interpreting the data, or they return to data producers with continued questions. So, data producers such as data owners, engineers, and analysts who own the data and make it available for consumers need to manually enter detailed context for higher-priority data to make data shareable and discoverable. This is time-consuming and the number one problem customers have when trying to collate their data in a system for self-service by consumers. When we launched the general availability of Amazon DataZone in October 2023, we introduced the first feature that brings generative AI capabilities to automate the generation of the table name and column names of a business catalog asset. In the data portal of Amazon DataZone, the green brain icon indicates automatically generated metadata suggestions. You could accept, edit, or reject each suggestion recommended by Amazon DataZone. What’s new with today’s preview announcement? Now, in addition to column and table names, you can automatically generate more detailed descriptions of the table and schema, as well as suggested uses. In the Business Metadata tab in the data portal, when you choose Generate summary , new content will be generated to explain the table and its metadata. You can also accept, edit, and reject this recommendation. When you choose the Schema tab, you can also see new Description recommendations as well as the Name . You can review generated metadata and choose to accept, edit, or reject the recommendation. This new feature will enhance data discoverability and reduce on back-and-forth communications between data consumers and producers. You will have a richer search experience based on extensive data insights in the future. Join the preview The new metadata generation ability is now previewed in the AWS US East (N. Virginia) and US West (Oregon) Regions.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='424e57d1-6b29-4f84-9564-d4789784f4d3', embedding=None, metadata={'title': 'New generative AI capabilities for Amazon DataZone further simplify data cataloging and discovery (preview)', 'summary': 'This new feature can automate the traditionally labor-intensive process of data cataloging and dramatically decrease the amount of time needed to provide context for organizational data.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='With this new generative AI capability, you can reduce time-to-insight by accelerating data cataloging and boosting data discovery. To learn more, visit the Amazon DataZone: Automate Data Discovery . Give it a try and send feedback to AWS re:Post for Amazon DataZone or through your usual AWS Support contacts. — Channy', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='4d37d5c2-017f-44f6-b72f-c554b6f8bf33', embedding=None, metadata={'title': 'Analyze large amounts of graph data to get insights and find trends with Amazon Neptune Analytics', 'summary': 'This new analytics database engine makes it faster for data scientists and application developers to quickly analyze large amounts of graph data.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='I am happy to announce the general availability of Amazon Neptune Analytics , a new analytics database engine that makes it faster for data scientists and application developers to quickly analyze large amounts of graph data. With Neptune Analytics, you can now quickly load your dataset from Amazon Neptune or your data lake on Amazon Simple Storage Service (Amazon S3) , run your analysis tasks in near real time, and optionally terminate your graph afterward. Graph data enables the representation and analysis of intricate relationships and connections within diverse data domains. Common applications include social networks, where it aids in identifying communities, recommending connections, and analyzing information diffusion. In supply chain management, graphs facilitate efficient route optimization and bottleneck identification. In cybersecurity, they reveal network vulnerabilities and identify patterns of malicious activity. Graph data finds application in knowledge management, financial services, digital advertising, and network security, performing tasks such as identifying money laundering networks in banking transactions and predicting network vulnerabilities. Since the launch of Neptune in May 2018 , thousands of customers have embraced the service for storing their graph data and performing updates and deletion on specific subsets of the graph. However, analyzing data for insights often involves loading the entire graph into memory. For instance, a financial services company aiming to detect fraud may need to load and correlate all historical account transactions. Performing analyses on extensive graph datasets, such as running common graph algorithms, requires specialized tools. Utilizing separate analytics solutions demands the creation of intricate pipelines to transfer data for processing, which is challenging to operate, time-consuming, and prone to errors. Furthermore, loading large datasets from existing databases or data lakes to a graph analytic solution can take hours or even days. Neptune Analytics offers a fully managed graph analytics experience. It takes care of the infrastructure heavy lifting, enabling you to concentrate on problem-solving through queries and workflows. Neptune Analytics automatically allocates compute resources according to the graph’s size and quickly loads all the data in memory to run your queries in seconds. Our initial benchmarking shows that Neptune Analytics loads data from Amazon S3 up to 80x faster than existing AWS solutions. Neptune Analytics supports 5 families of algorithms covering 15 different algorithms , each with multiple variants. For example, we provide algorithms for path-finding, detecting communities (clustering), identifying important data (centrality), and quantifying similarity. Path-finding algorithms are used for use cases such as route planning for supply chain optimization.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='15eacf88-263d-4240-9b4e-fc1d57440abb', embedding=None, metadata={'title': 'Analyze large amounts of graph data to get insights and find trends with Amazon Neptune Analytics', 'summary': 'This new analytics database engine makes it faster for data scientists and application developers to quickly analyze large amounts of graph data.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Centrality algorithms like page rank identify the most influential sellers in a graph. Algorithms like connected components , clustering , and similarity algorithms can be used for fraud-detection use cases to determine whether the connected network is a group of friends or a fraud ring formed by a set of coordinated fraudsters. Neptune Analytics facilitates the creation of graph applications using openCypher , presently one of the widely adopted graph query languages. Developers, business analysts, and data scientists appreciate openCypher’s SQL-inspired syntax, finding it familiar and structured for composing graph queries. Let’s see it at work As we usually do on the AWS News blog, let’s show how it works. For this demo, I first navigate to Neptune in the AWS Management Console . There is a new Analytics section on the left navigation pane. I select Graphs and then Create graph . On the Create graph page, I enter the details of my graph analytics database engine. I won’t detail each parameter here; their names are self-explanatory. Pay attention to Allow from public because, the vast majority of the time, you want to keep your graph only available from the boundaries of your VPC. I also create a Private endpoint to allow private access from machines and services inside my account VPC network. In addition to network access control, users will need proper IAM permissions to access the graph. Finally, I enable Vector search to perform similarity search using embeddings in the dataset. The dimension of the vector depends on the large language model (LLM) that you use to generate the embedding. When I am ready, I select Create graph (not shown here). After a few minutes, my graph is available. Under Connectivity & security , I take note of the Endpoint . This is the DNS name I will use later to access my graph from my applications. I can also create Replicas . A replica is a warm standby copy of the graph in another Availability Zone. You might decide to create one or more replicas for high availability. By default, we create one replica, and depending on your availability requirements, you can choose not to create replicas. Business queries on graph data Now that the Neptune Analytics graph is available, let’s load and analyze data. For the rest of this demo, imagine I’m working in the finance industry. I have a dataset obtained from the US Securities and Exchange Commission (SEC) .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='64c92753-1ab8-474f-9e00-b5d0b52d5571', embedding=None, metadata={'title': 'Analyze large amounts of graph data to get insights and find trends with Amazon Neptune Analytics', 'summary': 'This new analytics database engine makes it faster for data scientists and application developers to quickly analyze large amounts of graph data.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='This dataset contains the list of positions held by investors that have more than $100 million in assets. Here is a diagram to illustrate the structure of the dataset I use in this demo. I want to get a better understanding of the positions held by one investment firm (let’s name it “Seb’s Investments LLC”). I wonder what its top five holdings are and who else holds more than $1 billion in the same companies. I am also curious to know what are other investment companies that have a similar portfolio as Seb’s Investments LLC. To start my analysis, I create a Jupyter notebook in the Neptune section of the AWS Management Console . In the notebook, I first define my analytics endpoint and load the data set from an S3 bucket. It takes only 18 seconds to load 17 million records. Then, I start to explore the dataset using openCypher queries. I start by defining my parameters: params = {\\'name\\': \"Seb\\'s Investments LLC\", \\'quarter\\': \\'2023Q4\\'} First, I want to know what the top five holdings are for Seb’s Investments LLC in this quarter and who else holds more than $1 billion in the same companies. In openCypher, it translates to the query hereafter. The $name parameter’s value is “Seb’s Investment LLC” and the $quarter parameter’s value is 2023Q4. MATCH p=(h:Holder)-->(hq1)-[o:owns]->(holding)\\nWHERE h.name = $name AND hq1.name = $quarter\\nWITH DISTINCT holding as holding, o ORDER BY o.value DESC LIMIT 5\\nMATCH (holding)<-[o2:owns]-(hq2)<--(coholder:Holder)\\nWHERE hq2.name = \\'2023Q4\\'\\nWITH sum(o2.value) AS totalValue, coholder, holding\\nWHERE totalValue > 1000000000\\nRETURN coholder.name, collect(holding.name) Then, I want to know what the other top five companies are that have similar holdings as “Seb’s Investments LLC.” I use the topKByNode() function to perform a vector search.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='3f8f5cc9-dafd-4b60-8505-a58c9a440538', embedding=None, metadata={'title': 'Analyze large amounts of graph data to get insights and find trends with Amazon Neptune Analytics', 'summary': 'This new analytics database engine makes it faster for data scientists and application developers to quickly analyze large amounts of graph data.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='MATCH (n:Holder)\\nWHERE n.name = $name\\nCALL neptune.algo.vectors.topKByNode(n)\\nYIELD node, score\\nWHERE score >0\\nRETURN node.name LIMIT 5 This query identifies a specific Holder node with the name “Seb’s Investments LLC.” Then, it utilizes the Neptune Analytics custom vector similarity search algorithm on the embedding property of the Holder node to find other nodes in the graph that are similar. The results are filtered to include only those with a positive similarity score, and the query finally returns the names of up to five related nodes. Pricing and availability Neptune Analytics is available today in seven AWS Regions: US East (Ohio, N. Virginia), US West (Oregon), Asia Pacific (Singapore, Tokyo), and Europe (Frankfurt, Ireland). AWS charges for the usage on a pay-as-you-go basis, with no recurring subscriptions or one-time setup fees. Pricing is based on configurations of memory-optimized Neptune capacity units (m-NCU). Each m-NCU corresponds to one hour of compute and networking capacity and 1 GiB of memory. You can choose configurations starting with 128 m-NCUs and up to 4096 m-NCUs. In addition to m-NCU, storage charges apply for graph snapshots. I invite you to read the Neptune pricing page for more details Neptune Analytics is a new analytics database engine to analyze large graph datasets. It helps you discover insights faster for use cases such as fraud detection and prevention, digital advertising, cybersecurity, transportation logistics, and bioinformatics. Get started Log in to the AWS Management Console to give Neptune Analytics a try. -- seb', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ad90cd04-7292-4d7b-91f2-fb49e97ca652', embedding=None, metadata={'title': 'New Amazon Q in QuickSight uses generative AI assistance for quicker, easier data insights (preview)', 'summary': 'With a reimagined Q&A experience, users can generate stories examining their data, see executive summaries from data in seconds, and answer questions of data not answered by dashboards and reports.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, I’m happy to share that Amazon Q in QuickSight is available for preview. Now you can experience the Generative BI capabilities in Amazon QuickSight announced on July 26, as well as two additional capabilities for business users. Turning insights into impact faster with Amazon Q in QuickSight With this announcement, business users can now generate compelling sharable stories examining their data, see executive summaries of dashboards surfacing key insights from data in seconds, and confidently answer questions of data not answered by dashboards and reports with a reimagined Q&A experience. Before we go deeper into each capability, here’s a quick summary: Stories — This is a new and visually compelling way to present and share insights. Stories can automatically generated in minutes using natural language prompts, customized using point-and-click options, and shared securely with others. Executive summaries — With this new capability, Amazon Q helps you to understand key highlights in your dashboard. Data Q&A — This capability provides a new and easy-to-use natural-language Q&A experience to help you get answers for questions beyond what is available in existing dashboards and reports.\\u200b\\u200b To get started, you need to enable Preview Q Generative Capabilities in Preview manager . Once enabled, you’re ready to experience what Amazon Q in QuickSight brings for\\xa0business users and business analysts building dashboards. Stories automatically builds formatted narratives Business users often need to share their findings of data with others to inform team decisions; this has historically involved taking data out of the business intelligence (BI) system. Stories are a new feature enabling business users to create beautifully formatted narratives that describe data, and include visuals, images, and text in document or slide format directly that can easily be shared with others within QuickSight. Now, business users can use natural language to ask Amazon Q to build a story about their data by starting from the Amazon Q Build menu on an Amazon QuickSight dashboard. Amazon Q extracts data insights and statistics from selected visuals, then uses large language models (LLMs) to build a story in multiple parts, examining what the data may mean to the business and suggesting ideas to achieve specific goals. For example, a sales manager can ask, “Build me a story about overall sales performance trends. Break down data by product and region.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ab45c611-a92d-4a31-9c18-84c44f9a2bb5', embedding=None, metadata={'title': 'New Amazon Q in QuickSight uses generative AI assistance for quicker, easier data insights (preview)', 'summary': 'With a reimagined Q&A experience, users can generate stories examining their data, see executive summaries from data in seconds, and answer questions of data not answered by dashboards and reports.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Suggest some strategies for improving sales.” Or, “Write a marketing strategy that uses regional sales trends to uncover opportunities that increase revenue.” Amazon Q will build a story exploring specific data insights, including strategies to grow sales. Once built, business users get point-and-click tools augmented with artificial intelligence- (AI) driven rewriting capabilities to customize stories using a rich text editor to refine the message, add ideas, and highlight important details. Stories can also be easily and securely shared with other QuickSight users by email. Executive summaries deliver a quick snapshot of important information Executive summaries are now available with a single click using the Amazon Q Build menu in Amazon QuickSight. Amazon QuickSight automatically determines interesting facts and statistics, then use LLMs to write about interesting trends. This new capability saves time in examining detailed dashboards by providing an at-a-glance view of key insights described using natural language. The executive summaries feature provides two advantages. First, it helps business users generate all the key insights without the need to browse through tens of visuals on the dashboard and understand changes from each. Secondly, it enables readers to find key insights based on information in the context of dashboards and reports with minimum effort. New data Q&A experience Once an interesting insight is discovered, business users frequently need to dig in to understand data more deeply than they can from existing dashboards and reports. Natural language query (NLQ) solutions designed to solve this problem frequently expect that users already know what fields may exist or how they should be combined to answer business questions. However, business users aren’t always experts in underlying data schemas, and their questions frequently come in more general terms, like “How were sales last week in NY?” Or, “What’s our top campaign?” The new Q&A experience accessed within the dashboards and reports helps business users confidently answer questions about data. It includes AI-suggested questions and a profile of what data can be asked about and automatically generated multi-visual answers with narrative summaries explaining data context. Furthermore, Amazon Q brings the ability to answer vague questions and offer alternatives for specific data. For example, customers can ask a vague question, such as “Top products,” and Amazon Q will provide an answer that breaks down products by sales and offers alternatives for products by customer count and products by profit. Amazon Q explains answer context in a narrative summarizing total sales, number of products, and picking out the sales for the top product.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='cd594e35-b0d9-4edb-972c-af9b2719596e', embedding=None, metadata={'title': 'New Amazon Q in QuickSight uses generative AI assistance for quicker, easier data insights (preview)', 'summary': 'With a reimagined Q&A experience, users can generate stories examining their data, see executive summaries from data in seconds, and answer questions of data not answered by dashboards and reports.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Customers can search for specific data values and even a single word such as, for example, the product name “contactmatcher.” Amazon Q returns a complete set of data related to that product and provides a natural language breakdown explaining important insights like total units sold. Specific visuals from the answers can also be added to a pinboard for easy future access. Watch the demo To see these new capabilities in action, have a look at the demo. Things to Know Here are a few additional things that you need to know: The Amazon Q Build menu options for Executive Summaries and Stories need to be enabled in each dashboard when publishing. To learn how Amazon Q in QuickSight enables business analysts to build dashboards to apply an extended list of visual refinement options, read: Generative BI dashboard authoring capabilities now available in preview for Amazon QuickSight Q customers. Join the preview Amazon Q in QuickSight product page Read more about Amazon Q Introducing Amazon Q, a new generative AI-powered assistant (preview) Amazon Q brings generative AI-powered assistance to IT pros and developers (preview) Improve developer productivity with generative-AI powered Amazon Q in Amazon CodeCatalyst (preview) Upgrade your Java applications with Amazon Q Code Transformation (preview) New generative AI features in Amazon Connect, including Amazon Q, facilitate improved contact center service — Donnie', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='34cf9273-53b0-4ffe-9930-cbcf42edec97', embedding=None, metadata={'title': 'Use anomaly detection with AWS Glue to improve data quality (preview)', 'summary': 'This new feature will help to improve your data quality by using machine learning to detect statistical anomalies and unusual patterns.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='We are launching a preview of a new AWS Glue Data Quality feature that will help to improve your data quality by using machine learning to detect statistical anomalies and unusual patterns. You get deep insights into data quality issues, data quality scores, and recommendations for rules that you can use to continuously monitor for anomalies, all without having to write any code. Data quality counts AWS customers already build data integration pipelines to extract and transform data. They set up data quality rules to ensure that the resulting data is of high quality and can be used to make accurate business decisions. In many cases, these rules assess the data based on criteria that were chosen and locked in at a specific point in time, reflecting the current state of the business. However, as the business environment changes and the properties of the data shift, the rules are not always reviewed and updated. For example, a rule could be set to verify that daily sales are at least ten thousand dollars for an early-stage business. As the business succeeds and grows, the rule should be checked and updated from time to time, but in practice this rarely happens. As a result, if there’s an unexpected drop in sales, the outdated rule does not activate, and no one is happy. Anomaly detection in action To detect unusual patterns and to gain deeper insights into data, organizations try to create their own adaptive systems or turn to costly commercial solutions that require specific technical skills and specialized business knowledge. To address this widespread challenge, Glue Data Quality now makes use of machine learning (ML). Once activated, this cool new addition to Glue Data Quality gathers statistics as fresh data arrives, using ML and dynamic thresholds to learn from past patterns while looking outliers and unusual data patterns. This process produces observations and also visualizes trends so that you can quickly gain a better understanding of the anomaly. You will also get rule recommendations as part of the Observations, and you can easily and progressively add them to your data pipelines. Rules can enforce an action such as stopping your data pipelines. In the past, you could only write static rules. Now, you can write Dynamic rules that have auto-adjusting thresholds and AnomalyDetection Rules that grasp recurring patterns and spot deviations. When you use rules as part of data pipelines, they can stop the data flow so that a data engineer can review, fix and resume.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e4c47d01-c3dc-42bc-b427-d95df3d60ff3', embedding=None, metadata={'title': 'Use anomaly detection with AWS Glue to improve data quality (preview)', 'summary': 'This new feature will help to improve your data quality by using machine learning to detect statistical anomalies and unusual patterns.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='To use anomaly detection, I add an Evaluate Data Quality node to my job: I select the node and click Add analyzer to choose a statistic and the columns: Glue Data Quality learns from the data to recognize patterns and then generates observations that will be shown in the Data quality tab: And a visualization: After I review the observations I add new rules. The first one sets adaptive thresholds that check the row count is between the smallest of the last 10 runs and the largest of the last 20 runs. The second one looks for unusual patters, for example RowCount being abnormally high on weekends: Join the preview This new capability is available in preview in the US East (Ohio, N. Virginia), US West (Oregon), Asia Pacific (Tokyo), and Europe (Ireland) AWS Regions. To learn more, read Data Quality Anomaly Detection . Stay tuned for a detailed blog post when this feature launches! — Jeff ;', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='21d8859d-ecf7-4097-81f0-c033fc0ab3b1', embedding=None, metadata={'title': 'Amazon CloudWatch Logs now offers automated pattern analytics and anomaly detection', 'summary': 'Amazon CloudWatch can now automatically recognize and cluster patterns among log records, extract noteworthy content and trends, and notify you of anomalies using advanced machine learning algorithms.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Searching through log data to find operational or business insights often feels like looking for a needle in a haystack. It usually requires you to manually filter and review individual log records. To help you with that, Amazon CloudWatch has added new capabilities to automatically recognize and cluster patterns among log records, extract noteworthy content and trends, and notify you of anomalies using advanced machine learning (ML) algorithms trained using decades of Amazon and AWS operational data. Specifically, CloudWatch now offers the following: The Patterns tab on the Logs Insights page finds recurring patterns in your query results and lets you analyze them in detail. This makes it easier to find what you’re looking for and drill down into new or unexpected content in your logs. The Compare button in the time interval selector on the Logs Insights page lets you quickly compare the query result for the selected time range to a previous period, such as the previous day, week, or month. In this way, it takes less time to see what has changed compared to a previous stable scenario. The Log Anomalies page in the Logs section of the navigation pane automatically surfaces anomalies found in your logs while they are processed during ingestion. Let’s see how these work in practice with a typical troubleshooting journey. I will look at some application logs to find key patterns, compare two time periods to understand what changed, and finally see how detecting anomalies can help discover issues. Finding recurring patterns in the logs In the CloudWatch console , I choose Logs Insights from the Logs section of the navigation pane. To start, I have selected which log groups I want to query. In this case, I select a log group of a Lambda function that I want to inspect and choose Run query . In the Pattern tab, I see the patterns that have been found in these log groups. One of the patterns seems to be an error. I can select it to quickly add it as a filter to my query and focus on the logs that contain this pattern. For now, I choose the magnifying glass icon to analyze the pattern. In the Pattern inspect window, a histogram with the occurrences of the pattern in the selected time period is shown. After the histogram, samples from the logs are provided. The variable parts of the pattern (such as numbers) have been extracted as “tokens.” I select the Token values tab to see the values for a token.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='42e22a2c-fd7c-4216-b6c1-7ce809d5ea36', embedding=None, metadata={'title': 'Amazon CloudWatch Logs now offers automated pattern analytics and anomaly detection', 'summary': 'Amazon CloudWatch can now automatically recognize and cluster patterns among log records, extract noteworthy content and trends, and notify you of anomalies using advanced machine learning algorithms.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='I can select a token value to quickly add it as a filter to the query and focus on the logs that contain this pattern with this specific value. I can also look at the Related patterns tab to see other logs that typically occurred at the same time as the pattern I am analyzing. For example, if I am looking at an ERROR log that was always written alongside a DEBUG log showing more details, I would see that relationship there. Comparing logs with a previous period To better understand what is happening, I choose the Compare button in the time interval selector. This updates the query to compare results with a previous period. For example, I choose Previous day to see what changed compared to yesterday. In the Patterns tab, I notice that there has actually been a 10 percent decrease in the number of errors, so the current situation might not be too bad. I choose the magnifying glass icon on the pattern with severity type ERROR to see a full comparison of the two time periods. The graph overlaps the occurrences of the pattern over the two periods (now and yesterday in this case) inside the selected time range (one hour). Errors are decreasing but are still there. To reduce those errors, I make some changes to the application. I come back after some time to compare the logs, and a new ERROR pattern is found that was not present in the previous time period. My update probably broke something, so I roll back to the previous version of the application. For now, I’ll keep it as it is because the number of errors is acceptable for my use case. Detecting anomalies in the log I am reassured by the decrease in errors that I discovered comparing the logs. But how can I know if something unexpected is happening? Anomaly detection for CloudWatch Logs looks for unexpected patterns in the logs as they are processed during ingestion and can be enabled at log group level. I select Log groups in the navigation pane and type a filter to see the same log group I was looking at before. I choose Configure in the Anomaly detection column and select an Evaluation frequency of 5 minutes . Optionally, I can use a longer interval (up to 60 minutes) and add patterns to process only specific log events for anomaly detection. After I activate anomaly detection for this log group, incoming logs are constantly evaluated against historical baselines.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='23b22411-2b34-479f-9a6c-9aedc5b57f03', embedding=None, metadata={'title': 'Amazon CloudWatch Logs now offers automated pattern analytics and anomaly detection', 'summary': 'Amazon CloudWatch can now automatically recognize and cluster patterns among log records, extract noteworthy content and trends, and notify you of anomalies using advanced machine learning algorithms.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='I wait for a few minutes and, to see what has been found, I choose Log anomalies from the Logs section of the navigation pane. To simplify this view, I can suppress anomalies that I am not interested in following. For now, I choose one of the anomalies in order to inspect the corresponding pattern in a way similar to before. After this additional check, I am convinced there are no urgent issues with my application. With all the insights I collected with these new capabilities, I can now focus on the errors in the logs to understand how to solve them. Things to know Amazon CloudWatch automated log pattern analytics is available today in all commercial AWS Regions where Amazon CloudWatch Logs is offered excluding the China (Beijing), the China (Ningxia), and Israel (Tel Aviv) Regions. The patterns and compare query features are charged according to existing Logs Insights query costs. Comparing a one-hour time period against another one-hour time period is equivalent to running a single query over a two-hour time period. Anomaly detection is included as part of your log ingestion fees, and there is no additional charge for this feature. For more information, see CloudWatch pricing . Simplify how you analyze logs with CloudWatch automated log pattern analytics. — Danilo', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='1ae6473d-0731-411e-a7dc-5f9d728bf967', embedding=None, metadata={'title': 'AWS Step Functions Workflow Studio is now available in AWS Application Composer', 'summary': 'This new integration brings together the development of workflows and application resources into a unified visual infrastructure as code (IaC) builder.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, we’re announcing\\xa0that AWS Step Functions Workflow Studio is now available in AWS Application Composer . This new integration brings together the development of workflows and application resources into a unified visual infrastructure as code (IaC) builder. Now, you can have a seamless transition between authoring workflows with AWS Step Functions Workflow Studio and defining resources with AWS Application Composer. This announcement allows you to create and manage all resources at any stage of your development journey. You can visualize the full application in AWS Application Composer, then zoom into the workflow details with AWS Step Functions Workflow Studio—all within a single interface. Seamlessly build workflow and modern application To help you design and build modern applications, we launched AWS Application Composer in March 2023. With AWS Application Composer, you can use a visual builder to compose and configure serverless applications from AWS services backed by deployment-ready IaC. In various use cases of building modern applications, you may also need to orchestrate microservices, automate mission-critical business processes, create event-driven applications that respond to infrastructure changes, or build machine learning (ML) pipelines. To solve these challenges, you can use AWS Step Functions, a fully managed service that makes it easier to coordinate distributed application components using visual workflows. To simplify workflow development, in 2021 we introduced AWS Step Functions Workflow Studio ,\\xa0a low-code visual tool for rapid workflow prototyping and development across 12,000+ API actions from over 220 AWS services. While AWS Step Functions Workflow Studio brings simplicity to building workflows, customers that want to deploy workflows using IaC had to manually define their state machine resource and migrate their workflow definitions to the IaC template. Better together: AWS Step Functions Workflow Studio in AWS Application Composer With this new integration, you can now design AWS Step Functions workflows in AWS Application Composer using a drag-and-drop interface. This accelerates the path from prototyping to production deployment and iterating on existing workflows. You can start by composing your modern application with AWS Application Composer. Within the canvas, you can add a workflow by adding an AWS Step Functions state machine resource. This new capability provides you with the ability to visually design and build a workflow with an intuitive interface to connect workflow steps to resources. How it works Let me walk you through how you can use AWS Step Functions Workflow Studio in AWS Application Composer.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='75360c5e-74bb-4f12-9e03-1327468cc1c8', embedding=None, metadata={'title': 'AWS Step Functions Workflow Studio is now available in AWS Application Composer', 'summary': 'This new integration brings together the development of workflows and application resources into a unified visual infrastructure as code (IaC) builder.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='For this demo, let’s say that I need to improve handling e-commerce transactions by building a workflow and integrating with my existing serverless APIs. First, I navigate to AWS Application Composer.\\xa0Because I already have an existing project that includes application code and IaC templates from AWS Application Composer, I don’t need to build anything from scratch. I open the\\xa0menu\\xa0and select Project folder to open the files in my local development machine. Then, I select the path of my local folder, and AWS Application Composer automatically detects the IaC template that I currently have. Then, AWS Application Composer visualizes the diagram in the canvas. What I really like about using this approach is that AWS Application Composer activates Local sync mode, which automatically syncs and saves any changes in IaC templates into my local project. Here, I have a simple serverless API running on Amazon API Gateway, which invokes an AWS Lambda function and integrates with Amazon DynamoDB. Now, I’m ready to make some changes to my serverless API. I configure another route on Amazon API Gateway and add AWS Step Functions state machine to start building my workflow. When I configure my Step Functions state machine, I can start editing my workflow by selecting Edit in Workflow Studio . This opens Step Functions Workflow Studio within the AWS Application Composer canvas. I have the same experience as Workflow Studio in the AWS Step Functions console. I can use the canvas to add\\xa0actions,\\xa0flows\\xa0, and\\xa0patterns\\xa0into my Step Functions state machine. I start building my workflow, and here’s the result that I exported using Export PNG image\\xa0in Workflow Studio. But here’s where this new capability really helps me as a developer. In the workflow definition, I use various AWS resources, such as AWS Lambda functions and Amazon DynamoDB. If I need to reference the AWS resources I defined in AWS Application Composer, I can use an AWS CloudFormation substitution. With AWS CloudFormation substitutions, I can add a substitution using an AWS CloudFormation convention, which is a dynamic reference to a value that is provided in the IaC template. I am using a placeholder substitution here so I can map it with an AWS resource in the AWS Application Composer canvas in a later step. I can also define the AWS CloudFormation substitution for my Amazon DynamoDB table. At this stage, I’m happy with my workflow.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='fb441c31-bd29-4ca7-b2e4-169d01325af0', embedding=None, metadata={'title': 'AWS Step Functions Workflow Studio is now available in AWS Application Composer', 'summary': 'This new integration brings together the development of workflows and application resources into a unified visual infrastructure as code (IaC) builder.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='To review the Amazon States Language as my AWS Step Functions state machine definition, I can also open the Code tab. Now I don’t need to manually copy and paste this definition into IaC templates. I only need to\\xa0save\\xa0my work and choose Return to Application Composer . Here, I can see that my AWS Step Functions state machine is updated both in the visual diagram and in the\\xa0state machine definition\\xa0section. If I scroll down, I will find AWS Cloudformation Definition Substitutions for resources that I defined in Workflow Studio. I can manually replace the mapping here, or I can use the canvas. To use the canvas, I simply drag and drop the respective resources in my Step Functions state machine and in the Application Composer canvas. Here, I connect the Inventory Process task state with a new AWS Lambda function. Also, my Step Functions state machine tasks can reference existing resources. When I choose Template , the state machine definition is integrated with other AWS Application Composer resources. With this IaC template I can easily deploy using AWS Serverless Application Model Command Line Interface (AWS SAM CLI) or CloudFormation. Things to know Here is some additional information for you: Pricing – The AWS Step Functions Workflow Studio in AWS Application Composer comes at no additional cost. Availability – This feature is available in all AWS Regions where Application Composer is available. AWS Step Functions Workflow Studio in AWS Application Composer provides you with an easy-to-use experience to integrate your workflow into modern applications. Get started and learn more about this feature on the AWS Application Composer page. Happy building! — Donnie', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='1e2e6d04-3368-4087-ba73-5471002dceed', embedding=None, metadata={'title': 'Announcing throughput increase and dead letter queue redrive support for Amazon SQS FIFO queues', 'summary': 'With Amazon Simple Queue Service, you can send, store, and receive messages between software components at any volume. Today, we’ve introduced two new capabilities for first-in, first-out (FIFO) queues.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='With Amazon Simple Queue Service (Amazon SQS) , you can send, store, and receive messages between software components at any volume. Today, Amazon SQS has introduced two new capabilities for first-in, first-out (FIFO) queues: Maximum throughput has been increased up to 70,000 transactions per second (TPS) per API action in selected AWS Regions , supporting sending or receiving up to 700,000 messages per second with batching. Dead letter queue (DLQ) redrive support to handle messages that are not consumed after a specific number of retries in a way similar to what was already available for standard queues. Let’s take a more in-depth look at how these work in practice. FIFO queues throughput increase up to 70K TPS FIFO queues are designed for applications that require messages to be processed exactly once and in the order in which they are sent. While standard queues have an unlimited throughput, FIFO queues have an upper quota in the number of TPS per API action. Standard and FIFO queues support batch actions that can send and receive up to 10 messages with a single API call (up to a maximum total payload of 256 KB). This means that a FIFO queue can process up to 10 times more messages per second than its maximum throughput. At launch in 2016 , FIFO queues supported up to 300 TPS per API action (3,000 messages per second with batching). This was enough for many use cases, but some customers asked for more throughput. With high throughput mode launched in 2021, FIFO queues introduced a tenfold increase of the maximum throughput and could process up to 3,000 TPS per API action, depending on the Region. One year later, that quota was doubled to up to 6,000 TPS per API action. This year, Amazon SQS has already increased FIFO queue throughput quota two times, to up to 9,000 TPS per API action in August and up to 18,000 TPS per API action in October (depending on the Region). Today, the Amazon SQS team has been able to increase the FIFO queue throughput quota again, allowing you to process up to 70,000 TPS per API action (up to 700,000 messages per second with batching) in the US East (N. Virginia), US West (Oregon), and Europe (Ireland) Regions. This is more than two hundred times the maximum throughput at launch.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='9ffd7709-f210-4885-8a34-b8ae75ab7971', embedding=None, metadata={'title': 'Announcing throughput increase and dead letter queue redrive support for Amazon SQS FIFO queues', 'summary': 'With Amazon Simple Queue Service, you can send, store, and receive messages between software components at any volume. Today, we’ve introduced two new capabilities for first-in, first-out (FIFO) queues.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='DLQ redrive support for FIFO queues With Amazon SQS, messages that are not consumed after a specific number of retries can automatically be moved to a DLQ. There, messages can be analyzed to understand the reason why they have not been processed correctly. Sometimes there is a bug or a misconfiguration in the consumer application. Other times the messages contain invalid data from the source applications that needs to be fixed to allow the messages to be processed again. Either way, you can define a plan to reprocess these messages. For example, you can fix the consumer application and redrive all messages to the source queue. Or you can create a dedicated queue where a custom application receives the messages, fixes their content, and then sends them to the source queue. To simplify moving the messages back to the source queue or to a different queue, Amazon SQS allows you to create a redrive task. Redrive tasks are already available for standard queues. Starting today, you can also start a redrive task for FIFO queues. Using the Amazon SQS console , I create a first queue ( my-dlq.fifo ) to be used as a DLQ. To redrive messages back to the source FIFO queue, the queue type must match, so this is also a FIFO queue. Then, I create a source FIFO queue ( my-source-queue.fifo ) to handle messages as usual. When I create the source queue, I configure the first queue ( my-dlq.fifo ) as the DLQ and specify 3 as the Maximum receives condition under which messages are moved from the source queue to the DLQ. When a message has been received by a consumer for more than the number of times specified by this condition, Amazon SQS moves the message to the DLQ. The original message ID is retained and can be used to uniquely track the message. To test this setup, I use the console to send a message to the source queue. Then, I use the AWS Command Line Interface (AWS CLI) to receive the message multiple times without deleting it.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='5bcba706-0ccc-493c-98b5-1745482fb33b', embedding=None, metadata={'title': 'Announcing throughput increase and dead letter queue redrive support for Amazon SQS FIFO queues', 'summary': 'With Amazon Simple Queue Service, you can send, store, and receive messages between software components at any volume. Today, we’ve introduced two new capabilities for first-in, first-out (FIFO) queues.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='aws sqs receive-message --queue-url https://sqs.eu-west-1.amazonaws.com/123412341234/my-source-queue.fifo {\\n    \"Messages\": [\\n        {\\n            \"MessageId\": \"ef2f1c72-4bfe-4093-a451-03fe2dbd4d0f\",\\n            \"ReceiptHandle\": \"...\",\\n            \"MD5OfBody\": \"0f445a578fbcb0c06ca8aeb90a36fcfb\",\\n            \"Body\": \"My important message.\"\\n        }\\n    ]\\n} To receive the same message more than once, I wait for the time specified in the queue visibility timeout to pass (30 seconds by default). After the third time, the message is not in the source queue because it has been moved to the DLQ. When I try to receive messages from the source queue, the list is empty. aws sqs receive-message --queue-url https://sqs.eu-west-1.amazonaws.com/123412341234/my-source-queue.fifo {\\n    \"Messages\": []\\n} To confirm that the message has been moved, I poll the DLQ to see if the message is there. aws sqs receive-message --queue-url https://sqs.eu-west-1.amazonaws.com/123412341234/my-dlq.fifo {\\n    \"Messages\": [\\n        {\\n            \"MessageId\": \"ef2f1c72-4bfe-4093-a451-03fe2dbd4d0f\",\\n            \"ReceiptHandle\": \"...\",\\n            \"MD5OfBody\": \"0f445a578fbcb0c06ca8aeb90a36fcfb\",\\n            \"Body\": \"My important message.\"\\n        }\\n    ]\\n} Now that the message is in the DLQ, I can investigate why the message has not been processed (well, I know the reason this time) and decide whether to redrive messages from the DLQ using the Amazon SQS console or the new redrive API that was introduced a few months ago . For this example, I use the console. Back on the Amazon SQS console , I select the DLQ queue and choose Start DLQ redrive . In Redrive configuration , I choose to redrive the messages to the source queue. Optionally, I can specify another FIFO queue as a custom destination.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f18cebcf-c59e-4e9f-9da1-6713abda05dc', embedding=None, metadata={'title': 'Announcing throughput increase and dead letter queue redrive support for Amazon SQS FIFO queues', 'summary': 'With Amazon Simple Queue Service, you can send, store, and receive messages between software components at any volume. Today, we’ve introduced two new capabilities for first-in, first-out (FIFO) queues.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='I use System optimized in Velocity control settings to redrive messages with the maximum number of messages per second optimized by Amazon SQS. Optionally, if there is a large number of messages in the DLQ, I can configure a custom maximum rate of messages per second to avoid overloading consumers. Before starting the redrive task, I can use the Inspect messages section to poll and check messages. I already decided what to do, so I choose DLQ redrive to start the task. I have only one message to process, so the redrive task completes very quickly. As expected, the message is back in the source queue and is ready to be processed again. Things to know Dead letter queue (DLQ) support for FIFO queues is available today in all AWS Regions where Amazon SQS is offered with the exception of GovCloud Regions and those based in China. In the DLQ configuration, the maximum number of receives should be between 1 and 1,000. There is no additional cost for using high throughput mode or a DLQ. Every Amazon SQS action counts as a request. A single request can send or receive from 1 to 10 messages, up to a maximum total payload of 256 KB. You pay based on the number of requests, and requests are priced differently between standard and FIFO queues. As part of the AWS Free Tier , there is no cost for the first million requests per month for standard queues and for the first million requests per month for FIFO queues. For more information, see Amazon SQS pricing . With these updates and the increased throughput, you can cover the vast majority of use cases with FIFO queues. Use Amazon SQS FIFO queues to have high throughput, exactly-once processing, and first-in-first-out delivery. — Danilo', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='1d611e9d-72c6-4fcc-b734-5824af926d74', embedding=None, metadata={'title': 'Manage EDI at scale with new AWS B2B Data Interchange', 'summary': 'Now, organizations can automate and monitor the transformation of electronic data interchange-based business-critical transactions at cloud scale.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today we’re launching AWS B2B Data Interchange , a fully managed service allowing organizations to automate and monitor the transformation of EDI-based business-critical transactions at cloud scale. With this launch, AWS brings automation, monitoring, elasticity, and pay-as-you-go pricing to the world of B2B document exchange. Electronic data interchange (EDI) is the electronic exchange of business documents in a standard electronic format between business partners. While email is also an electronic approach, the documents exchanged via email must still be handled by people rather than computer systems. Having people involved slows down the processing of the documents and also introduces errors. Instead, EDI documents can flow straight through to the appropriate application on the receiver’s system, and processing can begin immediately. Electronic documents exchanged between computer systems help businesses reduce cost, accelerate transactional workflows, reduce errors, and improve relationships with business partners. Work on EDI started in the 1970s. I remember reading a thesis about EDIFACT , a set of standards defining the structure of business documents, back in 1994. But despite being a more than 50-year-old technology, traditional self-managed EDI solutions deployed to parse, validate, map, and translate data from business applications to EDI data formats are difficult to scale as the volume of business changes. They typically do not provide much operational visibility into communication and content errors. These challenges often oblige businesses to fall back to error-prone email document exchanges, leading to high manual work, increased difficulty controlling compliance, and ultimately constraining growth and agility. AWS B2B Data Interchange is a fully managed, easy-to-use, and cost-effective service for accelerating your data transformations and integrations. It eliminates the heavy lifting of establishing connections with your business partners and mapping the documents to your system’s data-formats and gives visibility on documents that can’t be processed. It provides a low-code interface for business partner onboarding and EDI data transformation to easily import the processed data to your business applications and analytics solutions. B2B Data Interchange gives you easy access to monitoring data, allowing you to build dashboards to monitor the volume of documents exchanged and the status of each document transformation. For example, it is easy to create alarms when incorrectly formatted documents can’t be transformed or imported into your business applications.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='89a7d656-e1ba-49ca-879f-daa476fac566', embedding=None, metadata={'title': 'Manage EDI at scale with new AWS B2B Data Interchange', 'summary': 'Now, organizations can automate and monitor the transformation of electronic data interchange-based business-critical transactions at cloud scale.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='It is common for large enterprises to have thousands of business partners and hundreds of types of documents exchanged with each partner, leading to millions of combinations to manage. AWS B2B Data Interchange is not only available through the AWS Management Console , it is also accessible with the AWS Command Line Interface (AWS CLI) and AWS SDKs . This allows you to write applications or scripts to onboard new business partners and their specific data transformations and to programmatically add alarms and monitoring logic to new or existing dashboards. B2B Data Interchange supports the X12 EDI data format. It makes it easier to validate and transform EDI documents to the formats expected by your business applications, such as JSON or XML. The raw documents and the transformed JSON or XML files are stored on Amazon Simple Storage Service (Amazon S3) . This allows you to build event-driven applications for real-time business data processing or to integrate business documents with your existing analytics or AI/ML solutions. For example, when you receive a new EDI business document, you can trigger additional routing, processing, and transformation logic using AWS Step Functions or Amazon EventBridge . When an error is detected in an incoming document, you can configure the sending of alarm messages by email or SMS or trigger an API call or additional processing logic using AWS Lambda . Let’s see how it works As usual on this blog, let me show you how it works. Let’s imagine I am in charge of the supply chain for a large retail company, and I have hundreds of business partners to exchange documents such as bills of lading , customs documents, advanced shipment notices , invoices, or receiving advice certificates . In this demo, I use the AWS Management Console to onboard a new business partner. By onboarding, I mean defining the contact details of the business partner, the type of documents I will exchange with them, the technical data transformation to the JSON formats expected by my existing business apps, and where to receive the documents. With this launch, the configuration of the transport mechanism for the EDI document is managed outside B2B Data Interchange. Typically, you will configure a transfer gateway and propose that your business partner transfer the document using SFTP or AS2 . There are no servers to manage or application packages to install and configure. I can get started in just four steps. First, I create a profile for my business partner. Second, I create a transformer.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='4d92adb6-073b-42bc-89e0-16ce76732689', embedding=None, metadata={'title': 'Manage EDI at scale with new AWS B2B Data Interchange', 'summary': 'Now, organizations can automate and monitor the transformation of electronic data interchange-based business-critical transactions at cloud scale.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='A transformer defines the source document format and the mapping to my existing business application data format: JSON or XML. I can use the graphical editor to validate a sample document and see the result of the transformation directly from the console. We use the standard JSONATA query and transformation language to define the transformation logic to JSON documents and standard XSLT when transforming to XML documents. I activate the transformer once created. Third, I create a trading capability. This defines which Amazon Simple Storage Service (Amazon S3) buckets will receive the documents from a specific business partner and where the transformed data will be stored. There is a one-time additional configuration to make sure proper permissions are defined on the S3 bucket policy. I select Copy policy and navigate to the Amazon S3 page of the console to apply the policies to the S3 bucket. One policy allows B2B Data Interchange to read from the incoming bucket, and one policy allows it to write to your outgoing bucket. While I am configuring the S3 bucket, it is also important to turn on Amazon EventBridge on the S3 bucket. This is the mechanism we use to trigger the data transformation upon the arrival of a new business document. Finally, back at the B2B Data Interchange configuration, I create a partnership. Partnerships are dedicated resources that establish a relationship between you and your individual trading partners. Partnerships contain details about a specific trading partner, the types of EDI documents you receive from them, and how those documents should be transformed into custom JSON or XML formats. A partnership links the business profile I created in the first step with one or multiple document types and transformations I defined in step two. This is also where I can monitor the status of the last set of documents I received and the status of their transformation. For more historical data, you can navigate to Amazon CloudWatch using the links provided in the console. To test my setup, I upload an EDI 214 document to the incoming bucket and a few seconds later, I can see the transformed JSON document appearing in the destination bucket. I can observe the status of document processing and transformation using Invocations and TriggeredRules CloudWatch metrics from EventBridge. From there, together with the CloudWatch Logs, I can build dashboards and configure alarms as usual.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='5c3ca8ee-c392-4593-b7a9-86a1b883f338', embedding=None, metadata={'title': 'Manage EDI at scale with new AWS B2B Data Interchange', 'summary': 'Now, organizations can automate and monitor the transformation of electronic data interchange-based business-critical transactions at cloud scale.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='I can also configure additional enrichment, routing, and processing of the incoming or transformed business documents by writing an AWS Lambda function or a workflow using AWS Step Functions . Pricing and availability AWS B2B Data Interchange is available today in three of the AWS Regions: US East (Ohio, N. Virginia) and US West (Oregon). There is no one-time setup fee or recurring monthly subscription. AWS charges you on demand based on your real usage. There is a price per partnership per month and a price per document transformed. The B2B Data Interchange pricing page has the details . AWS B2B Data Interchange makes it easy to manage your trading partner relationships so you can automatically exchange, transform, and monitor EDI workflows at cloud scale. It doesn’t require you to install or manage any infrastructure and makes it easy for you to integrate with your existing business applications and systems. You can use the AWS B2B Data Interchange API or the AWS SDK to automate the onboarding of your partners. Combined with a fully managed and scalable infrastructure, AWS B2B Data Interchange helps your business to be more agile and scale your operations. Learn more: AWS B2B Data Interchange web page Log in to the console Go build! -- seb', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='b07e98a7-bf8a-4baa-b051-1b62cd7bbf55', embedding=None, metadata={'title': 'Easily deploy SaaS products with new Quick Launch in AWS Marketplace', 'summary': 'SaaS Quick Launch helps buyers make the deployment process easy, fast, and secure by offering step-by-step instructions and resource deployment using preconfigured AWS CloudFormation templates'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today we are excited to announce the general availability of SaaS Quick Launch, a new feature in AWS Marketplace that makes it easy and secure to deploy SaaS products. Before SaaS Quick Launch, configuring and launching third-party SaaS products could be time-consuming and costly, especially in certain categories like security and monitoring. Some products require hours of engineering time to manually set up permissions policies and cloud infrastructure. Manual multistep configuration processes also introduce risks when buyers rely on unvetted deployment templates and instructions from third-party resources. SaaS Quick Launch helps buyers make the deployment process easy, fast, and secure by offering step-by-step instructions and resource deployment using preconfigured AWS CloudFormation templates. The software vendor and AWS validate these templates to ensure that the configuration adheres to the latest AWS security standards. Getting started with SaaS Quick Launch It’s easy to find which SaaS products have Quick Launch enabled when you are browsing in AWS Marketplace. Products that have this feature configured have a Quick Launch tag in their description. After completing the purchase process for a Quick Launch–enabled product, you will see a button to set up your account. That button will take you to the Configure and launch page, where you can complete the registration to set up your SaaS account, deploy any required AWS resources, and launch the SaaS product. The first step ensures that your account has the required AWS permissions to configure the software. The second step involves configuring the vendor account, either to sign in to an existing account or to create a new account on the vendor website. After signing in, the vendor site may pass essential keys and parameters that are needed in the next step to configure the integration. The third step allows you to configure the software and AWS integration. In this step, the vendor provides one or more CloudFormation templates that provision the required AWS resources to configure and use the product. The final step is to launch the software once everything is configured. Availability Sellers can enable this feature in their SaaS product. If you are a seller and want to learn how to set this up in your product, check the Seller Guide for detailed instructions. To learn more about SaaS in AWS Marketplace, visit the service page and view all the available SaaS products currently in AWS Marketplace . — Marcia', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='396be11a-e038-4b4d-84ca-f05ea0ffb104', embedding=None, metadata={'title': 'Check your AWS Free Tier usage programmatically with a new API', 'summary': 'You can use the API directly with the AWS Command Line Interface or integrate it into an application with the AWS SDKs.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Starting today, you can check your usage of the AWS Free Tier using the new AWS Free Tier API . You can use the API directly with the AWS Command Line Interface (AWS CLI) or integrate it into an application with the AWS SDKs . The AWS Free Tier program provides you with the ability to explore and try out AWS services free of charge up to specified limits for each service. The AWS Free Tier includes three different types of offerings : Always free offers allow customers to use a service for free up to specified limits as long as they are an AWS customer. 12 months free offers allow customers to use a service for free up to specified limits for one year from the date the account was activated. Short-term trials are free to use for a specified period or up to a one-time limit, depending on the service. Once you begin turning on AWS resources and interacting with AWS services that offer a free tier, you need to keep track of your progress toward the free tier limit so that you know when to expect to switch to pay-as-you-go pricing. There are a few ways you can keep track of your AWS Free Tier usage: The usage alerts in the Billing preferences of the AWS Billing and Cost Management console are enabled by default (unless the account was created via AWS Organizations ) and send you emails when you exceed 85 percent of the Free Tier limit for each service. You can create a zero-spend or a monthly-cost budget in the Budgets section of the Billing and Cost Management console . Using templates, it just requires a couple of clicks and the email address to notify. The Free Tier page in the Billing and Cost Management console tells you the service, the type of offer, the current usage, and the forecasted usage for each offer in the current billing period. The new GetFreeTierUsage API provides the same information in the Free Tier page with a structured format that you can use programmatically. Let’s see how this new API works in practice. Using the AWS Free Tier API with the AWS CLI I got access to a new account created in the last few months. Here, I use the AWS Command Line Interface (AWS CLI) to call the GetFreeTierUsage API. aws freetier get-free-tier-usage The response is a JSON document that contains a description of the current usage for each offer that is applicable to this account during this billing period. For simplicity, I only show a few offers here.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='2baf1360-811f-48c2-9c9e-355c1a393d94', embedding=None, metadata={'title': 'Check your AWS Free Tier usage programmatically with a new API', 'summary': 'You can use the API directly with the AWS Command Line Interface or integrate it into an application with the AWS SDKs.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='{\\n    \"freeTierUsages\": [\\n        {\\n            \"service\": \"Amazon Simple Queue Service\",\\n            \"operation\": \"\",\\n            \"usageType\": \"Requests\",\\n            \"region\": \"global\",\\n            \"actualUsageAmount\": 294387.0,\\n            \"forecastedUsageAmount\": 679354.6153846154,\\n            \"limit\": 1000000.0,\\n            \"unit\": \"Requests\",\\n            \"description\": \"1000000.0 Requests are always free per month as part of AWS Free Usage Tier (Global-Requests)\",\\n            \"freeTierType\": \"Always Free\"\\n        },\\n        {\\n            \"service\": \"Amazon Elastic Compute Cloud\",\\n            \"operation\": \"\",\\n            \"usageType\": \"EBS:VolumeUsage\",\\n            \"region\": \"global\",\\n            \"actualUsageAmount\": 9.0,\\n            \"forecastedUsageAmount\": 33.0,\\n            \"limit\": 30.0,\\n            \"unit\": \"GB-Mo\",\\n            \"description\": \"30.0 GB-Mo for free for 12 months as part of AWS Free Usage Tier (Global-EBS:VolumeUsage)\",\\n            \"freeTierType\": \"12 Months Free\"\\n        },\\n        {\\n            \"service\": \"Amazon Elastic Compute Cloud\",\\n            \"operation\": \"RunInstances:0002\",\\n            \"usageType\": \"BoxUsage:freetier.micro\",\\n            \"region\": \"global\",\\n            \"actualUsageAmount\": 476.0,\\n            \"forecastedUsageAmount\": 851.0,\\n            \"limit\": 750.0,\\n            \"unit\": \"Hrs\",\\n            \"description\": \"750.0 Hrs for free for 12 months as part of AWS Free Usage Tier (Global-BoxUsage:freetier.micro)\",\\n            \"freeTierType\": \"12 Months Free\"\\n        },\\n        {\\n            \"service\": \"Amazon Elastic Compute Cloud\",\\n            \"operation\": \"RunInstances\",\\n            \"usageType\": \"BoxUsage:freetier.micro\",\\n            \"region\": \"global\",\\n            \"actualUsageAmount\": 225.0,\\n            \"forecastedUsageAmount\": 485.0,\\n            \"limit\": 750.0,\\n            \"unit\": \"Hrs\",\\n            \"description\": \"750.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='3a7b4b8f-6129-440f-b9fc-c03c2425d25b', embedding=None, metadata={'title': 'Check your AWS Free Tier usage programmatically with a new API', 'summary': 'You can use the API directly with the AWS Command Line Interface or integrate it into an application with the AWS SDKs.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='0 Hrs for free for 12 months as part of AWS Free Usage Tier (Global-BoxUsage:freetier.micro)\",\\n            \"freeTierType\": \"12 Months Free\"\\n        },\\n        {\\n            \"service\": \"Amazon Redshift\",\\n            \"operation\": \"RunComputeNode:0001\",\\n            \"usageType\": \"Node:dc2.large\",\\n            \"region\": \"global\",\\n            \"actualUsageAmount\": 367.0,\\n            \"forecastedUsageAmount\": 735.0,\\n            \"limit\": 750.0,\\n            \"unit\": \"Hrs\",\\n            \"description\": \"750.0 Hrs for free per month during a short-term trial as part of AWS Free Usage Tier (Global-Node:dc2.large)\",\\n            \"freeTierType\": \"Free Trial\"\\n        },\\n        .\\n    ]\\n} In the freeTierUsages list, I find some of the most common offers: Two compute offers for Amazon Elastic Compute Cloud (Amazon EC2) . The offer with operation RunInstances:0002 is for Windows. The offer with operation RunInstances is for Linux. The value of the operation property is the same as the platform details and usage operation displayed on the Instances or AMIs pages in the Amazon EC2 console . For more information, see the AMI billing information fields in the Amazon EC2 User Guide . One storage offer for Amazon Elastic Block Store (Amazon EBS) volumes. This and the two Amazon EC2 compute offers have freeTierType equal to 12 Months Free . An Always Free offer for Amazon Simple Queue Service (Amazon SQS) . A Free Trial (short-term) offer for Amazon Redshift . Let’s have a look at some properties of these offers: description gives a readable explanation of what the offer is about. freeTierType tells the type of offer: Always Free , 12 Months Free , or Free Trial (short-term). unit describes the unit used to measure usage for the offer. For example, Hrs (hours) for EC2 instances, GB-Mo (GB per month) for EBS volumes, Requests for Amazon SQS, and so on.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e2a61541-f0b8-448b-b4c9-7ebe39344453', embedding=None, metadata={'title': 'Check your AWS Free Tier usage programmatically with a new API', 'summary': 'You can use the API directly with the AWS Command Line Interface or integrate it into an application with the AWS SDKs.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Three interesting properties are the limit of the offer ( limit ), the actual usage amount of the offer ( actualUsageAmount ), and the forecasted usage amount ( forecastedUsageAmount ) at the end of the billing period (the current month). They are all based on the unit used by the offer. For example, the Windows and Linux compute offers each have a limit of 750 hours per month. For the storage offer, the limit is 30 GB per month. For Amazon SQS, the limit of the offer is one million requests per month. Details on the limits and services provided for free are detailed in each card on the AWS Free Tier page and on the pricing page of each service. The actual and forecast usage amounts provided by the AWS Free Tier API are estimated up to three times per day, similar to AWS Cost and Usage Reports . If the forecast usage is greater than the limit for the offer, I should expect to switch to pay-as-you-go pricing before the end of the billing period if I continue to use the service in the same way. Actual usage is no longer tracked by the GetFreeTierUsage API once the limit is reached. This means that the actual usage amount cannot be greater than its limit. If that’s the case, the corresponding offer is not returned by the API.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='7285fbd4-55f8-4cde-b69c-bb755c71b319', embedding=None, metadata={'title': 'Check your AWS Free Tier usage programmatically with a new API', 'summary': 'You can use the API directly with the AWS Command Line Interface or integrate it into an application with the AWS SDKs.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='For example, I look for the offers for which the forecast is greater than the limit using the --query option of the AWS CLI: aws freetier get-free-tier-usage --query \\'freeTierUsages[?forecastedUsageAmount > limit]\\' {\\n    \"freeTierUsages\": [\\n        {\\n            \"service\": \"Amazon Elastic Compute Cloud\",\\n            \"operation\": \"\",\\n            \"usageType\": \"EBS:VolumeUsage\",\\n            \"region\": \"global\",\\n            \"actualUsageAmount\": 9.0,\\n            \"forecastedUsageAmount\": 33.0,\\n            \"limit\": 30.0,\\n            \"unit\": \"GB-Mo\",\\n            \"description\": \"30.0 GB-Mo for free for 12 months as part of AWS Free Usage Tier (Global-EBS:VolumeUsage)\",\\n            \"freeTierType\": \"12 Months Free\"\\n        },\\n        {\\n            \"service\": \"Amazon Elastic Compute Cloud\",\\n            \"operation\": \"RunInstances:0002\",\\n            \"usageType\": \"BoxUsage:freetier.micro\",\\n            \"region\": \"global\",\\n            \"actualUsageAmount\": 476.0,\\n            \"forecastedUsageAmount\": 851.0,\\n            \"limit\": 750.0,\\n            \"unit\": \"Hrs\",\\n            \"description\": \"750.0 Hrs for free for 12 months as part of AWS Free Usage Tier (Global-BoxUsage:freetier.micro)\",\\n            \"freeTierType\": \"12 Months Free\"\\n        }\\n    ]\\n} According to this result, if I want to stay within the Free Tier limits, I can check how I use EBS volumes and Amazon EC2 compute with Windows. For example, I am currently using 476 hours out of the 750 available in a month for Windows EC2 instances. At this pace, I am forecasted to cross the limit and reach about 851 hours. If I am concerned by the costs, I can switch off my Windows instances when not in use or during the night. Things to know Previously, the Free Tier API was not publicly available and was used internally for the Free Tier page in the AWS Billing console , where you can find the same data. We hope that making the GetFreeTierUsage API publicly available can help you have fun with AWS, have better use of the AWS Free Tier offers, and be aware of what is free and what to do when you get close to or over a limit.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='8d77e21d-02a1-4b0e-85e7-4d55c8a2989d', embedding=None, metadata={'title': 'Check your AWS Free Tier usage programmatically with a new API', 'summary': 'You can use the API directly with the AWS Command Line Interface or integrate it into an application with the AWS SDKs.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Using this information, you can build custom reporting that meets your business needs. For example, if you want to avoid compute costs, you can programmatically stop or hibernate your EC2 instances or set the size of an EC2 Auto Scaling group to zero. You can use any of the AWS SDKs to create a web app or integrate this data in a monitoring solution. More generally, you can send additional emails or notifications (for example, using Amazon SES or Amazon SNS ) when the usage of an offer is close to its limit. This can help you get the maximum benefit of an offer without incurring additional costs. You can also do this with AWS Budgets if you set a usage budget amount to the Free Tier limit. If an offer is no longer applicable to this account (for example, because it expired at the end of the previous month), the corresponding item is not included in the list. If you save the results from previous invocations of the API, you can compare the list of offers with those reported during the previous billing cycle to see which offers have recently expired. To learn more about keeping track of your AWS Free Tier usage, we created these three 10-minute courses on AWS Skill Builder , an online learning center where you can learn from AWS experts and build cloud skills online: AWS Free Tier: Introduction to Offerings AWS Free Tier: Introduction to Monitoring Services AWS Free Tier: Introduction to Managing Services — Danilo', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='9a537cb3-3942-4e6f-877a-4e1bfa851b03', embedding=None, metadata={'title': 'New Cost Optimization Hub centralizes recommended actions to save you money', 'summary': 'This new AWS Billing and Cost Management feature makes it easy for you to identify, filter, aggregate, and quantify savings for AWS cost optimization recommendations.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, we are announcing Cost Optimization Hub , a new AWS Billing and Cost Management feature that makes it easy for you to identify, filter, aggregate, and quantify savings for AWS cost optimization recommendations. With the new Cost Optimization Hub, you can interactively query cost optimization recommendations such as idle resource detection, resource rightsizing, and purchasing options across multiple AWS Regions and AWS accounts in your organizations without any data aggregation and processing. You can find out how much you’ll save if you implement those recommendations and easily compare and prioritize recommendations by savings. Andy Jassy, CEO of Amazon, told shareholders, “We’re trying to build customer relationships (and a business) that outlast all of us, and as a result, our AWS sales and support teams are spending much of their time helping customers optimize their AWS spend so they can better weather this uncertain economy” in his 2022 Letter to Shareholders . Cost Optimization Hub gathers all cost-optimizing recommended actions across AWS Cloud Financial Management (CFM) services , including AWS Cost Explorer and AWS Compute Optimizer , in one place. It incorporates customer-specific pricing and discounts into these recommendations, and it deduplicates findings and savings to give a consolidated view of your cost optimization opportunities. If you are a FinOps team or infrastructure management team member who wants to understand your cost optimization opportunities in aggregate, such as which AWS accounts or AWS Regions have the most cost optimization opportunities, you should start with Cost Optimization Hub. You can easily analyze cost optimization opportunities with built-in filters and grouping options. For example, after understanding which AWS account has the most cost optimization opportunities, you can identify the top cost optimization strategies, such as stopping idle resources, rightsizing, and Graviton migration. If you identify which AWS Region has the highest number of rightsizing opportunities, you can get a list of rightsizing recommendations for the Region. It will redirect you to the Compute Optimizer console through deep linking to validate the details, such as the projected CPU utilization if you implement the change. Getting started with Cost Optimization Hub To get started, choose Cost Optimization Hub in the left navigation menu of the AWS Billing and Cost Management Console . You can opt in by selecting Enable . There is a 24-hour wait time for Cost Optimization Hub to populate data initially and data will be refreshed daily afterward. After opt-in, you can see the dashboard of cost optimization recommendations by AWS account, AWS Region, and tag key.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='81556788-0a85-4a71-bfa0-ecf4b23e19b7', embedding=None, metadata={'title': 'New Cost Optimization Hub centralizes recommended actions to save you money', 'summary': 'This new AWS Billing and Cost Management feature makes it easy for you to identify, filter, aggregate, and quantify savings for AWS cost optimization recommendations.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='If you want to see the list of resources available for optimization, choose View opportunities . Cost Optimization Hub supports six types of cost-optimizing recommended actions, including: Stop – Stop idle or unused resources to save up to 100 percent of the resources’ cost. Rightsize – Move to a smaller Amazon EC2 instance type, Amazon EBS volume, AWS Lambda memory size, or AWS Fargate task size Upgrade – Move to a later-generation product, such as moving from EBS io1 volume type to io2. Graviton migration – Move from EC2 instance types with x86-based processors to EC2 instance types with AWS Graviton-based processors to save costs. Purchase Savings Plans – Purchase Compute Savings Plans, EC2 Instance Savings Plans, and Amazon SageMaker Savings Plans Purchase Reserved Instances – Purchase Amazon EC2, Amazon RDS, Amazon ElastiCache, and Amazon Redshift Reserved Instances. You can see the resource type, top recommended action, and estimated monthly savings. You can also filter the list by AWS account, AWS Region, implementation effort, and tag key as the group-by dimension. You also can classify each recommendation as “Is resources restart needed” or “Is rollback possible.” If you specify Is resources restart needed=No as the filter, you can only see recommendations that don’t require you to restart your resources, such as EBS volume recommendations. Similarly, If you specify Is rollback possible=Yes as the filter, you can only see recommendations that can be rolled back. If you select a specific source, for example, right-sizing EC2 instance, you can view details and connect to Amazon EC2 and the AWS Compute Optimizer console. Note that estimated monthly savings is a quick approximation of future savings. The actual savings you will realize are dependent on your future AWS usage patterns. You can also interactively query through AWS Command Line Interface (AWS CLI) and AWS SDKs. \\xa0Here’s a sample query to find the recommendations about deleting and rightsizing resources: $ aws cost-optimization-hub list-recommendations The preceding query gives you the following results: {\\n   \"items\":[\\n      {\\n         \"recommendationId\":\"MDA2MDI1ODQ1MTA1XzQ5MzNhYzZlLWZmYTUtNGI2ZC04YzBkLTAxYWE3Y2JlNjNlYg==\",', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='bd8f9fd4-39e2-446c-90c1-4030ccba997c', embedding=None, metadata={'title': 'New Cost Optimization Hub centralizes recommended actions to save you money', 'summary': 'This new AWS Billing and Cost Management feature makes it easy for you to identify, filter, aggregate, and quantify savings for AWS cost optimization recommendations.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\"accountId\":\"006025845105\",\\n         \"region\":\"Global\",\\n         \"resourceId\":\"006025845105_ComputeSavingsPlans\",\\n         \"currentResourceType\":\"ComputeSavingsPlans\",\\n         \"recommendedResourceType\":\"ComputeSavingsPlans\",\\n         \"estimatedMonthlySavings\":1506.591472696,\\n         \"estimatedSavingsPercentage\":55.46400024,\\n         \"estimatedMonthlyCost\":2716.341169146,\\n         \"currencyCode\":\"USD\",\\n         \"implementationEffort\":\"VeryLow\",\\n         \"restartNeeded\":false,\\n         \"actionType\":\"PurchaseSavingsPlans\",\\n         \"rollbackPossible\":false,\\n         \"recommendedResourceSummary\":\"$1.628/hour with three years term\",\\n         \"lastRefreshTimestamp\":\"2023-10-23T16:54:13-07:00\",\\n         \"recommendationLookbackPeriodInDays\":30,\\n         \"source\":\"CostExplorer\"\\n      },\\n      {\\n         \"recommendationId\":\"MDA2MDI1ODQ1MTA1XzhiZTRlNTczLTE0MDctNGIzOS05MmY3LTdmN2EzOTU2Y2ZkYw==\",\\n         \"accountId\":\"006025845105\",\\n         \"region\":\"us-east-1\",\\n         \"resourceId\":\"arn:aws:lambda:us-east-1:006025845105:function:Lambda-recommendation-testing:$LATEST\",\\n         \"resourceArn\":\"arn:aws:lambda:us-east-1:006025845105:function:Lambda-recommendation-testing:$LATEST\",\\n         \"currentResourceType\":\"LambdaFunction\",\\n         \"recommendedResourceType\":\"LambdaFunction\",\\n         \"estimatedMonthlySavings\":3.1682091425308054e-06,\\n         \"estimatedSavingsPercentage\":1.936368871741565,\\n         \"estimatedMonthlyCost\":0.00016044778307703665,\\n         \"currencyCode\":\"USD\",\\n         \"implementationEffort\":\"Low\",\\n         \"restartNeeded\":false,\\n         \"actionType\":\"Rightsize\",\\n         \"rollbackPossible\":true,\\n         \"currentResourceSummary\":\"128 MB memory\",\\n         \"recommendedResourceSummary\":\"160 MB memory\",\\n         \"lastRefreshTimestamp\":\"2023-10-24T04:07:35.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='bd122d28-5c14-4af8-b085-ada995749777', embedding=None, metadata={'title': 'New Cost Optimization Hub centralizes recommended actions to save you money', 'summary': 'This new AWS Billing and Cost Management feature makes it easy for you to identify, filter, aggregate, and quantify savings for AWS cost optimization recommendations.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='364000-07:00\",\\n         \"recommendationLookbackPeriodInDays\":14,\\n         \"source\":\"ComputeOptimizer\"\\n      }\\n   ]\\n} For more information about new Cost Optimization Hub APIs, see the Cost Optimization Hub documentation . Now available Cost Optimization Hub is now generally available for all customers. There is no additional charge for this new capability. You can now get started and view cost optimization recommendations across all AWS Regions. To learn more, see the Cost Optimization Hub page and send feedback to AWS re:Post for Cost Optimization or through your usual AWS Support contacts. — Channy', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='5ae84016-79a3-46be-9c3f-3ba8ce719316', embedding=None, metadata={'title': 'Join the preview for new memory-optimized, AWS Graviton4-powered Amazon EC2 instances (R8g)', 'summary': 'Equipped with brand-new Graviton4 processors, the new R8g instances will deliver better price performance than any existing memory-optimized instance.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='We are opening up a preview of the next generation of Amazon Elastic Compute Cloud (Amazon EC2) instances. Equipped with brand-new Graviton4 processors, the new R8g instances will deliver better price performance than any existing memory-optimized instance. The R8g instances are suitable for your most demanding memory-intensive workloads: big data analytics, high-performance databases, in-memory caches and so forth. Graviton history Let’s take a quick look back in time and recap the evolution of the Graviton processors: November 2018 – The Graviton processor made its debut in the A1 instances, optimized for both performance and cost, and delivering cost reductions of up to 45% for scale-out workloads. December 2019 – The Graviton2 processor debuted with the announcement of M6g, M6gd, C6g, C6gd, R6g, and R6gd instances with up to 40% better price performance than equivalent non-Graviton instances. The second-generation processor delivered up to 7x performance of the first one, including twice the floating point performance. November 2021 – The Graviton3 processor made its debut with the announcement of the compute-optimized C7g instances. In addition to up to 25% better compute performance, this generation of processors once again doubled floating point and cryptographic performance when compared to the previous generation. November 2022 – The Graviton 3E processor was announced , for use in the Hpc7g and C7gn instances, with up to 35% higher vector instruction processing performance than the Graviton3. Today, every one of the top 100 Amazon Elastic Compute Cloud (EC2) customers makes use of Graviton, choosing between more than 150 Graviton-powered instances. New Graviton4 I’m happy to be able to tell you about the latest in our series of innovative custom chip designs, the energy-efficient AWS Graviton4 processor. 96 Neoverse V2 cores, 2 MB of L2 cache per core, and 12 DDR5-5600 channels work together to make the Graviton4 up to 40% faster for databases, 30% faster for web applications, and 45% faster for large Java applications than the Graviton3.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='120d41b3-9839-4f00-8b61-10e9b3452e5f', embedding=None, metadata={'title': 'Join the preview for new memory-optimized, AWS Graviton4-powered Amazon EC2 instances (R8g)', 'summary': 'Equipped with brand-new Graviton4 processors, the new R8g instances will deliver better price performance than any existing memory-optimized instance.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Graviton4 processors also support all of the security features from the previous generations, and includes some important new ones including encrypted high-speed hardware interfaces and Branch Target Identification (BTI). R8g instance sizes The 8th generation R8g instances will be available in multiple sizes with up to triple the number of vCPUs and triple the amount of memory of the 7th generation (R7g) of memory-optimized, Graviton3-powered instances. Join the preview R8g instances with Graviton4 processors — Jeff ;', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='df7091ca-807b-41a3-a66e-33438a380bd4', embedding=None, metadata={'title': 'Introducing Amazon EC2 high memory U7i Instances for large in-memory databases (preview)', 'summary': 'The new U7i instances are designed to support large, in-memory databases including SAP HANA, Oracle, and SQL Server.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='The new U7i instances are designed to support large, in-memory databases including SAP HANA, Oracle, and SQL Server. Powered by custom fourth generation Intel Xeon Scalable Processors (Sapphire Rapids), the instances are now available in multiple AWS regions in preview form, in the US West (Oregon), Asia Pacific (Seoul), and Europe (Frankfurt) AWS Regions, as follows: Instance Name vCPUs Memory (DDR5) EBS Bandwidth Network Bandwidth u7in-16tb.224xlarge 896 16,384 GiB 100 Gbps 100 Gbps u7in-24tb.224xlarge 896 24,576 GiB 100 Gbps 100 Gbps u7in-32tb.224xlarge 896 32,768 GiB 100 Gbps 100 Gbps We are also working on a smaller instance: Instance Name vCPUs Memory (DDR5) EBS Bandwidth Network Bandwidth u7i-12tb.224xlarge 896 12,288 GiB 60 Gbps 100 Gbps Here’s what 32 TiB of memory looks like: And here are the 896 vCPUs (and lots of other info): When compared to the first generation of High Memory instances, the U7i instances offer up to 125% more compute performance and up to 120% more memory performance. They also provide 2.5x as much EBS bandwidth, giving you the ability to hydrate in-memory databases at a rate of up to 44 terabytes per hour. Each U7i instance supports attachment of up to 128 General Purpose (gp2 and gp3) or Provisioned IOPS (io1 and io2 Block Express ) EBS volumes. Each io2 Block Express volume can be as big as 64 TiB and can deliver up to 256K IOPS at up to 32 Gbps, making them a great match for the U7i instance. On the network side, the instances support ENA Express and deliver up to 25 Gbps of bandwidth per network flow. Supported operating systems include Red Hat Enterprise Linux and SUSE Enterprise Linux Server. Join the Preview If you are ready to put the U7i instances to the test in your environment, join the preview . — Jeff ;', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='9f8b37b4-c991-4d3a-9d55-46bf22932c04', embedding=None, metadata={'title': 'Amazon Managed Service for Prometheus collector provides agentless metric collection for Amazon EKS', 'summary': 'This new capability discovers and collects Prometheus metrics from Amazon Elastic Kubernetes Service (Amazon EKS) automatically and without an agent.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, I’m happy to announce a new capability, Amazon Managed Service for Prometheus collector, to automatically and agentlessly discover and collect Prometheus metrics from Amazon Elastic Kubernetes Service (Amazon EKS) . Amazon Managed Service for Prometheus collector consists of a scraper that discovers and collects metrics from Amazon EKS applications and infrastructure without needing to run any collectors in-cluster. This new capability provides fully managed Prometheus-compatible monitoring and alerting with Amazon Managed Service for Prometheus. One of the significant benefits is that the collector is fully managed, automatically right-sized, and scaled for your use case. This means you don’t have to run any compute for collectors to collect the available metrics. This helps you optimize metric collection costs to monitor your applications and infrastructure running on\\xa0EKS. With this launch, Amazon Managed Service for Prometheus now supports two major modes of Prometheus metrics collection: AWS managed collection, a fully managed and agentless collector, and customer managed collection. Getting started with Amazon Managed Service for Prometheus Collector Let’s take a look at how to use AWS managed collectors to ingest metrics using this new capability into a workspace in Amazon Managed Service for Prometheus. Then, we will evaluate the collected metrics in Amazon Managed Grafana . When you create a new EKS cluster using the Amazon EKS console, you now have the option to enable AWS managed collector by selecting Send Prometheus metrics to Amazon Managed Service for Prometheus . In the Destination section, you can also create a new workspace or select your existing Amazon Managed Service for Prometheus workspace. You can learn more about how to create a workspace by following the getting started guide . Then, you have the flexibility to define your\\xa0scraper configuration\\xa0using the editor or upload your existing configuration. The scraper configuration controls how you would like the scraper to discover and collect metrics. To see possible values you can configure, please visit the Prometheus Configuration page. Once you’ve finished the EKS cluster creation, you can go to the Observability tab on your cluster page to see the list of scrapers running in your EKS cluster. The next step is to configure your EKS cluster to allow the scraper to access metrics. You can find the steps and information on Configuring your Amazon EKS cluster . Once your EKS cluster is properly configured, the collector will automatically discover metrics from your EKS cluster and nodes. To visualize the metrics, you can use Amazon Managed Grafana integrated with your Prometheus workspace.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='bd225e67-c372-41c3-b2ae-6970462fbe4a', embedding=None, metadata={'title': 'Amazon Managed Service for Prometheus collector provides agentless metric collection for Amazon EKS', 'summary': 'This new capability discovers and collects Prometheus metrics from Amazon Elastic Kubernetes Service (Amazon EKS) automatically and without an agent.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Visit the Set up Amazon Managed Grafana for use with Amazon Managed Service for Prometheus page to learn more. The following is a screenshot of metrics ingested by the collectors and visualized in an Amazon Managed Grafana workspace. From here, you can run a simple query to get the metrics that you need. Using AWS CLI and APIs Besides using the Amazon EKS console, you can also use the APIs or AWS Command Line Interface (AWS CLI) to add an AWS managed collector. This approach is useful if you want to add an AWS managed collector into an existing\\xa0EKS\\xa0cluster or make some modifications to the existing collector configuration. To create a scraper, you can run the following command: aws amp create-scraper \\\\ \\n       --source eksConfiguration=\"{clusterArn=<EKS-CLUSTER-ARN>,securityGroupIds=[<SG-SECURITY-GROUP-ID>],subnetIds=[<SUBNET-ID>]}\" \\\\ \\n       --scrape-configuration configurationBlob=<BASE64-CONFIGURATION-BLOB> \\\\ \\n       --destination ampConfiguration={workspaceArn=\"<WORKSPACE_ARN>\"} You can get most of the parameter values from the respective AWS console, such as your EKS cluster ARN and your Amazon Managed Service for Prometheus workspace ARN. Other than that, you also need to define the scraper configuration defined as configurationBlob . Once you’ve defined the scraper configuration, you need to encode the configuration file into base64 encoding before passing the API call. The following is the command that I use in my Linux development machine to encode sample-configuration.yml into base64 and copy it onto the clipboard. $ base64 sample-configuration.yml | pbcopy Now Available The Amazon Managed Service for Prometheus collector capability is now available to all AWS customers in all AWS Regions where Amazon Managed Service for Prometheus is supported. Learn more: Amazon Managed Service for Prometheus product page AWS managed collectors documentation page Happy building! — Donnie', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='a294f21d-fd80-4966-b433-d342c5dd2f0b', embedding=None, metadata={'title': 'Amazon EKS Pod Identity simplifies IAM permissions for applications on Amazon EKS clusters', 'summary': 'This enhancement lets you define required IAM permissions for your applications in Amazon Elastic Kubernetes Service clusters so you can connect with AWS services outside the cluster.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Starting today, you can use Amazon EKS Pod Identity to simplify your applications that access AWS services. This enhancement provides you with a seamless and easy to configure experience that lets you define required IAM permissions for your applications in Amazon Elastic Kubernetes Service (Amazon EKS) clusters so you can connect with AWS services outside the cluster. Amazon EKS Pod Identity helps you solve growing challenges for managing permissions across many of your EKS clusters. Simplifying experience with Amazon EKS Pod Identity In 2019, we introduced IAM roles for service accounts (IRSA) . IRSA lets you associate an IAM role with a Kubernetes service account. This helps you to implement the principle of least privilege by giving pods only the permissions they need. This approach prioritizes pods in IAM and helps developers configure applications with fine-grained permissions that enable the least privileged access to AWS services. Now, with Amazon EKS Pod Identity, it’s even easier to configure and automate granting AWS permissions to Kubernetes identities. As the cluster administrator, you no longer need to switch between Amazon EKS and IAM services to authenticate your applications to all AWS resources. The overall workflow to start using Amazon EKS Pod Identity can be summarized in a few simple steps: Step 1: Create an IAM role with required permissions for your application and specify pods.eks.amazonaws.com as the service principal in its trust policy. Step 2: Install Amazon EKS Pod Identity Agent add-on using the Amazon EKS console or AWS Command Line Interface (AWS CLI). Step 3: Map the role to a service account directly in the Amazon EKS console, APIs, or AWS CLI. Once it’s done, any new pods that use that service account will automatically be configured to receive IAM credentials. Let’s get started Let me show you how you can get started with EKS Pod Identity. For the demo in this post, I need to configure permission for a simple API running in my Amazon EKS cluster, which will return the list of files in my Amazon Simple Storage Service (Amazon S3) bucket. First, I need to create an IAM role to provide the required permissions so my applications can run properly. In my case, I need to configure permissions to access my S3 bucket. Next, on the same IAM role, I need to configure its trust policy and configure the principal to pods.eks.amazonaws.com .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='381b83b5-896e-43bf-8e3c-f29fdc77aece', embedding=None, metadata={'title': 'Amazon EKS Pod Identity simplifies IAM permissions for applications on Amazon EKS clusters', 'summary': 'This enhancement lets you define required IAM permissions for your applications in Amazon Elastic Kubernetes Service clusters so you can connect with AWS services outside the cluster.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='The following is the IAM template that I use: {\\n    \"Version\": \"2012-10-17\",\\n    \"Statement\": [\\n        {\\n            \"Effect\": \"Allow\",\\n            \"Principal\": {\\n                \"Service\": \"pods.eks.amazonaws.com\"\\n            },\\n            \"Action\": [\\n                \"sts:AssumeRole\",\\n                \"sts:TagSession\"\\n            ]\\n        }\\n    ]\\n} At this stage, my IAM role is ready, and now we need to configure the Amazon EKS Pod Identity Agent in my cluster. For this article, I’m using my existing EKS cluster. If you want to learn how to do that, visit Getting started with Amazon EKS . Moving on, I navigate to the Amazon EKS dashboard and then select my EKS cluster. In my EKS cluster page, I need to select the Add-ons tab and then choose Get more add-ons . Then, I need to add the Amazon EKS Pod Identity Agent add-on. On the next page, I can add additional configuration if needed. In this case, I leave the default configuration and choose Next . Then, I just need to review my add-on configuration and choose Create . After a few minutes, the Amazon EKS Pod Identity Agent add-on is active for my cluster. Once I have Amazon EKS Pod Identity in my cluster, I need to associate the IAM role to my Kubernetes pods. I need to navigate to the Access tab in my EKS cluster. On the Pod Identity associations section, I select Create Pod Identity association to map my IAM role to Kubernetes pods. Here, I use the IAM role that I created in the beginning. I also need to define my Kubernetes namespace and service account. If they don’t exist yet, I can type in the name of the namespace and service account. If they already exist, I can select them from the dropdown. Then, I choose Create . Those are all the steps I need to do to configure IAM permissions for my applications running on Amazon EKS with EKS Pod Identity. Now, I can see my IAM role is listed in Pod Identity associations . When I test my API running on Amazon EKS, it runs as expected and returns the list of files in my S3 bucket.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='79236176-d2ee-4eeb-aa0d-aac0fabd7422', embedding=None, metadata={'title': 'Amazon EKS Pod Identity simplifies IAM permissions for applications on Amazon EKS clusters', 'summary': 'This enhancement lets you define required IAM permissions for your applications in Amazon Elastic Kubernetes Service clusters so you can connect with AWS services outside the cluster.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='curl -X https://<API-URL> -H \"Accept: application/json\" \\n\\n{\\n   \"files\": [\\n         \"test-file-1.md\",\\n         \"test-file-2.md\"\\n    ]        \\n} I found that Amazon EKS Pod Identity simplifies the experience of managing IAM roles for my applications running on Amazon EKS. I can easily reuse IAM roles across multiple EKS clusters without needing to update the role trust policy each time a new cluster is created. New AWS APIs to configure EKS Pod Identity You also have the flexibility to configure Amazon EKS Pod Identity for your cluster using AWS CLI. Amazon EKS Pod Identity provides a new set of APIs that you can use. For example, I can use aws eks create-addon to install the Amazon EKS Pod Identity Agent add-on into my cluster. Here’s the AWS CLI command: $ aws eks create-addon \\\\\\n--cluster-name <CLUSTER_NAME> \\\\\\n--addon-name eks-pod-identity-agent \\\\\\n--addon-version v1.0.0-eksbuild.1\\n\\n{\\n    \"addon\": {\\n    \"addonName\": \"eks-pod-identity-agent\",\\n    \"clusterName\": \"<CLUSTER_NAME>\",\\n    \"status\": \"CREATING\",\\n    \"addonVersion\": \"v1.0.0-eksbuild.1\",\\n    \"health\": {\\n        \"issues\": []\\n        },\\n    \"addonArn\": \"<ARN>\",\\n    \"createdAt\": 1697734297.597,\\n    \"modifiedAt\": 1697734297.612,\\n    \"tags\": {}\\n    }\\n} Another example of what you can do with AWS APIs is to map the IAM role into your Kubernetes pods. $ aws eks create-pod-identity-association \\\\\\n  --cluster-name <CLUSTER_NAME> \\\\\\n  --namespace <NAMESPACE> \\\\\\n  --service-account <SERVICE_ACCOUNT_NAME> \\\\\\n  --role-arn <IAM_ROLE_ARN> Things to know Availability – Amazon EKS Pod Identity is available in all AWS Regions supported by Amazon EKS, except the AWS GovCloud (US-East), AWS GovCloud (US-West), China (Beijing, operated by Sinnet), and China (Ningxia, operated by NWCD). Pricing – Amazon EKS Pod Identity is available at no charge. Supported Amazon EKS cluster – Amazon EKS Pod Identity supports Kubernetes running version 1.24 and above in Amazon EKS.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='a0b469ca-ef95-449e-a416-a116a1dbc4a9', embedding=None, metadata={'title': 'Amazon EKS Pod Identity simplifies IAM permissions for applications on Amazon EKS clusters', 'summary': 'This enhancement lets you define required IAM permissions for your applications in Amazon Elastic Kubernetes Service clusters so you can connect with AWS services outside the cluster.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='You can see EKS Pod Identity cluster versions for more information. Supported AWS SDK versions –\\xa0You need to update your application to use the latest AWS SDK versions. Check out AWS developer tools to find out how to install and update your AWS SDK. Get started today and visit EKS Pod Identities documentation page to learn more about how to simplify IAM management for your applications. Happy building! — Donnie', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='3b9c877b-6c0e-499a-a564-8ea88949d0b0', embedding=None, metadata={'title': 'Increase collaboration and securely share cloud knowledge with AWS re:Post Private', 'summary': 're:Post Private includes content tailored specifically for your organization’s use cases, along with private discussion and collaboration forums for the members of your organization and your AWS account team.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today we’re launching AWS re:Post Private , a fully managed knowledge service to accelerate cloud adoption, improve productivity, and drive innovation. re:Post Private allows organizations to increase collaboration and access knowledge resources built for your cloud community. It includes curated collections of technical content and training materials from AWS. The content is tailored specifically for your organization’s use cases, along with private discussion and collaboration forums for the members of your organization and your AWS account team. As its name implies, you can think of it as a private version of AWS re:Post , with private content and access limited to people that belong to your organization and your AWS Account team. Organizations of all sizes and verticals are increasingly moving their operations to the cloud. To ensure cloud adoption success, organizations must have the right skills and structure in place. The optimal way to achieve this is by setting up a centralized cloud center of excellence (CCOE). A CCOE is a centralized governance function for the organization and acts in a consultative role for central IT, business-unit IT, and cloud service consumers in the business. According to Gartner , a CCOE has three pillars: governance, brokerage, and community. The community pillar establishes the cloud community of practice (COP) that brings together stakeholders and facilitates cloud collaboration. It helps organizations adapt themselves for cloud adoption by promoting COP member interaction and facilitating cloud-related training and skills development. AWS re:Post Private facilitates the creation, structure, and management of an internal cloud community of practice. It allows you to build a custom knowledge base that is searchable, reusable, and scalable. It allows community members to post private questions and answers and publish articles. It combines the benefits of traditional forums, such as community discussion and collaboration, with the benefits of an integrated information experience. AWS re:Post Private is a fully managed service: there is no need to operate complex knowledge management and collaboration technologies or to develop custom solutions. AWS re:Post Private also facilitates your interactions with AWS Support . You can create a support case directly from your private re:Post, and you can convert case resolution to reusable knowledge visible to all in your organization. You choose in which AWS Region re:Post Private stores your data and who has access. All data at rest and in transit is encrypted using industry-standard algorithms. Your administrator chooses between using AWS-managed encryption keys or keys you manage and control.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='8ca5d765-b967-4e63-9d59-c26770d98176', embedding=None, metadata={'title': 'Increase collaboration and securely share cloud knowledge with AWS re:Post Private', 'summary': 're:Post Private includes content tailored specifically for your organization’s use cases, along with private discussion and collaboration forums for the members of your organization and your AWS account team.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Your organization’s Technical Account Managers are automatically added to your private re:Post. You can select other persons to invite among your organization and AWS teams, such as your AWS Solutions Architect. Only your private re:Post administrators need an AWS account. All other users can federate from your organization’s identity provider, such as Microsoft Active Directory. Let’s see how to create a re:Post Private To get started with AWS re:Post Private, as an administrator, I point my browser to the re:Post section of the AWS Management Console . I select Create private re:Post and enter the information needed to create a private re:Post for my organization, my team, or my project. I can choose the Data encryption parameters and whether or not I enable Service access for Support case integration . When I’m ready, I select Create this re:Post . Once the private re:Post is created, I can grant access to users and groups. User and group information comes from AWS IAM Identity Center and your identity provider. Invited users receive an email inviting them to connect to the private re:Post and create their profile. That’s pretty much it for the administrator part. Once the private re:Post is created, I receive an endpoint name that I can share with the rest of my organization. Let’s see how to use re:Post Private As a member of the organization, I navigate to re:Post Private using the link I received from the administrator. I authenticate with the usual identity service of my organization, and I am redirected to the re:Post Private landing page. On the top menu, I can select a tab to view the contents for Questions , Community Articles , Selections , Tags , Topics , Community Groups , or My Dashboard . This should be familiar if you already use the public knowledge service AWS re:Post that adopted a similar structure. Further down on the page, I see the popular topics and the top contributors in my organization.I also have access to Questions and Community Groups . I can search the available content by keyword, tags, author, and so on. Pricing and availability You can create your organization’s AWS re:Post Private in the following AWS Regions: US West (Oregon) and Europe (Frankfurt). AWS re:Post Private is available to customers having an AWS Enterprise or Enterprise On-Ramp support plan .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f1613438-c003-458d-b8c3-9611d1e65ba0', embedding=None, metadata={'title': 'Increase collaboration and securely share cloud knowledge with AWS re:Post Private', 'summary': 're:Post Private includes content tailored specifically for your organization’s use cases, along with private discussion and collaboration forums for the members of your organization and your AWS account team.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='re:Post Private offers a free tier that allows you to explore and try out standard capabilities for six months. There is no limit on the number of users in the free tier, and content storage is limited to 10 GB. When you reach the free storage limit, the plan is converted to the paid standard tier. With AWS re:Post Private Standard tier, you only pay for what you use. We charge based on the number of users per month. Please visit the re:Post Private pricing page for more information. Get started today and activate AWS re:Post Private for your organization. -- seb', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='566626df-1e2b-47e6-9b84-05257171ca96', embedding=None, metadata={'title': 'Amazon Redshift adds new AI capabilities, including Amazon Q, to boost efficiency and productivity', 'summary': 'Now you can get SQL recommendations from natural language prompts, and Redshift now scales capacity proactively and automatically to deliver tailored performance optimizations.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Amazon Redshift puts artificial intelligence (AI) at your service to optimize efficiencies and make you more productive with two new capabilities that we are launching in preview today. First, Amazon Redshift Serverless becomes smarter. It scales capacity proactively and automatically along dimensions such as the complexity of your queries, their frequency, the size of the dataset, and so on to deliver tailored performance optimizations. This allows you to spend less time tuning your data warehouse instances and more time getting value from your data. Second, Amazon Q generative SQL in Amazon Redshift Query Editor generates SQL recommendations from natural language prompts. This helps you to be more productive in extracting insights from your data. Let’s start with Amazon Redshift Serverless When you use Amazon Redshift Serverless, you can now opt in for a preview of AI-driven scaling and optimizations. When enabled, the system observes and learns from your usage patterns, such as the concurrent number of queries, their complexity, and the time it takes to run them. Then, it automatically optimizes your serverless endpoint to meet your price performance target. Based on AWS internal testing, this new capability may give you up to ten times better price performance for variable workloads without any manual intervention. AI-driven scaling and optimizations eliminate the time and effort to manually resize your workgroup and plan background optimizations based on workload needs. It continually runs automatic optimizations when they are most valuable for better performance, avoiding performance cliffs and time-outs. This new capability goes beyond the existing self-tuning capabilities of Amazon Redshift Serverless , such as machine learning (ML)-enhanced techniques to adjust your compute, modify the physical schema of the database, create or drop materialized views as needed (the one we manage automatically , not yours), and vacuum tables. This new capability brings more intelligence to decide how to adjust the compute, what background optimizations are required, and when to apply them, and it makes its decisions based on more dimensions. We also orchestrate ML-based optimizations for materialized views , table optimizations , and workload management when your queries need it. During the preview, you must opt in to enable these AI-driven scaling and optimizations on your workgroups . You configure the system to balance the optimization for price or performance. There is only one slider to adjust in the console. As usual, you can track resource usage and associated changes through the console, Amazon CloudWatch metrics, and the system table SYS_SERVERLESS_USAGE .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='78d0cf3e-637a-4488-b7df-b2b406313848', embedding=None, metadata={'title': 'Amazon Redshift adds new AI capabilities, including Amazon Q, to boost efficiency and productivity', 'summary': 'Now you can get SQL recommendations from natural language prompts, and Redshift now scales capacity proactively and automatically to deliver tailored performance optimizations.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Now, let’s look at Amazon Q generative SQL in Amazon Redshift Query Editor What if you could use generative AI to help analysts write effective SQL queries more rapidly? This is the new experience we introduce today in Amazon Redshift Query Editor , our web-based SQL editor. You can now describe the information you want to extract from your data in natural language, and we generate the SQL query recommendations for you. Behind the scenes, Amazon Q generative SQL uses a large language model (LLM) and Amazon Bedrock to generate the SQL query. We use different techniques, such as prompt engineering and Retrieval Augmented Generation (RAG) , to query the model based on your context: the database you’re connected to, the schema you’re working on, your query history, and optionally the query history of other users connected to the same endpoint. The system also remembers previous questions. You can ask it to refine a previously generated query. The SQL generation model uses metadata specific to your data schema to generate relevant queries. For example, it uses the table and column names and the relationship between the tables in your database. In addition, your database administrator can authorize the model to use the query history of all users in your AWS account to generate even more relevant SQL statements. We don’t share your query history with other AWS accounts and we don’t train our generation models with any data coming from your AWS account. We maintain the high level of privacy and security that you expect from us. Using generated SQL queries helps you to get started when discovering new schemas. It does the heavy lifting of discovering the column names and relationships between tables for you. Senior analysts also benefit from asking what they want in natural language and having the SQL statement automatically generated. They can review the queries and run them directly from their notebook. Let’s explore a schema and extract information For this demo, let’s pretend I am a data analyst at a company that sells concert tickets. The database schema and data are available for you to download. My manager asks me to analyze the ticket sales data to send a thank you note with discount coupons to the highest-spending customers in Seattle. I connect to Amazon Redshift Query Editor and connect the analytic endpoint. I create a new tab for a Notebook (SQL generation is available from notebooks only).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='adc81072-1626-4af2-960e-bf76e39fc9ce', embedding=None, metadata={'title': 'Amazon Redshift adds new AI capabilities, including Amazon Q, to boost efficiency and productivity', 'summary': 'Now you can get SQL recommendations from natural language prompts, and Redshift now scales capacity proactively and automatically to deliver tailored performance optimizations.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Instead of writing a SQL statement, I open the chat panel and type, “ Find the top five users from Seattle who bought the most number of tickets in 2022. ” I take the time to verify the generated SQL statement. It seems correct, so I decide to run it. I select Add to notebook and then Run . The query returns the list of the top five buyers in Seattle. I had no previous knowledge of the data schema, and I did not type a single line of SQL to find the information I needed. But generative SQL is not limited to a single interaction. I can chat with it to dynamically refine the queries. Here is another example. I ask “ Which state has the most venues? ” Generative SQL proposes the following query. The answer is New York, with 49 venues, if you’re curious. I changed my mind, and I want to know the top three cities with the most venues. I simply rephrase my question: “ What about the top three venues? ” I add the query to the notebook and run it. It returns the expected result. Best practices for prompting Here are a couple of tips and tricks to get the best results out of your prompts. Be specific – When asking questions in natural language, be as specific as possible to help the system understand exactly what you need. For example, instead of writing “find the top venues that sold the most tickets,” provide more details like “find the names of the top three venues that sold the most tickets in 2022.” Use consistent entity names like venue, ticket, and location instead of referring to the same entity in different ways, which can confuse the system. Iterate – Break your complex requests into multiple simple statements that are easier for the system to interpret. Iteratively ask follow-up questions to get more detailed analysis from the system. For example, start by asking, “Which state has the most venues?” Then, based on the response, ask a follow-up question like “Which is the most popular venue from this state?” Verify – Review the generated SQL before running it to ensure accuracy. If the generated SQL query has errors or does not match your intent, provide instructions to the system on how to correct it instead of rephrasing the entire request.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='76f66e0a-0ef6-4e91-a4c8-05137a85f51b', embedding=None, metadata={'title': 'Amazon Redshift adds new AI capabilities, including Amazon Q, to boost efficiency and productivity', 'summary': 'Now you can get SQL recommendations from natural language prompts, and Redshift now scales capacity proactively and automatically to deliver tailored performance optimizations.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='For example, if the query is missing a filter clause on year, write “provide venues from year 2022.” Availability and pricing AI-driven scaling and optimizations are in preview in six AWS Regions: US East (Ohio, N. Virginia), US West (Oregon), Asia Pacific (Tokyo), and Europe (Ireland, Stockholm). They come at no additional cost. You pay only for the compute capacity your data warehouse consumes when it is active. Pricing is per Redshift Processing Unit (RPU) per hour. The billing is per second of used capacity. The pricing page for Amazon Redshift has the details. Amazon Q generative SQL for Amazon Redshift Query Editor is in preview in two AWS Regions today: US East (N. Virginia) and US West (Oregon). There is no charge during the preview period. These are two examples of how AI helps to optimize performance and increase your productivity, either by automatically adjusting the price-performance ratio of your Amazon Redshift Serverless endpoints or by generating correct SQL statements from natural language prompts. Previews are essential for us to capture your feedback before we make these capabilities available for all. Experiment with these today and let us know what you think on the re:Post forums or using the feedback button on the bottom left side of the console. -- seb', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='7f2baf31-d3fd-46a2-bda5-6fa3294fd1fd', embedding=None, metadata={'title': 'Vector search for Amazon DocumentDB (with MongoDB compatibility) is now generally available', 'summary': 'This new built-in capability lets you store, index, and search millions of vectors with millisecond response times within your document database.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, we are announcing the general availability of vector search for Amazon DocumentDB (with MongoDB compatibility), a new built-in capability that lets you store, index, and search millions of vectors with millisecond response times within your document database. Vector search is an emerging technique used in machine learning (ML) to find similar data points to given data by comparing their vector representations using distance or similarity metrics. Vectors are numerical representation of unstructured data created from large language models (LLM) hosted in Amazon Bedrock, Amazon SageMaker, and other open source or proprietary ML services. This approach is useful in creating generative artificial intelligence (AI) applications, such as intuitive search, product recommendation, personalization, and chatbots using Retrieval Augmented Generation (RAG) model approach. For example, if your data set contained individual documents for movies, you could semantically search for movies similar to Titanic based on shared context such as “boats”, “tragedy”, or “movies based on true stories” instead of simply matching keywords. With vector search for Amazon DocumentDB, you can effectively search the database based on nuanced meaning and context without spending time and cost to manage a separate vector database infrastructure. You also benefit from the fully managed, scalable, secure, and highly available JSON-based document database that Amazon DocumentDB provides. Getting started with vector search on Amazon DocumentDB The vector search feature is available on your Amazon DocumentDB 5.0 instance-based clusters. To implement a vector search application, you generate vectors using embedding models for fields inside your document and store vectors side by side your source data inside Amazon DocumentDB. Next, you create a vector index on a vector field that will help retrieve similar vectors and can search the Amazon DocumentDB database using semantic search. Finally, user-submitted queries are converted to vectors using the same embedding model to get semantically similar documents and return them to the client. Let’s look at how to implement a simple semantic search application using vector search on Amazon DocumentDB. Step 1. Create vector embeddings using the Amazon Titan Embeddings model Let’s use the Amazon Titan Embeddings model to create an embedding vector. Amazon Titan Embeddings model is available in Amazon Bedrock, a serverless generative AI service. You can easily access it using a single API and without managing any infrastructure. prompt = \"I love dog and cat.\"', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='1fda531b-1b8b-4312-a3fe-538f0b92a028', embedding=None, metadata={'title': 'Vector search for Amazon DocumentDB (with MongoDB compatibility) is now generally available', 'summary': 'This new built-in capability lets you store, index, and search millions of vectors with millisecond response times within your document database.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='response = bedrock_runtime.invoke_model(\\n    body= json.dumps({\"inputText\": prompt}), \\n    modelId=\\'amazon.titan-embed-text-v1\\', \\n    accept=\\'application/json\\', \\n    contentType=\\'application/json\\'\\n)\\nresponse_body = json.loads(response[\\'body\\'].read())\\nembedding = response_body.get(\\'embedding\\') The returned vector embedding will look similar to this: [0.82421875, -0.6953125, -0.115722656, 0.87890625, 0.05883789, -0.020385742, 0.32421875, -0.00078201294, -0.40234375, 0.44140625, ...] Step 2. Insert vector embeddings and create a vector index You can add generated vector embeddings using the insertMany( [{},...,{}] ) operation with a list of the documents that you want added to your collection in Amazon DocumentDB. db.collection.insertMany([\\n    {sentence: \"I love a dog and cat.\", vectorField: [0.82421875, -0.6953125,...]},\\n    {sentence: \"My dog is very cute.\", vectorField: [0.05883789, -0.020385742,...]},\\n    {sentence: \"I write with a pen.\", vectorField: [-0.020385742, 0.32421875,...]},\\n  ...\\n]); You can create a vector index using the createIndex command. Amazon DocumentDB performs an approximate nearest neighbor (ANN) search using the inverted file with flat compression (IVFFLAT) vector index. The feature supports three distance metrics: euclidean, cosine, and inner product. We will use the euclidean distance, a measure of the straight-line distance between two points in space. The smaller the euclidean distance, the closer the vectors are to each other. db.collection.createIndex (\\n   { vectorField: \"vector\" },\\n   { \"name\": \"index name\",\\n     \"vectorOptions\": {\\n        \"dimensions\": 100, // the number of vector data dimensions\\n        \"similarity\": \"euclidean\", // Or cosine and dotProduct\\n        \"lists\": 100 \\n      }\\n   }\\n); Step 3. \\xa0Search vector embeddings from Amazon DocumentDB You can now search for similar vectors within your documents using a new aggregation pipeline operator within $search .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='d9c22be9-7237-444d-8f80-824d42691690', embedding=None, metadata={'title': 'Vector search for Amazon DocumentDB (with MongoDB compatibility) is now generally available', 'summary': 'This new built-in capability lets you store, index, and search millions of vectors with millisecond response times within your document database.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='The example code to search “ I like pets ” is as follows: db.collection.aggregate ({\\n  $search: {\\n    \"vectorSearch\": {\\n      \"vector\": [0.82421875, -0.6953125,...], // Search for ‘I like pets’\\n      \"path\": vectorField,\\n      \"k\": 5,\\n      \"similarity\": \"euclidean\", // Or cosine and dotProduct\\n      \"probes\": 1 // the number of clusters for vector search\\n      }\\n     }\\n   }); This returns search results such as “ I love a dog and cat. ” which is semantically similar. To learn more, see Amazon DocumentDB documentation. To see a more practical example—a semantic movie search with Amazon DocumentDB—find the Python source codes and data-sets in the GitHub repository . Now available Vector search for Amazon DocumentDB is now available at no additional cost to all customers using Amazon DocumentDB 5.0 instance-based clusters in all AWS Regions where Amazon DocumentDB is available . Standard compute, I/O, storage, and backup charges will apply as you store, index, and search vector embeddings on Amazon DocumentDB. To learn more, see the Amazon DocumentDB documentation and send feedback to AWS re:Post for Amazon DocumentDB or through your usual AWS Support contacts. — Channy', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='70e6f1b6-c17f-4620-bc5e-c1b20225fd53', embedding=None, metadata={'title': 'Amazon DynamoDB zero-ETL integration with Amazon OpenSearch Service is now available', 'summary': 'This capability lets you perform a search on your DynamoDB data by automatically replicating and transforming it without custom code or infrastructure.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, we are announcing the general availability of Amazon DynamoDB zero-ETL integration with Amazon OpenSearch Service , which lets you perform a search on your DynamoDB data by automatically replicating and transforming it without custom code or infrastructure. This zero-ETL integration reduces the operational burden and cost involved in writing code for a data pipeline architecture, keeping the data in sync, and updating code with frequent application changes, enabling you to focus on your application. With this zero-ETL integration, Amazon DynamoDB customers can now use the powerful search features of Amazon OpenSearch Service , such as full-text search, fuzzy search, auto-complete, and vector search for machine learning (ML) capabilities to offer new experiences that boost user engagement and improve satisfaction with their applications. This zero-ETL integration uses Amazon OpenSearch Ingestion to synchronize the data between Amazon DynamoDB and Amazon OpenSearch Service. You choose the DynamoDB table whose data needs to be synchronized and Amazon OpenSearch Ingestion synchronizes the data to an Amazon OpenSearch managed cluster or serverless collection within seconds of it being available. You can also specify index mapping templates to ensure that your Amazon DynamoDB fields are mapped to the correct fields in your Amazon OpenSearch Service indexes. Also, you can synchronize data from multiple DynamoDB tables into one Amazon OpenSearch Service managed cluster or serverless collection to oﬀer holistic insights across several applications. Getting started with this zero-ETL integration With a few clicks, you can synchronize data from DynamoDB to OpenSearch Service. To create an integration between DynamoDB and OpenSearch Service, choose the Integrations menu in the left pane of the DynamoDB console and the DynamoDB table whose data you want to synchronize. You must turn on point-in-time recovery (PITR) and the DynamoDB Streams feature. This feature allows you to capture item-level changes in your table and push the changes to a stream. Choose Turn on for PITR and enable DynamoDB Streams in the Exports and streams tab. After turning on PITR and DynamoDB Stream, choose Create to set up an OpenSearch Ingestion pipeline in your account that replicates the data to an OpenSearch Service managed domain. In the first step, enter a unique pipeline name and set up pipeline capacity and compute resources to automatically scale your pipeline based on the current ingestion workload. Now you can configure the pre-defined pipeline configuration in YAML file format.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='186f4b00-75cd-43fe-a6e8-a1b757a2d821', embedding=None, metadata={'title': 'Amazon DynamoDB zero-ETL integration with Amazon OpenSearch Service is now available', 'summary': 'This capability lets you perform a search on your DynamoDB data by automatically replicating and transforming it without custom code or infrastructure.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='You can browse resources to look up and paste information to build the pipeline configuration. This pipeline is a combination of a source part from DyanmoDB settings and a sink part for OpenSearch Service. You must set multiple IAM roles ( sts_role_arn ) with the necessary permissions to read data from the DynamoDB table and write to an OpenSearch domain. This role is then assumed by OpenSearch Ingestion pipelines to ensure that the right security posture is always maintained when moving the data from source to destination. To learn more, see Setting up roles and users in Amazon OpenSearch Ingestion in the AWS documentation. After entering all required values, you can validate the pipeline configuration to ensure that your configuration is valid. To learn more, see Creating Amazon OpenSearch Ingestion pipelines in the AWS documentation. Take a few minutes to set up the OpenSearch Ingestion pipeline, and you can see your integration is completed in the DynamoDB table. Now you can search synchronized items in the OpenSearch Dashboards. Things to know Here are a couple of things that you should know about this feature: Custom schema – You can specify your custom data schema along with the index mappings used by OpenSearch Ingestion when writing data from Amazon DynamoDB to OpenSearch Service. This experience is added to the console within Amazon DynamoDB so that you have full control over the format of indices that are created on OpenSearch Service. Pricing – There will be no additional cost to use this feature apart from the cost of the existing underlying components. Note that Amazon OpenSearch Ingestion charges OpenSearch Compute Units (OCUs) which will be used to replicate data between Amazon DynamoDB and Amazon OpenSearch Service. Furthermore, this feature uses Amazon DynamoDB streams for the change data capture (CDC) and you will incur the standard costs for Amazon DynamoDB Streams. Monitoring – You can monitor the state of the pipelines by checking the status of the integration on the DynamoDB console or using the OpenSearch Ingestion dashboard. Additionally, you can use Amazon CloudWatch to provide real-time metrics and logs, which lets you set up alerts in case of a breach of user-defined thresholds. Now available Amazon DynamoDB zero-ETL integration with Amazon OpenSearch Service is now generally available in all AWS Regions where OpenSearch Ingestion is available today.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ee721820-1375-494a-93a0-15e4e9bbbac0', embedding=None, metadata={'title': 'Amazon DynamoDB zero-ETL integration with Amazon OpenSearch Service is now available', 'summary': 'This capability lets you perform a search on your DynamoDB data by automatically replicating and transforming it without custom code or infrastructure.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='To learn more, see DynamoDB zero-ETL integration with Amazon OpenSearch Service and Using an OpenSearch Ingestion pipeline with Amazon DynamoDB in the AWS documentation. Give it a try and send feedback to AWS re:Post for Amazon OpenSearch Service or through your usual AWS Support contacts. — Channy', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='8337bf99-558a-4782-968c-28d335bc072c', embedding=None, metadata={'title': 'Amazon ElastiCache Serverless for Redis and Memcached is now available', 'summary': 'This new serverless offering allows customers to create a cache in under a minute and instantly scale capacity based on application traffic patterns.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, we are announcing the availability of Amazon ElastiCache Serverless , a new serverless option that allows customers to create a cache in under a minute and instantly scale capacity based on application traffic patterns. ElastiCache Serverless is compatible with two popular open-source caching solutions, Redis and Memcached. You can use ElastiCache Serverless to operate a cache for even the most demanding workloads without spending time in capacity planning or requiring caching expertise. ElastiCache Serverless constantly monitors your application’s memory, CPU, and network resource utilization and scales instantly to accommodate changes to the access patterns of workloads it serves. You can create a highly available cache with data automatically replicated across multiple Availability Zones and up to 99.99 percent availability Service Level Agreement (SLA) for all workloads, which saves you time and money. Customers wanted to get radical simplicity to deploy and operate a cache. ElastiCache Serverless offers a simple endpoint experience abstracting the underlying cluster topology and cache infrastructure. You can reduce application complexity and have more operational excellence without handling reconnects and rediscovering nodes. With ElastiCache Serverless, there are no upfront costs, and you pay for only the resources you use. You pay for the amount of cache data storage and ElastiCache Processing Units (ECPUs) resources consumed by your applications. Getting started with Amazon ElastiCache Serverless To get started, go to the ElastiCache console and choose Redis caches or Memcached caches in the left navigation pane. ElastiCache Serverless supports engine versions of Redis 7.1 or higher and Memcached 1.6 or higher. For example, in the case of Redis caches, choose Create Redis cache . You see two deployment options: either Serverless or Design your own cache to create a node-based cache cluster. Choose the Serverless option, the New cache method, and provide a name. Use the default settings to create a cache in your default VPC, Availability Zones, service-owned encryption key, and security groups. We will automatically set recommended best practices. You don’t have to enter any additional settings. If you want to customize default settings, you can set your own security groups, or enable automatic backups. You can also set maximum limits for your compute and memory usage to ensure your cache doesn’t grow beyond a certain size.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ba3256e0-b0eb-4488-93d2-68adb7c68a4d', embedding=None, metadata={'title': 'Amazon ElastiCache Serverless for Redis and Memcached is now available', 'summary': 'This new serverless offering allows customers to create a cache in under a minute and instantly scale capacity based on application traffic patterns.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='When your cache reaches the memory limit, keys with a time to live (TTL) are evicted according to the least recently used (LRU) logic. When your compute limit is reached, ElastiCache will throttle requests, which will lead to elevated request latencies. When you create a new serverless cache, you can see the details of settings for connectivity and data protection, including an endpoint and network environment. Now, you can configure the ElastiCache Serverless endpoint in your application and connect using any Redis client that supports Redis in cluster mode, such as redis-cli . $ redis-cli -h channy-redis-serverless.elasticache.amazonaws.com --tls -c -p 6379\\nset x Hello\\nOK\\nget x\\n\"Hello\" You can manage the cache using AWS Command Line Interface (AWS CLI) or AWS SDKs. For more information, see Getting started with Amazon ElastiCache for Redis in the AWS documentation. If you have an existing Redis cluster, you can migrate your data to ElastiCache Serverless by specifying the ElastiCache backups or Amazon S3 location of a backup file in a standard Redis rdb file format when creating your ElastiCache Serverless cache. For a Memcached cache, you can create and use a new serverless cache in the same way as Redis. If you use ElastiCache Serverless for Memcached, there are significant benefits of high availability and instant scaling because they are not natively available in the Memcached engine. You no longer have to write custom business logic, manage multiple caches, or use a third-party proxy layer to replicate data to get high availability with Memcached. Now you can get up to 99.99 percent availability SLA and data replication across multiple Availability Zones. To connect to the Memcached endpoint, run the openssl client and Memcached commands as shown in the following example output: $ /usr/bin/openssl s_client -connect channy-memcached-serverless.cache.amazonaws.com:11211 -crlf \\nset a 0 0 5\\nhello\\nSTORED\\nget a\\nVALUE a 0 5\\nhello\\nEND For more information, see Getting started with Amazon ElastiCache Serverless for Memcached in the AWS documentation. Scaling and performance ElastiCache Serverless scales without downtime or performance degradation to the application by allowing the cache to scale up and initiating a scale-out in parallel to meet capacity needs just in time.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='adb4e68f-aec2-4bf9-a8c3-dc295cc0d87f', embedding=None, metadata={'title': 'Amazon ElastiCache Serverless for Redis and Memcached is now available', 'summary': 'This new serverless offering allows customers to create a cache in under a minute and instantly scale capacity based on application traffic patterns.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='To show ElastiCache Serverless’ performance we conducted a simple scaling test. We started with a typical Redis workload with an 80/20 ratio between reads and writes with a key size of 512 bytes. Our Redis client was configured to Read From Replica (RFR) using the READONLY Redis command, for optimal read performance. Our goal is to show how fast workloads can scale on ElastiCache Serverless without any impact on latency. As you can see in the graph above, we were able to double the requests per second (RPS) every 10 minutes up until the test’s target request rate of 1M RPS. During this test, we observed that p50 GET latency remained around 751 microseconds and at all times below 860 microseconds. Similarly, we observed p50 SET latency remained around 1,050 microseconds, not crossing the 1,200 microseconds even during the rapid increase in throughput. Things to know Upgrading engine version – ElastiCache Serverless transparently applies new features, bug fixes, and security updates, including new minor and patch engine versions on your cache. When a new major version is available, ElastiCache Serverless will send you a notification in the console and an event in Amazon EventBridge . ElastiCache Serverless major version upgrades are designed for no disruption to your application. Performance and monitoring – ElastiCache Serverless publishes a suite of metrics to Amazon CloudWatch, including memory usage ( BytesUsedForCache ), CPU usage ( ElastiCacheProcessingUnits ), and cache metrics, including CacheMissRate , CacheHitRate , CacheHits , CacheMisses , and ThrottledRequests . ElastiCache Serverless also publishes Amazon EventBridge events for significant events, including cache creation, deletion, and limit updates. For a full list of available metrics and events, see the documentation. Security and compliance – ElastiCache Serverless caches are accessible from within a VPC. You can access the data plane using AWS Identity and Access Management (IAM) . By default, only the AWS account creating the ElastiCache Serverless cache can access it. ElastiCache Serverless encrypts all data at rest and in-transit by transport layer security (TLS) encrypting each connection to ElastiCache Serverless.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='05f665aa-69f0-417a-a198-da83bf802864', embedding=None, metadata={'title': 'Amazon ElastiCache Serverless for Redis and Memcached is now available', 'summary': 'This new serverless offering allows customers to create a cache in under a minute and instantly scale capacity based on application traffic patterns.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='You can optionally choose to limit access to the cache within your VPCs, subnets, IAM access, and AWS Key Management Service (AWS KMS) key for encryption. ElastiCache Serverless is compliant with PCI-DSS, SOC, and ISO and is HIPAA eligible. Now available Amazon ElastiCache Serverless is now available in all commercial AWS Regions, including China. With ElastiCache Serverless, there are no upfront costs, and you pay for only the resources you use. You pay for cached data in GB-hours, ECPUs consumed, and Snapshot storage in GB-months. To learn more, see the ElastiCache Serverless page and the pricing page . Give it a try, and please send feedback to AWS re:Post for Amazon ElastiCache or through your usual AWS support contacts. — Channy', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='93a9ad72-11dd-448f-870b-2d68545aec19', embedding=None, metadata={'title': 'Join the preview of Amazon Aurora Limitless Database', 'summary': 'This new capability supports automated horizontal scaling to process millions of write transactions per second and manage petabytes of data in a single Aurora database.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, we are announcing the preview of Amazon Aurora Limitless Database , a new capability supporting automated horizontal scaling to process millions of write transactions per second and manage petabytes of data in a single Aurora database. Amazon Aurora read replicas allow you to increase the read capacity of your Aurora cluster beyond the limits of what a single database instance can provide. Now, Aurora Limitless Database scales write throughput and storage capacity of your database beyond the limits of a single Aurora writer instance. The compute and storage capacity that is used for Limitless Database is in addition to and independent of the capacity of your writer and reader instances in the cluster. With Limitless Database, you can focus on building high-scale applications without having to build and maintain complex solutions for scaling your data across multiple database instances to support your workloads. Aurora Limitless Database scales based on the workload to support write throughput and storage capacity that, until today, would require multiple Aurora writer instances. The architecture of Amazon Aurora Limitless Database Limitless Database has a two-layer architecture consisting of multiple database nodes, either transaction routers or shards. Shards are Aurora PostgreSQL DB instances that each store a subset of the data for your database, allowing for parallel processing to achieve higher write throughput. Transaction routers manage the distributed nature of the database and present a single database image to database clients. Transaction routers maintain metadata about where data is stored, parse incoming SQL commands and send those commands to shards, aggregate data from shards to return a single result to the client, and manage distributed transactions to maintain consistency across the entire distributed database. All the nodes that make up your Limitless Database architecture are contained in a DB shard group. The DB shard group has a separate endpoint where your access your Limitless Database resources. Getting started with Aurora Limitless Database To get started with a preview of Aurora Limitless Database, you can sign up today and will be invited soon. The preview runs in a new Aurora PostgreSQL cluster with version 15 in the AWS US East (Ohio), US East (N. Virginia), US West (Oregon), Asia Pacific (Tokyo), and Europe (Ireland) Regions. As part of the creation workflow for an Aurora cluster, choose the Limitless Database compatible version in the Amazon RDS console or the Amazon RDS API. Then you can add a DB shard group and create new Limitless Database tables. You can choose the maximum Aurora capacity units (ACUs).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='c318236b-9a0e-4d46-97fc-ebbc391c2643', embedding=None, metadata={'title': 'Join the preview of Amazon Aurora Limitless Database', 'summary': 'This new capability supports automated horizontal scaling to process millions of write transactions per second and manage petabytes of data in a single Aurora database.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='After the DB shard group is created, you can view its details on the Databases page, including its endpoint. To use Aurora Limitless Database, you should connect to a DB shard group endpoint, also called the limitless endpoint, using psql or any other connection utility that works with PostgreSQL. There will be two types of tables that contain your data in Aurora Limitless Database: Sharded tables – These tables are distributed across multiple shards. Data is split among the shards based on the values of designated columns in the table, called shard keys. Reference tables – These tables have all their data present on every shard so that join queries can work faster by eliminating unnecessary data movement. They are commonly used for infrequently modified reference data, such as product catalogs and zip codes. Once you have created a sharded or reference table, you can load massive data into Aurora Limitless Database and manipulate data in those tables using the standard PostgreSQL queries. Join the preview You can join the preview of Amazon Aurora Limitless Database to be among the first to experience all of this power. Sign up now , give it a try, and please send feedback to AWS re:Post for Amazon Aurora or through your usual AWS support contacts. — Channy', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='65b25051-7992-4827-8be5-d9ed5c77c875', embedding=None, metadata={'title': 'Getting started with new Amazon RDS for Db2', 'summary': 'IBM Db2 is an enterprise-grade relational database management system (RDBMS) developed by IBM.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='I am pleased to announce that IBM and AWS have come together to offer Amazon Relational Database Service (Amazon RDS) for Db2, a fully managed Db2 database engine running on AWS infrastructure. IBM Db2 is an enterprise-grade relational database management system (RDBMS) developed by IBM. It offers a comprehensive set of features, including strong data processing capabilities, robust security mechanisms, scalability, and support for diverse data types. Db2 is a well-established choice among organizations for effectively managing data in various applications and handling data-intensive workloads due to its reliability and performance. Db2 has its roots in the pioneering work around data storage and structured query language (SQL) IBM has done since the 1970s. It has been commercially available since 1983, initially just for mainframes, and was later ported to Linux, Unix, and Windows platforms (LUW). Today, Db2 powers thousands of business-critical applications in all verticals. With Amazon RDS for Db2, you can now create a Db2 database with just a few clicks in the AWS Management Console , one command to type with the AWS Command Line Interface (AWS CLI) , or a few lines of code with the AWS SDKs . AWS takes care of the infrastructure heavy lifting, freeing your time for higher-level tasks such as schema and query optimizations for your applications. If you are new to Amazon RDS or coming from an on-premises Db2 background, let me quickly recap the benefits of Amazon RDS. Amazon RDS offers the same Db2 database as the one you use on-premises today. Your existing applications will reconnect to RDS for Db2 without changing their code. The database runs on a fully managed infrastructure. You don’t have to provision servers, install the packages, install patches, or maintain the infrastructure in an operational state. The database is also fully managed. We take care of the installation, minor version upgrades, daily backup, scaling, and high availability. The infrastructure can scale up and down as required. You can simply stop and then restart the database to change the underlying hardware and meet changing performance requirements or benefit from last-generation hardware. Amazon RDS offers a choice of storage types designed to deliver fast, predictable, and consistent I/O performance. For new or unpredictable workloads, you can configure the system to automatically scale your storage .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ad9c94d1-47ff-49e7-89b7-c13919be60fc', embedding=None, metadata={'title': 'Getting started with new Amazon RDS for Db2', 'summary': 'IBM Db2 is an enterprise-grade relational database management system (RDBMS) developed by IBM.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Amazon RDS automatically takes care of your backups, and you can restore them to a new database with just a few clicks. Amazon RDS helps to deploy highly available architectures. Amazon RDS synchronously replicates data to a standby database in a different Availability Zone (an Availability Zone is a group of distinct data centers). When a failure is detected with a Multi-AZ deployment, Amazon RDS automatically fails over to the standby instance and routes requests without changing the database endpoint DNS name. This switch happens with minimal downtime and zero data loss. Amazon RDS is built on the secure infrastructure of AWS . It encrypts data in transit using TLS and at rest using keys managed with AWS Key Management Service (AWS KMS) . This helps you deploy workloads that are compliant with your company or industry regulations, such as FedRAMP , GDPR , HIPAA , PCI , and SOC . Third-party auditors assess the security and compliance of Amazon RDS as part of multiple AWS compliance programs and you can verify the full list of Amazon RDS compliance validations . You can migrate your existing on-premises Db2 database to Amazon RDS using native Db2 tools, such as restore and import , or AWS Database Migration Service (AWS DMS). AWS DMS allows you to migrate databases in a single operation or continuously, while your applications continue to update the data on the source database, until you decide on the cut off. Amazon RDS supports multiple tools for monitoring your database instances, including Amazon RDS Enhanced Monitoring and Amazon CloudWatch , or you can continue to use the IBM Data Management Console or IBM DSMtop . Let’s see how it works I always like to get my hands on a new service to learn how it works. Let’s create a Db2 database and connect to it using the standard tool provided by IBM . I assume most of you reading this post come from an IBM Db2 background and don’t know much about Amazon RDS. First, I create a Db2 database. To do this, I navigate to the Amazon RDS page of the AWS Management Console and select Create database . For this demo, I’ll accept most of the default values. I’ll show you, however, all the sections and will comment on the important configuration points you have to think about. I select Db2 from among the multiple database engines Amazon RDS offers.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='2063314a-c649-401a-8e4d-0d8c1cf19af0', embedding=None, metadata={'title': 'Getting started with new Amazon RDS for Db2', 'summary': 'IBM Db2 is an enterprise-grade relational database management system (RDBMS) developed by IBM.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='I scroll down the page and select IBM Db2 Standard and Engine Version 11.5.9 . Amazon RDS patches the database instances automatically if you so desire. You can learn more about Amazon RDS database maintenance here . I select Production . Amazon RDS will deploy a default configuration tuned for high availability and fast, consistent performance. Under Settings , I give a name to my RDS instance (this is not the Db2 catalog name!), and I select the master username and password . Under Instance configuration , I choose the type of node to run my database. This will define the hardware characteristics of the virtual server: the number of vCPUs, quantity of memory, and so on. Depending on the requirements of your application, you can allocate instances offering up to 32 vCPUs and 128 GiB of RAM for IBM Db2 Standard instances. When you select IBM Db2 Advanced instances, you can allocate instances offering up to 128 vCPUs and 1 TiB of RAM. This parameter has a direct impact on the price. Under Storage , I choose the type of Amazon Elastic Block Store (Amazon EBS) volumes, their size, and their IOPS and throughput. For this demo, I accept the values proposed by default. This is also a set of parameters that directly impact the price. Under Connectivity , I select the VPC (in AWS terms, a VPC is a private network) where the database will be deployed. Under Public access , I select No to make sure the database instance is only accessible from my private network. I can’t think of a (good) use case where you want to select Yes for this option. This is also where you select the VPC security group . A security group is a network filter that defines what IP addresses or networks can access your database instance and on what TCP port. Be sure to select or create a security group with TCP 50000 open to allow applications to connect to your Db2 database. I leave all other options with their default value. It is important to open the Additional configuration section at the very bottom of the page. This is where you can give an Initial database name . If you don’t name your Db2 database here, your only option will be to restore an existing Db2 database backup on that instance. This section also contains the parameters for the Amazon RDS automatic backup.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='0d0a8cd3-fb23-428c-a0cc-0ef6e3e8dafe', embedding=None, metadata={'title': 'Getting started with new Amazon RDS for Db2', 'summary': 'IBM Db2 is an enterprise-grade relational database management system (RDBMS) developed by IBM.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='You can choose a time window and how long we will retain the backups. I accept all the defaults and select Create database . After a few minutes, you can see your database is available. I select the DNS name of the database instance Endpoint , and I connect to a Linux machine running in the same network. After installing the Db2 client package that I downloaded from the IBM website , I type the following commands to connect to the database. There is nothing specific to Amazon RDS here. db2 catalog TCPIP node blognode remote awsnewsblog-demo.abcdef.us-east-2.rds-preview.amazonaws.com server 50000\\ndb2 catalog database NEWSBLOG as blogdb2 at node blognode authentication server_encrypt\\ndb2 connect to blogdb2 user admin using MySuperPassword Once connected, I download a sample dataset and script from the popular Db2Tutorial website . I run the scripts against the database I just created. wget https://www.db2tutorial.com/wp-content/uploads/2019/06/books.zip\\nunzip books.zip \\ndb2 -stvf ./create.sql \\ndb2 -stvf ./data.sql \\ndb2 \"select count(*) author_count from authors\" As you can see, there is nothing specific to Amazon RDS when it comes to connecting and using the database. I use standard Db2 tools and scripts. One more thing Amazon RDS for Db2 requires you to bring your own Db2 license. You must enter your IBM customer ID and site number before starting a Db2 instance. To do so, create a custom DB parameter group and attach it to your database instance at launch time. A DB parameter group acts as a container for engine configuration values that are applied to one or more DB instances. In a Db2 parameter group, there are two parameters specific to IBM Db2 licenses: your IBM Customer Number ( rds.ibm_customer_id ) and your IBM site number ( rds.ibm_site_id ). If you do not know your site number, reach out to your IBM sales organization for a copy of a recent Proof-of-Entitlement (PoE), invoice, or sales order. All these documents should include your site number. Pricing and availability Amazon RDS for Db2 is available in all AWS Regions except China and GovCloud. Amazon RDS pricing is on demand, and there are no upfront costs or subscriptions.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='68c75e80-4537-48f7-9cac-bbe35eb01bb6', embedding=None, metadata={'title': 'Getting started with new Amazon RDS for Db2', 'summary': 'IBM Db2 is an enterprise-grade relational database management system (RDBMS) developed by IBM.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='You only pay by the hour when the database is running, plus the GB per month of database storage provisioned and backup storage you use and the number of IOPS you provision. The Amazon RDS for Db2 pricing page has the details of pricing per Region. As I mentioned earlier, Amazon RDS for Db2 requires you to bring your own Db2 license. If you already know Amazon RDS, you’ll be delighted to have a new database engine available for your application developers. If you’re coming from an on-premises world, you will love the simplicity and automation that Amazon RDS offers. You can learn many more details on the Amazon RDS for Db2 documentation page . Now go and deploy your first database with Amazon RDS for Db2 today! -- seb', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='c0093b21-b060-402e-bd95-e8d634bee0f4', embedding=None, metadata={'title': 'Use AWS Fault Injection Service to demonstrate multi-region and multi-AZ application resilience', 'summary': 'New scenarios let you demonstrate that your applications perform as intended if an AWS Availability Zone experiences a full power interruption or connectivity from one AWS region to another is lost.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='AWS Fault Injection Service (FIS) helps you to put chaos engineering into practice at scale. Today we are launching new scenarios that will let you demonstrate that your applications perform as intended if an AWS Availability Zone experiences a full power interruption or connectivity from one AWS region to another is lost. You can use the scenarios to conduct experiments that will build confidence that your application (whether single-region or multi-region) works as expected when something goes wrong, help you to gain a better understanding of direct and indirect dependencies, and test recovery time. After you have put your application through its paces and know that it works as expected, you can use the results of the experiment for compliance purposes. When used in conjunction with other parts of AWS Resilience Hub , FIS can help you to fully understand the overall resilience posture of your applications. Intro to Scenarios We launched FIS in 2021 to help you perform controlled experiments on your AWS applications. In the post that I wrote to announce that launch, I showed you how to create experiment templates and to use them to conduct experiments. The experiments are built using powerful, low-level actions that affect specified groups of AWS resources of a particular type. For example, the following actions operate on EC2 instances and Auto Scaling Groups: With these actions as building blocks, we recently launched the AWS FIS Scenario Library . Each scenario in the library defines events or conditions that you can use to test the resilience of your applications: Each scenario is used to create an experiment template. You can use the scenarios as-is, or you can take any template as a starting point and customize or enhance it as desired. The scenarios can target resources in the same AWS account or in other AWS accounts: New Scenarios With all of that as background, let’s take a look at the new scenarios. AZ Availability: Power Interruption – This scenario temporarily “pulls the plug” on a targeted set of your resources in a single Availability Zone including EC2 instances (including those in EKS and ECS clusters), EBS volumes, Auto Scaling Groups, VPC subnets, Amazon ElastiCache for Redis clusters, and Amazon Relational Database Service (RDS) clusters. In most cases you will run it on an application that has resources in more than one Availability Zone, but you can run it on a single-AZ app with an outage as the expected outcome.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='c5a50ca0-dc5b-48c2-988e-e92f07663ff1', embedding=None, metadata={'title': 'Use AWS Fault Injection Service to demonstrate multi-region and multi-AZ application resilience', 'summary': 'New scenarios let you demonstrate that your applications perform as intended if an AWS Availability Zone experiences a full power interruption or connectivity from one AWS region to another is lost.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='It targets a single AZ, and also allows you to disallow a specified set of IAM roles or Auto Scaling Groups from being able to launch fresh instances or start stopped instances during the experiment. The New actions and targets experience makes it easy to see everything at a glance — the actions in the scenario and the types of AWS resources that they affect: The scenarios include parameters that are used to customize the experiment template: The Advanced parameters – targeting tags lets you control the tag keys and values that will be used to locate the resources targeted by experiments: Cross-Region: Connectivity – This scenario prevents your application in a test region from being able to access resources in a target region. This includes traffic from EC2 instances, ECS tasks, EKS pods, and Lambda functions attached to a VPC. It also includes traffic flowing across Transit Gateways and VPC peering connections, as well as cross-region S3 and DynamoDB replication. The scenario looks like this out of the box: This scenario runs for 3 hours (unless you change the disruptionDuration parameter), and isolates the test region from the target region in the specified ways, with advanced parameters to control the tags that are used to select the affected AWS resources in the isolated region: You might also find that the Disrupt and Pause actions used in this scenario useful on their own: For example, the aws:s3:bucket-pause-replication action can be used to pause replication within a region. Things to Know Here are a couple of things to know about the new scenarios: Regions – The new scenarios are available in all commercial AWS Regions where FIS is available, at no additional cost. Pricing – You pay for the action-minutes consumed by the experiments that you run; see the AWS Fault Injection Service Pricing Page for more info. Naming – This service was formerly called AWS Fault Injection Simulator. — Jeff ;', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='a6d5b8ba-317e-41bf-bbaf-efdc870592dc', embedding=None, metadata={'title': 'IDE extension for AWS Application Composer enhances visual modern applications development with AI-generated IaC', 'summary': 'Now you can use AWS Application Composer directly in your IDE to visually build modern applications and iteratively develop your infrastructure as code templates with Amazon CodeWhisperer.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, I’m happy to share the integrated development environment (IDE) extension for AWS Application Composer . Now you can use AWS Application Composer directly in your IDE to visually build modern applications and iteratively develop your infrastructure as code templates with Amazon CodeWhisperer . Announced as preview at AWS re:Invent 2022 and generally available in March 2023 , Application Composer is a visual builder that makes it easier for developers to visualize, design, and iterate on an application architecture by dragging, grouping, and connecting AWS services on a visual canvas. Application Composer simplifies building modern applications by providing an easy-to-use visual drag-and-drop interface and generates IaC templates in real time. AWS Application Composer also lets you work with AWS CloudFormation resources. In September, AWS Application Composer announced support for 1000+ AWS CloudFormation resources . This provides you the flexibility to define configuration for your AWS resources at a granular level. Building modern applications with modern tools The IDE extension for AWS Application Composer provides you with the same visual drag-and-drop experience and functionality as what it offers you in the console. Utilizing the visual canvas in your IDE means you can quickly prototype your ideas and focus on your application code. With Application Composer running in your IDE, you can also use the various tools available in your IDE. For example, you can seamlessly integrate IaC templates generated real-time by Application Composer with AWS Serverless Application Model (AWS SAM)\\xa0to manage and deploy your serverless applications. In addition to making Application Composer available in your IDE, you can create generative AI powered code suggestions in the CloudFormation template in real time while visualizing the application architecture in split view. You can pair and synchronize Application Composer’s visualization and CloudFormation template editing side by side in the IDE without context switching between consoles to iterate on their designs. This minimizes hand coding and increase your productivity. Using AWS Application Composer in Visual Studio Code First, I need to install the latest AWS Toolkit for Visual Studio Code plugin. If you already have the AWS Toolkit plugin installed, you only need to update the plugin to start using Application Composer. To start using Application Composer,\\xa0I don’t need to authenticate into my AWS account.\\xa0With Application Composer available on my IDE, I can open my existing AWS CloudFormation or AWS SAM templates.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='08af39b2-d4df-45bd-be3b-49df0903ff9d', embedding=None, metadata={'title': 'IDE extension for AWS Application Composer enhances visual modern applications development with AI-generated IaC', 'summary': 'Now you can use AWS Application Composer directly in your IDE to visually build modern applications and iteratively develop your infrastructure as code templates with Amazon CodeWhisperer.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Another method is to create a new blank file, then right-click on the file and select Open with Application Composer to start designing my application visually. This will provide me with a blank canvas. Here I have both code and visual editors at the same time to build a simple serverless API using Amazon API Gateway , AWS Lambda , and Amazon DynamoDB . Any changes that I make on the canvas will also be reflected in real time on my IaC template. I get consistent experiences, such as when I use the Application Composer console. For example, if I make some modifications to my AWS Lambda function, it will also create relevant files in my local folder. With IaC templates available in my local folder, it’s easier for me to manage my applications with AWS SAM CLI. I can create continuous integration and continuous delivery (CI/CD) with sam pipeline or deploy my stack with sam deploy . One of the features that accelerates my development workflow is the built-in Sync feature that seamlessly integrates with AWS SAM command sam sync . This feature syncs my local application changes to my AWS account, which is helpful for me to do testing and validation before I deploy my applications into a production environment. Developing IaC templates with generative AI With this new capability, I can use generative AI code suggestions to quickly get started with any of CloudFormation’s 1000+ resources.\\xa0This also means that it’s now even easier to include standard IaC resources to extend my architecture. For example, I need to use Amazon MQ , which is a standard IaC resource, and I need to modify some configurations for its AWS CloudFormation resource using Application Composer. In the Resource configuration section, change some values if needed, then choose Generate . Application Composer provides code suggestions that I can accept and incorporate into my IaC template. This capability helps me to improve my development velocity by eliminating context switching. I can design my modern applications using AWS Application Composer canvas and use various tools such as Amazon CodeWhisperer and AWS SAM to accelerate my development workflow. Things to know Here are a couple of things to note: Supported IDE – At launch, this new capability is available for Visual Studio Code. Pricing – The IDE extension for AWS Application Composer is available at no charge. Get started with IDE extension for AWS Application Composer by installing the latest AWS Toolkit for Visual Studio Code . Happy coding! — Donnie', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ad507c4f-140b-4fdc-aa75-1ec61cae2dd3', embedding=None, metadata={'title': 'Upgrade your Java applications with Amazon Q Code Transformation (preview)', 'summary': 'This new capability simplifies upgrading and modernizing existing application code using Amazon Q.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='As our applications age, it takes more and more effort just to keep them secure and running smoothly. Developers managing the upgrades must spend time relearning the intricacies and nuances of breaking changes and performance optimizations others have already discovered in past upgrades. As a result, it’s difficult to balance the focus between new features and essential maintenance work. Today, we are introducing in preview Amazon Q Code Transformation . This new capability simplifies upgrading and modernizing existing application code using Amazon Q , a new type of assistant powered by generative artificial intelligence (AI) . Amazon Q is specifically designed for work and can be tailored to your business. Amazon Q Code Transformation can perform Java application upgrades now, from version 8 and 11 to version 17, a Java Long-Term Support (LTS) release, and it will soon be able to transform Windows-based .NET Framework applications to cross-platform .NET. Previously, developers could spend two to three days upgrading each application. Our internal testing shows that the transformation capability can upgrade an application in minutes compared to the days or weeks typically required for manual upgrades, freeing up time to focus on new business requirements. For example, an internal Amazon team of five people successfully upgraded one thousand production applications from Java 8 to 17 in 2 days. It took, on average, 10 minutes to upgrade applications, and the longest one took less than an hour. Amazon Q Code Transformation automatically analyzes the existing code, generates a transformation plan, and completes the transformation tasks suggested by the plan. While doing so, it identifies and updates package dependencies and refactors deprecated and inefficient code components, switching to new language frameworks and incorporating security best practices. Once complete, you can review the transformed code, complete with build and test results, before accepting the changes. In this way, you can keep applications updated and supported in just a few steps, gain performance benefits, and remove vulnerabilities from using unsupported versions, freeing up time to focus on new business requirements. Let’s see how this works in practice. Upgrading a Java application from version 8 to 17 I am using IntelliJ IDEA in this walkthrough (the same is available for Visual Studio Code). To have Amazon Q Code Transformation in my IDE, I install the latest version of the AWS Toolkit for IntelliJ IDEA and sign in using the AWS IAM Identity Center credentials provided by my organization.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='1cc2d381-440e-4fe4-befe-819901cab38d', embedding=None, metadata={'title': 'Upgrade your Java applications with Amazon Q Code Transformation (preview)', 'summary': 'This new capability simplifies upgrading and modernizing existing application code using Amazon Q.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Note that to access Amazon Q Code Transformation, the CodeWhisperer administrator needs to explicitly give\\xa0access to Amazon Q features in the profile used by the organization. I open an old project that I never had the time to update to a more recent version of Java. The project is using Apache Maven to manage the build. The project object model (POM) file ( pom.xml ), an XML representation of the project, is in the root directory. First, in the project settings, I check that the project is configured to use the correct SDK version (1.8 in this case). I choose AWS Toolkit on the left pane and then the Amazon Q + CodeWhisperer tab. In the Amazon Q (Preview) section, I choose Transform . This opens a dialog where I check that the correct Maven module is selected for the upgrade before proceeding with the transformation. I follow the progress in the Transformation Hub window. The upgrade completes in a few minutes for my small application, while larger ones might take more than an hour to complete. The end-to-end application upgrade consists of three steps: Identifying and analyzing the application – The code is copied to a managed environment in the cloud where the build process is set up based on the instructions in the repository. At this stage, the components to be upgraded are identified. Creating a transformation plan – The code is analyzed to create a transformation plan that lists the steps that Amazon Q Code Transformation will take to upgrade the code, including updating dependencies, building the upgraded code, and then iteratively fixing any build errors encountered during the upgrade. Code generation, build testing, and finalization – The transformation plan is followed iteratively to update existing code and configuration files, generate new files where needed, perform build validation using the tests provided with the code, and fix issues identified in failed builds. After a few minutes, the transformation terminates successfully. From here, I can open the plan and a summary of the transformation. I choose View diff to see the proposed changes. In the Apply Patch dialog, I see a recap of the files that have been added, modified, or deleted. First, I select the pom.xml file and then choose Show Difference (the icon with the left/right arrows) to have a side-by-side view of the current code in the project and the proposed changes.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ee93f7eb-d6c1-48d6-b66c-c2b483f4f9cc', embedding=None, metadata={'title': 'Upgrade your Java applications with Amazon Q Code Transformation (preview)', 'summary': 'This new capability simplifies upgrading and modernizing existing application code using Amazon Q.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='For example, I see that the version of one of the dependencies ( Project Lombok ) has been increased for compatibility with the target Java version. In the Java file, the annotations used by the upgraded dependency have been updated. With the new version, @With has been promoted, and @Wither (which was experimental) deprecated. These changes are reflected in the import statements. There is also a summary file that I keep in the code repo to quickly look up the changes made to complete the upgrade. I spend some time reviewing the files. Then, I choose OK to accept all changes. Now the patch has been successfully applied, and the proposed changes merged with the code. I commit changes to my repo and move on to focus on business-critical changes that have been waiting for the migration to be completed. Things to know The preview of Amazon Q Code Transformation is available today for customers on the Amazon CodeWhisperer Professional Tier in the AWS Toolkit for IntelliJ IDEA and the AWS Toolkit for Visual Studio Code . To use Amazon Q Code Transformation, the CodeWhisperer administrator needs to give access to the profile used by the organization. There is no additional cost for using Amazon Q Code Transformation during the preview. You can upgrade Java 8 and 11 applications that are built using Apache Maven to Java version 17. The project must have the POM file ( pom.xml ) in the root directory. We’ll soon add the option to transform Windows-based .NET Framework applications to cross-platform .NET and help accelerate migrations to Linux. Once a transformation job is complete, you can use a diff view to verify and accept the proposed changes. The final transformation summary provides details of the dependencies updated and code files changed by Amazon Q Code Transformation. It also provides details of any build failures encountered in the final build of the upgraded code that you can use to fix the issues and complete the upgrade. Combining Amazon’s long-term investments in automated reasoning and static code analysis with the power of generative AI , Amazon Q Code Transformation incorporates foundation models that we found to be essential for context-specific code transformations that often require updating a long tail of Java libraries with backward-incompatible changes. In addition to generative AI-powered code transformations built by AWS, Amazon Q Code Transformation uses parts of OpenRewrite to further accelerate Java upgrades for customers.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='8e8b5d34-a529-4a0c-8f23-852d3422c691', embedding=None, metadata={'title': 'Upgrade your Java applications with Amazon Q Code Transformation (preview)', 'summary': 'This new capability simplifies upgrading and modernizing existing application code using Amazon Q.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='At AWS, many of our services are built with open source components and promoting the long-term sustainability of these communities is critical to us and our customers. That is why it’s important for us to contribute back to communities like OpenRewrite, helping ensure the whole industry can continue to benefit from their innovations. AWS plans to contribute to OpenRewrite recipes and improvements developed as part of Amazon Q Code Transformation to open source. “The ability for software to adapt at a much faster pace is one of the most fundamental advantages any business can have. That’s why we’re excited to see AWS using OpenRewrite, the open source automated code refactoring technology, as a component of their service,” said Jonathan Schneider , CEO and Co-founder of Moderne (the sponsor of OpenRewrite). “We’re happy to have AWS join the OpenRewrite community and look forward to their contributions to make it even easier to migrate frameworks, patch vulnerabilities, and update APIs.” Upgrade your Java applications now Amazon Q Code Transformation product page Read more about Amazon Q Introducing Amazon Q, a new generative AI-powered assistant (preview) Amazon Q brings generative AI-powered assistance to IT pros and developers (preview) Improve developer productivity with generative-AI powered Amazon Q in Amazon CodeCatalyst (preview) New generative AI features in Amazon Connect, including Amazon Q, facilitate improved contact center service New Amazon Q in QuickSight uses generative AI assistance for quicker, easier data insights (preview) — Danilo', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='9ae94250-d132-4393-b1c4-845f93296573', embedding=None, metadata={'title': 'Improve developer productivity with generative-AI powered Amazon Q in Amazon CodeCatalyst (preview)', 'summary': 'Developers can go from an idea to fully tested, merge-ready, running code with only natural language inputs, in just a few clicks.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, I’m excited to introduce the preview of new generative artificial intelligence (AI) capabilities within Amazon CodeCatalyst that accelerate software delivery using Amazon Q. Accelerate feature development – The feature development capability in Amazon Q can help you accelerate the implementation of software development tasks such as adding comments and READMEs, refining issue descriptions, generating small classes and unit tests, and updating CodeCatalyst workflows — tedious and undifferentiated tasks that take up developers’ time. Developers can go from an idea in an issue to fully tested, merge-ready, running code with only natural language inputs, in just a few clicks. AI does the heavy lifting of converting the human prompt to an actionable plan, summarizing source code repositories, generating code, unit tests, and workflows, and summarizing any changes in a pull request which is assigned back to the developer. You can also provide feedback to Amazon Q directly on the published pull request and ask it to generate a new revision. If the code change falls short of expectations, you can create a development environment directly from the pull request, make any necessary adjustments manually, publish a new revision, and proceed with the merge upon approval. Example: make an API change in an existing application In the navigation pane, I choose Issues and then I choose Create issue . I give the issue the title, Change the get_all_mysfits() API to return mysfits sorted by the Age attribute . I then assign this issue to Amazon Q and choose Create issue . Amazon Q will automatically move the issue into the In progress state while it analyzes the issue title and description to formulate a potential solution approach. If there is already some discussion on the issue, it should be summarized in the description to help Q understand what needs to be done. As it works, Amazon Q will report on its progress by leaving comments on the issue at every stage. It will attempt to create a solution based on its understanding of the code already present in the repository and the approach it formulated. If Amazon Q is able to successfully generate a potential solution, it will create a branch and commit code to that branch. It will then create a pull request that will merge the changes into the default branch once approved. Once the pull request is published, Amazon Q will change the issue status to In Review so that you and your team know that the code is now ready for you to review.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='25de1aef-2bc3-4ee4-b180-d0da9286767d', embedding=None, metadata={'title': 'Improve developer productivity with generative-AI powered Amazon Q in Amazon CodeCatalyst (preview)', 'summary': 'Developers can go from an idea to fully tested, merge-ready, running code with only natural language inputs, in just a few clicks.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Summarize a change – Pull request authors can save time by asking Amazon Q to summarize the change they are publishing for review. Today pull request authors have to write the description manually or they may choose not to write it at all. If the author does not provide a description, it makes it harder for reviewers to understand what changes are being made and why, delaying the review process and slowing down software delivery. Pull request authors and reviewers can also save time by asking Amazon Q to summarize the comments left on the pull request. The summary is useful for the author because they can easily see common feedback themes. For the reviewers it is useful because they can quickly catch up on the conversation and feedback from themselves and other team members. The overall benefits are streamlined collaboration, accelerated review process, and faster software delivery. Join the preview Amazon Q features in CodeCatalyst are off by default, and can be enabled by the user in space settings. Amazon Q is available in Amazon CodeCatalyst today for spaces in AWS Region US West (Oregon). Learn more Amazon CodeCatalyst product page Amazon CodeCatalyst User Guide Read more about Amazon Q Introducing Amazon Q, a new generative AI-powered assistant (preview) Amazon Q brings generative AI-powered assistance to IT pros and developers (preview) Upgrade your Java applications with Amazon Q Code Transformation (preview) New generative AI features in Amazon Connect, including Amazon Q, facilitate improved contact center service New Amazon Q in QuickSight uses generative AI assistance for quicker, easier data insights (preview) — Irshad', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='93d4f7cf-5646-403c-8309-f1f0e9bf7c56', embedding=None, metadata={'title': 'New Amazon WorkSpaces Thin Client provides cost-effective, secure access to virtual desktops', 'summary': 'The Thin Client is a small cube that connects directly to a monitor, keyboard, mouse, and other USB peripherals such as headsets, microphones, and cameras.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='The new Amazon WorkSpaces Thin Client improves end-user and IT staff productivity with cost-effective, secure, easy-to-manage access to virtual desktops. The devices are preconfigured and shipped directly to the end user, ready to deploy, connect, and use. Here’s my testing setup: The Thin Client is a small cube that connects directly to a monitor, keyboard, mouse, and other USB peripherals such as headsets, microphones, and cameras. With the optional hub it can also drive a second monitor. The administrator can create environments that give users access to Amazon WorkSpaces , Amazon WorkSpaces Web , or Amazon AppStream 2.0 , with multiple options for managing user identities and credentials using Active Directory. Thin Clients in action As a very long-time user of Amazon WorkSpaces via a thin client, I am thrilled to be able to tell you about this device and the administrative service behind it. While my priority is the ease with which I can switch from client to client while maintaining my working context (running apps, browser tabs, and so forth), administrators will find it attractive for other reasons. For example: Cost – The device itself is low cost ($195 in the United States), far less expensive than a laptop and the associated operating system. Because the working environments are centrally configured and administered, there’s less work to be done in the field, leading to further cost savings. Further, the devices are far simpler than laptops, with less parts to break, wear out, or replace. Security – The devices are shipped with a secure “secret” that is used to establish a trust relationship with the administrative service. There’s no data storage on the device, and it cannot host rogue apps that could attempt to exfiltrate data. It also helps to reduce risk of data leakage should a worker leave their job without returning their employer-supplied laptop. Ease of Management – Administrators can easily create new environments for users or groups of users, distribute activation codes to them, and manage the environment via the AWS Management Console . They can set schedules for software updates and patches, verify compliance, and manage users over time. Ease of Use – Users can unpack and connect the devices in minutes, enter their activation codes, log in to their virtual desktop environment, and start to work right away. They don’t have to take responsibility for installing software patches or updates, and can focus on their jobs. There are lots of great use cases for these devices!', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='707a776d-bae8-41a0-a6ad-df39cc8b0a40', embedding=None, metadata={'title': 'New Amazon WorkSpaces Thin Client provides cost-effective, secure access to virtual desktops', 'summary': 'The Thin Client is a small cube that connects directly to a monitor, keyboard, mouse, and other USB peripherals such as headsets, microphones, and cameras.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='First, there are situations where there’s a long-term need for regular access: call centers, task workers, training centers, and so forth. Second, there are other situations, where there’s a transient or short-term need for access: registration systems at large events, call centers stood up on a temporary basis for a special event or an emergency, disaster response, and the like. Given that some employees do not return laptops to their employers when they leave their job, providing them with inexpensive devices that do not have local storage makes a lot of sense. Let’s walk through the process of getting set up, first as an administrator and then as a user. Getting started as an administrator The first step is to order some devices and have them shipped to my users, along with any desired peripherals. Next, in my position as administrator, I open the Amazon WorkSpaces Thin Client Console , and click Get started : Each Amazon WorkSpaces Thin Client environment provides access to a specific virtual desktop service ( WorkSpaces , WorkSpaces Web , or Amazon AppStream 2.0 ). I click Create environment to move ahead: I give my environment a name, indicate that I want patches applied automatically, and select WorkSpaces Web : Next, I click Create WorkSpaces Web portal and go through those steps (not shown, basically choosing a VPC and two or more subnets, a security group, and an optional Private Certificate Authority): I refresh, and my new portal is visible. I select it, enter a tag for tracking, and click Create environment : My environment is ready right away. I keep a copy of the activation code ( aci3a5yj ) at hand to use later in the post: I am using AWS Identity Center as my identity provider. I already set up my first user, and assigned myself to the MyWebPortal app (the precise steps that you take to do this will vary depending on your choice of identity provider): Finally, as my last step in this process in my role as administrator, I share the activation code with my users (that would be me, in this case).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='9af33bd7-480e-4826-8845-0ef51ff51d09', embedding=None, metadata={'title': 'New Amazon WorkSpaces Thin Client provides cost-effective, secure access to virtual desktops', 'summary': 'The Thin Client is a small cube that connects directly to a monitor, keyboard, mouse, and other USB peripherals such as headsets, microphones, and cameras.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Getting started as a user In my role as a user I return to my testing setup, power-on, go through a couple of quick steps to select my keyboard and connect to my home Wi-Fi, and enter my activation code: Then I sign in using my AWS Identity Center user name and password: And my WorkSpace is ready to use: Administrator tools As an administrator, I can manage environments, devices, and device software updates from the Thin Client Console. For example, I can review the list of devices that I manage: Things to know Here are a couple of things that are important to know: Regions – The Thin Client Console is available in the US East (N. Virginia), US West (Oregon), Asia Pacific (Mumbai), Canada (Central), and Europe (Frankfurt, Ireland, London) Regions. Device Sales – The Amazon WorkSpaces Thin Clients are available in the United States now, with availability in other countries in early 2024. Pricing – Devices are priced at $195, or $280 with an optional hub that allows you to use a second monitor. There’s a $6 per month fee to manage, maintain, and monitor each device, and you also pay for the underlying virtual desktop service. Learn more Visit the WorkSpaces Thin Client web page and Amazon Business Marketplace to learn more. — Jeff ;', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='af2c6a6d-cbbf-4c6a-ae49-f251e25d389f', embedding=None, metadata={'title': 'Announcing cross-region data replication for Amazon WorkSpaces', 'summary': 'Snapshots are taken every 12 hours, replicated to the desired destination region, and are used to provide a recovery point objective of 12-24 hours.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='You can now use cross-region data replication to provide business continuity for your Amazon WorkSpaces users. Snapshots are taken every 12 hours, replicated to the desired destination region, and are used to provide a recovery point objective (RPO) of 12-24 hours. Multi-Region Resilience Review In her 2022 post Advancing business continuity with Amazon WorkSpaces Multi-Region Resilience , my colleague Ariel introduced you to the initial version of this feature and showed you how to use it to set up standby virtual desktops available for your users. After it has been set up, users log in with Amazon WorkSpaces registration codes that include fully qualified domain names (FQDNs). If the WorkSpaces in the primary region are unavailable, the users are redirected to standby WorkSpaces in the secondary region. The standby WorkSpaces are available for a small, fixed monthly fee for infrastructure and storage, with a low flat rate change for each hour of usage during the month. Together, this feature and this business model make it easy and economical for you to maintain a standby deployment. Cross-Region Data Replication Today we are making this feature even more powerful by adding one-way cross-region data replication. Applications, documents, and other resources stored on the primary WorkSpace are snapshotted every 12 hours and copied to the region hosting the secondary WorkSpace. You get an additional layer of redundancy, enhanced data protection, and can minimize productivity that would otherwise be lost to disruptions. This is particularly helpful if users have installed and configured applications on top of the base image since they won’t have to repeat these steps on the secondary WorkSpace. Here’s how it all works: Normal Operation – During normal operation, the users in your WorkSpaces fleet are using the primary region. EBS snapshots of the system (C:) and data (D:) drives are created every 12 hours. Multi-Region Resilience runs in the secondary region and checks for fresh snapshots regularly. When it finds them, it initiates a copy to the secondary region. As the copies arrive in the secondary region they are used to update the secondary WorkSpace. Failover Detection – As part of the setup process, you will follow Configure your DNS service and setup DNS routing policies to set up DNS routing policies and optional Amazon Route 53 health checks to manage cross-Region redirection.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='85c83d0c-f256-4a57-a5ba-d5b9ad2cc1b9', embedding=None, metadata={'title': 'Announcing cross-region data replication for Amazon WorkSpaces', 'summary': 'Snapshots are taken every 12 hours, replicated to the desired destination region, and are used to provide a recovery point objective of 12-24 hours.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Failover – If a large-scale event (LSE) affects the primary region and the primary WorkSpace, the failover detection that I just described goes in to effect. When users try to reconnect, they are redirected to the secondary region, the latest snapshots are used to launch a WorkSpace for them, and they are back up and running with access to data and apps that are between 12 and 24 hours old. Failback – At the conclusion of the LSE, the users manually back up any data that they have created on the secondary WorkSpace and log out of it. Then they log in again, and this time they will be directed to the primary region and WorkSpace, where they can restore their backups and continue to work. Getting Set Up As a WorkSpaces administrator, I start by locating the desired primary WorkSpace: I select it and choose Create Standby WorkSpaces from the Actions menu: I select the desired region for the secondary WorkSpace and click Next : Then I choose the right directory in the region, and again click Next : If the primary WorkSpace is encrypted, I must enter the ARN of the KMS key in the secondary region (or, even better, use a multi-Region key ). I check Enable data replication and confirm that I am authorizing an additional monthly charge: On the next page I review my choices and click Create to initiate the creation of the secondary WorkSpace in the region that I selected. As mentioned earlier I also need to set up Multi-Region Resilience . This includes setting up a domain name to use as a WorkSpaces registration code, setting up Route 53 health checks, and using them to power routing policies. Things to Know Here are a couple of important things to know about Cross-Region Data Replication: Directories – You can use a self-managed Active Directory, AWS Managed AD or AD Connector configured as described in this post . Simple AD is not supported. Snapshots – The first-ever EBS snapshot for a particular data volume is full, and subsequent snapshots are incremental. As a result, the first replication for a given WorkSpace will likely take longer than subsequent ones. Snapshots are initiated on a schedule that is internal to WorkSpaces and you cannot control the timing. Encryption – You can use this feature with Encrypted WorkSpaces as long as you use the same AWS Key Management Service (AWS KMS) keys in the primary and secondary regions.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='68a0aa15-ea79-4911-a6bf-bba032573eaf', embedding=None, metadata={'title': 'Announcing cross-region data replication for Amazon WorkSpaces', 'summary': 'Snapshots are taken every 12 hours, replicated to the desired destination region, and are used to provide a recovery point objective of 12-24 hours.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='You can also use multi-Region keys . Bundles – You can use the Windows 10 and Windows 11 bundles, and you can also BYOL. Accounts – Every AWS account has a fixed limit on the number of pending EBS snapshots. This may affect your ability to use this feature with large fleets of WorkSpaces. Pricing – You pay a fixed monthly fee based on the amount of storage configured for each primary WorkSpace. See the Amazon WorkSpaces Pricing page for more information. — Jeff ;', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='5fe74615-73e0-4de0-95d1-e98c609c6483', embedding=None, metadata={'title': 'Amazon SageMaker Studio adds web-based interface, Code Editor, flexible workspaces, and streamlines user onboarding', 'summary': 'The new web-based interface loads faster and provides consistent access to your preferred integrated development environment (IDE) and SageMaker resources and tooling, regardless of your IDE choice.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, we are announcing an improved Amazon SageMaker Studio experience! The new SageMaker Studio web-based interface loads faster and provides consistent access to your preferred integrated development environment (IDE) and SageMaker resources and tooling, irrespective of your IDE choice. In addition to JupyterLab and RStudio, SageMaker Studio now includes a fully managed Code Editor based on Code-OSS (Visual Studio Code Open Source). Both Code Editor and JupyterLab can be launched using a flexible workspace. With spaces, you can scale the compute and storage for your IDE up and down as you go, customize runtime environments, and pause-and-resume coding anytime from anywhere. You can spin up multiple such spaces, each configured with a different combination of compute, storage, and runtimes. SageMaker Studio now also comes with a streamlined onboarding and administration experience to help both individual users and enterprise administrators get started in minutes. Let me give you a quick tour of some of these highlights. New SageMaker Studio web-based interface The new SageMaker Studio web-based interface acts as a command center for launching your preferred IDE and accessing your SageMaker tools to build, train, tune, and deploy models. You can now view SageMaker training jobs and endpoints in SageMaker Studio and access foundation models (FMs) via SageMaker JumpStart . Also, you no longer need to manually upgrade SageMaker Studio. New Code Editor based on Code-OSS (Visual Studio Code Open Source) As a data scientist or machine learning (ML) practitioner, you can now sign in to SageMaker Studio and launch Code Editor directly from your browser. With Code Editor, you have access to thousands of VS Code compatible extensions from Open VSX registry and the preconfigured AWS toolkit for Visual Studio Code for developing and deploying applications on AWS. You can also use the artificial intelligence (AI)-powered coding companion and security scanning tool powered by Amazon CodeWhisperer and Amazon CodeGuru. Launch Code Editor and JupyterLab in a flexible workspace You can launch both Code Editor and JupyterLab using private spaces that only the user creating the space has access to. This flexible workspace is designed to provide a faster and more efficient coding environment. The spaces come preconfigured with a SageMaker distribution that contains popular ML frameworks and Python packages. With the help of the AI-powered coding companions and security tools, you can quickly generate, debug, explain, and refactor your code.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='d9b7bf6c-7913-488d-b94e-0c8c8e7f3726', embedding=None, metadata={'title': 'Amazon SageMaker Studio adds web-based interface, Code Editor, flexible workspaces, and streamlines user onboarding', 'summary': 'The new web-based interface loads faster and provides consistent access to your preferred integrated development environment (IDE) and SageMaker resources and tooling, regardless of your IDE choice.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='In addition, SageMaker Studio comes with an improved collaboration experience. You can use the built-in Git integration to share and version code or bring your own shared file storage using Amazon EFS to access a collaborative filesystem across different users or teams. Streamlined user onboarding and administration With redesigned setup and onboarding workflows, you can now set up SageMaker Studio domains within minutes. As an individual user, you can now use a one-click experience to launch SageMaker Studio using default presets and without the need to learn about domains or AWS IAM roles. As an enterprise administrator, step-by-step instructions help you choose the right authentication method, connect to your third-party identity providers, integrate networking and security configurations, configure fine-grained access policies, and choose the right applications to enable in SageMaker Studio. You can also update settings at any time. To get started, navigate to the SageMaker console and select either Set up for single user or Set up for organization . The single-user setup will start deploying a SageMaker Studio domain using default presets and will be ready within a few minutes. The setup for organizations will guide you through the configuration step-by-step. Note that you can choose to keep working with the classic SageMaker Studio experience or start exploring the new experience. Now available The new Amazon SageMaker Studio experience is available today in all AWS Regions where SageMaker Studio is available. Starting today, new SageMaker Studio domains will default to the new web-based interface. If you have an existing setup and want to start using the new experience, check out the SageMaker Developer Guide for instructions on how to migrate your existing domains. Give it a try, and let us know what you think.\\xa0You can send feedback to AWS re:Post for Amazon SageMaker Studio or through your usual AWS contacts. Start building your ML projects with Amazon SageMaker Studio today! — Antje', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='33a8a7fa-9250-4bb6-9150-c3ebd2eebcea', embedding=None, metadata={'title': 'Package and deploy models faster with new tools and guided workflows in Amazon SageMaker', 'summary': 'Among several new capabilities, you can use the new ModelBuilder class in the SageMaker Python SDK to package models, perform local inference to validate runtime errors, and deploy to SageMaker from your local IDE or SageMaker Studio notebooks.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='I’m happy to share that Amazon SageMaker now comes with an improved model deployment experience to help you deploy traditional machine learning (ML) models and foundation models (FMs) faster. As a data scientist or ML practitioner, you can now use the new ModelBuilder class in the SageMaker Python SDK to package models, perform local inference to validate runtime errors, and deploy to SageMaker from your local IDE or SageMaker Studio notebooks. In SageMaker Studio , new interactive model deployment workflows give you step-by-step guidance on which instance type to choose to find the most optimal endpoint configuration. SageMaker Studio also provides additional interfaces to add models, test inference, and enable auto scaling policies on the deployed endpoints. New tools in SageMaker Python SDK The SageMaker Python SDK has been updated with new tools, including ModelBuilder and SchemaBuilder classes that unify the experience of converting models into SageMaker deployable models across ML frameworks and model servers. Model builder automates the model deployment by selecting a compatible SageMaker container and capturing dependencies from your development environment. Schema builder helps to manage serialization and deserialization tasks of model inputs and outputs. You can use the tools to deploy the model in your local development environment to experiment with it, fix any runtime errors, and when ready, transition from local testing to deploy the model on SageMaker with a single line of code. Let me show you how this works. In the following example, I choose the Falcon-7B model from the Hugging Face model hub . I first deploy the model locally, run a sample inference, perform local benchmarking to find the optimal configuration, and finally deploy the model with the suggested configuration to SageMaker. First, import the updated SageMaker Python SDK and define a sample model input and output that matches the prompt format for the selected model. import sagemaker\\nfrom sagemaker.serve.builder.model_builder import ModelBuilder\\nfrom sagemaker.serve.builder.schema_builder import SchemaBuilder\\nfrom sagemaker.serve import Mode\\n\\nprompt = \"Falcons are\"\\nresponse = \"Falcons are small to medium-sized birds of prey related to hawks and eagles.\"', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='997f8758-3b63-4f65-bf1b-b9b8c16c65ce', embedding=None, metadata={'title': 'Package and deploy models faster with new tools and guided workflows in Amazon SageMaker', 'summary': 'Among several new capabilities, you can use the new ModelBuilder class in the SageMaker Python SDK to package models, perform local inference to validate runtime errors, and deploy to SageMaker from your local IDE or SageMaker Studio notebooks.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='sample_input = {\\n    \"inputs\": prompt,\\n    \"parameters\": {\"max_new_tokens\": 32}\\n}\\n\\nsample_output = [{\"generated_text\": response}] Then, create a ModelBuilder instance with the Hugging Face model ID, a SchemaBuilder instance with the sample model input and output, define a local model path, and set the mode to LOCAL_CONTAINER to deploy the model locally. The schema builder generates the required functions for serializing and deserializing the model inputs and outputs. model_builder = ModelBuilder(\\n    model=\"tiiuae/falcon-7b\",\\n    schema_builder=SchemaBuilder(sample_input, sample_output),\\n    model_path=\"/path/to/falcon-7b\",\\n    mode=Mode.LOCAL_CONTAINER,\\n\\tenv_vars={\"HF_TRUST_REMOTE_CODE\": \"True\"}\\n) Next, call build() to convert the PyTorch model into a SageMaker deployable model. The build function generates the required artifacts for the model server, including the inferency.py and serving.properties files. local_mode_model = model_builder.build() For FMs, such as Falcon, you can optionally run tune() in local container mode that performs local benchmarking to find the optimal model serving configuration. This includes the tensor parallel degree that specifies the number of GPUs to use if your environment has multiple GPUs available. Once ready, call deploy() to deploy the model in your local development environment. tuned_model = local_mode_model.tune()\\ntuned_model.deploy() Let’s test the model. updated_sample_input = model_builder.schema_builder.sample_input\\nprint(updated_sample_input)\\n\\n{\\'inputs\\': \\'Falcons are\\',\\n \\'parameters\\': {\\'max_new_tokens\\': 32}}\\n \\nlocal_tuned_predictor.predict(updated_sample_input)[0][\"generated_text\"] In my demo, the model returns the following response: a type of bird that are known for their sharp talons and powerful beaks. They are also known for their ability to fly at high speeds […] When you’re ready to deploy the model on SageMaker, call deploy() again, set the mode to SAGEMAKLER_ENDPOINT , and provide an AWS Identity and Access Management (IAM) role with appropriate permissions. sm_predictor = tuned_model.deploy(\\n    mode=Mode.SAGEMAKER_ENDPOINT, \\n\\trole=\"arn:aws:iam::012345678910:role/role_name\"\\n) This starts deploying your model on a SageMaker endpoint. Once the endpoint is ready, you can run predictions.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='8c2222b5-04f4-4543-927b-97c6e44cca35', embedding=None, metadata={'title': 'Package and deploy models faster with new tools and guided workflows in Amazon SageMaker', 'summary': 'Among several new capabilities, you can use the new ModelBuilder class in the SageMaker Python SDK to package models, perform local inference to validate runtime errors, and deploy to SageMaker from your local IDE or SageMaker Studio notebooks.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='new_input = {\\'inputs\\': \\'Eagles are\\',\\'parameters\\': {\\'max_new_tokens\\': 32}}\\nsm_predictor.predict(new_input)[0][\"generated_text\"]) New SageMaker Studio model deployment experience You can start the new interactive model deployment workflows by selecting one or more models to deploy from the models landing page or SageMaker JumpStart model details page or by creating a new endpoint from the endpoints details page. The new workflows help you quickly deploy the selected model(s) with minimal inputs. If you used SageMaker Inference Recommender to benchmark your model, the dropdown will show instance recommendations from that benchmarking. Without benchmarking your model, the dropdown will display prospective instances that SageMaker predicts could be a good fit based on its own heuristics. For some of the most popular SageMaker JumpStart models, you’ll see an AWS pretested optimal instance type. For other models, you’ll see generally recommended instance types. For example, if I select the Falcon 40B Instruct model in SageMaker JumpStart, I can see the recommended instance types. However, if I want to optimize the deployment for cost or performance to meet my specific use cases, I could open the Alternate configurations panel to view more options based on data from before benchmarking. Once deployed, you can test inference or manage auto scaling policies. Things to know Here are a couple of important things to know: Supported ML models and frameworks – At launch, the new SageMaker Python SDK tools support model deployment for XGBoost and PyTorch models. You can deploy FMs by specifying the Hugging Face model ID or SageMaker JumpStart model ID using the SageMaker LMI container or Hugging Face TGI-based container . You can also bring your own container (BYOC) or deploy models using the Triton model server in ONNX format. Now available The new set of tools is available today in all AWS Regions where Amazon SageMaker real-time inference is available. There is no cost to use the new set of tools; you pay only for any underlying SageMaker resources that get created. Learn more Amazon SageMaker Model Deployment SageMaker Developer Guide Get started Explore the new SageMaker model deployment experience in the AWS Management Console today! — Antje', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='3949c551-ed3c-425d-8b0e-77989801630d', embedding=None, metadata={'title': 'Use natural language to explore and prepare data with a new capability of Amazon SageMaker Canvas', 'summary': 'SageMaker Canvas now supports using foundation model-(FM) powered natural language instructions to complement its comprehensive data preparation capabilities for data exploration, analysis, visualization, and transformation.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, I’m happy to introduce the ability to use natural language instructions in Amazon SageMaker Canvas to explore, visualize, and transform data for machine learning (ML). SageMaker Canvas now supports using foundation model-(FM) powered natural language instructions to complement its comprehensive data preparation capabilities for data exploration, analysis, visualization, and transformation. Using natural language instructions, you can now explore and transform your data to build highly accurate ML models. This new capability is powered by Amazon Bedrock. Data is the foundation for effective machine learning, and transforming raw data to make it suitable for ML model building and generating predictions is key to better insights. Analyzing, transforming, and preparing data to build ML models is often the most time-consuming part of the ML workflow. With SageMaker Canvas, data preparation for ML is seamless and fast with 300+ built-in transforms, analyses, and an in-depth data quality insights report without writing any code. Starting today, the process of data exploration and preparation is faster and simpler in SageMaker Canvas using natural language instructions for exploring, visualizing, and transforming data. Data preparation tasks are now accelerated through a natural language experience using queries and responses. You can quickly get started with contextual, guided prompts to understand and explore your data. Say I want to build an ML model to predict house prices Using SageMaker Canvas. First, I need to prepare my housing dataset to build an accurate model. To get started with the new natural language instructions, I open the SageMaker Canvas application, and in the left navigation pane, I choose Data Wrangler . Under the Data tab and from the list of available datasets, I select the canvas-housing-sample.csv as the dataset, then select Create a data flow and choose Create . I see the tabular view of my dataset and an introduction to the new Chat for data prep capability. I select Chat for data prep, and it displays the chat interface with a set of guided prompts relevant to my dataset. I can use any of these prompts or query the data for something else. First, I want to understand the quality of my dataset to identify any outliers or anomalies. I ask SageMaker Canvas to generate a data quality report to accomplish this task. I see there are no major issues with my data. I would now like to visualize the distribution of a couple of features in the data. I ask SageMaker Canvas to plot a chart.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='6e425355-72f3-47bb-b66f-c34f3718a6c2', embedding=None, metadata={'title': 'Use natural language to explore and prepare data with a new capability of Amazon SageMaker Canvas', 'summary': 'SageMaker Canvas now supports using foundation model-(FM) powered natural language instructions to complement its comprehensive data preparation capabilities for data exploration, analysis, visualization, and transformation.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='I now want to filter certain rows to transform my data. I ask SageMaker Canvas to remove rows where the population is less than 1,000. Canvas removes those rows, shows me a preview of the transformed data, and also gives me the option to view and update the code that generated the transform. I am happy with the preview and add the transformed data to my list of data transform steps on the right. SageMaker Canvas adds the step along with the code. Now that my data is transformed, I can go on to build my ML model to predict house prices and even deploy the model into production using the same visual interface of SageMaker Canvas, without writing a single line of code. Data preparation has never been easier for ML! Availability The new capability in Amazon SageMaker Canvas to explore and transform data using natural language queries is available in all AWS Regions where Amazon SageMaker Canvas and Amazon Bedrock are supported. Learn more Amazon SageMaker Canvas product page Go build! — Irshad', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='3944de2d-509c-4d66-8485-c0495eeaea58', embedding=None, metadata={'title': 'Amazon SageMaker adds new inference capabilities to help reduce foundation model deployment costs and latency', 'summary': 'With the new inference capabilities, you can deploy one or more foundation models (FMs) on the same SageMaker endpoint and control how many accelerators and how much memory is reserved for each FM.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, we are announcing new Amazon SageMaker inference capabilities that can help you optimize deployment costs and reduce latency. With the new inference capabilities, you can deploy one or more foundation models (FMs) on the same SageMaker endpoint and control how many accelerators and how much memory is reserved for each FM. This helps to improve resource utilization, reduce model deployment costs on average by 50 percent, and lets you scale endpoints together with your use cases. For each FM, you can define separate scaling policies to adapt to model usage patterns while further optimizing infrastructure costs. In addition, SageMaker actively monitors the instances that are processing inference requests and intelligently routes requests based on which instances are available, helping to achieve on average 20 percent lower inference latency. Key components The new inference capabilities build upon SageMaker real-time inference endpoints. As before, you create the SageMaker endpoint with an endpoint configuration that defines the instance type and initial instance count for the endpoint. The model is configured in a new construct, an inference component. Here, you specify the number of accelerators and amount of memory you want to allocate to each copy of a model, together with the model artifacts, container image, and number of model copies to deploy. Let me show you how this works. New inference capabilities in action You can start using the new inference capabilities from SageMaker Studio , the SageMaker Python SDK , and the AWS SDKs and AWS Command Line Interface (AWS CLI) . They are also supported by AWS CloudFormation . For this demo, I use the AWS SDK for Python (Boto3) to deploy a copy of the Dolly v2 7B model and a copy of the FLAN-T5 XXL model from the Hugging Face model hub on a SageMaker real-time endpoint using the new inference capabilities.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='550eb4ec-36de-4c0a-a443-7ae0f8b7bf92', embedding=None, metadata={'title': 'Amazon SageMaker adds new inference capabilities to help reduce foundation model deployment costs and latency', 'summary': 'With the new inference capabilities, you can deploy one or more foundation models (FMs) on the same SageMaker endpoint and control how many accelerators and how much memory is reserved for each FM.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Create a SageMaker endpoint configuration import boto3\\nimport sagemaker\\n\\nrole = sagemaker.get_execution_role()\\nsm_client = boto3.client(service_name=\"sagemaker\")\\n\\nsm_client.create_endpoint_config(\\n    EndpointConfigName=endpoint_config_name,\\n    ExecutionRoleArn=role,\\n    ProductionVariants=[{\\n        \"VariantName\": \"AllTraffic\",\\n        \"InstanceType\": \"ml.g5.12xlarge\",\\n        \"InitialInstanceCount\": 1,\\n\\t\\t\"RoutingConfig\": {\\n            \"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"\\n        }\\n    }]\\n) Create the SageMaker endpoint sm_client.create_endpoint(\\n    EndpointName=endpoint_name,\\n    EndpointConfigName=endpoint_config_name,\\n) Before you can create the inference component, you need to create a SageMaker-compatible model and specify a container image to use. For both models, I use the Hugging Face LLM Inference Container for Amazon SageMaker.\\xa0These deep learning containers (DLCs) include the necessary components, libraries, and drivers to host large models on SageMaker. Prepare the Dolly v2 model from sagemaker.huggingface import get_huggingface_llm_image_uri\\n\\n# Retrieve the container image URI\\nhf_inference_dlc = get_huggingface_llm_image_uri(\\n  \"huggingface\",\\n  version=\"0.9.3\"\\n)\\n\\n# Configure model container\\ndolly7b = {\\n    \\'Image\\': hf_inference_dlc,\\n    \\'Environment\\': {\\n        \\'HF_MODEL_ID\\':\\'databricks/dolly-v2-7b\\',\\n        \\'HF_TASK\\':\\'text-generation\\',\\n    }\\n}\\n\\n# Create SageMaker Model\\nsagemaker_client.create_model(\\n    ModelName        = \"dolly-v2-7b\",\\n    ExecutionRoleArn = role,\\n    Containers       = [dolly7b]\\n) Prepare the FLAN-T5 XXL model # Configure model container\\nflant5xxlmodel = {\\n    \\'Image\\': hf_inference_dlc,\\n    \\'Environment\\': {\\n        \\'HF_MODEL_ID\\':\\'google/flan-t5-xxl\\',\\n        \\'HF_TASK\\':\\'text-generation\\',\\n    }\\n}\\n\\n# Create SageMaker Model\\nsagemaker_client.create_model(\\n    ModelName        = \"flan-t5-xxl\",\\n    ExecutionRoleArn = role,\\n    Containers       = [flant5xxlmodel]\\n) Now, you’re ready to create the inference component.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='2160a356-9136-4c62-804e-2614aed17aea', embedding=None, metadata={'title': 'Amazon SageMaker adds new inference capabilities to help reduce foundation model deployment costs and latency', 'summary': 'With the new inference capabilities, you can deploy one or more foundation models (FMs) on the same SageMaker endpoint and control how many accelerators and how much memory is reserved for each FM.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Create an inference component for each model Specify an inference component for each model you want to deploy on the endpoint. Inference components let you specify the SageMaker-compatible model and the compute and memory resources you want to allocate. For CPU workloads, define the number of cores to allocate. For accelerator workloads, define the number of accelerators. RuntimeConfig defines the number of model copies you want to deploy. # Inference compoonent for Dolly v2 7B\\nsm_client.create_inference_component(\\n    InferenceComponentName=\"IC-dolly-v2-7b\",\\n    EndpointName=endpoint_name,\\n    VariantName=variant_name,\\n    Specification={\\n        \"ModelName\": \"dolly-v2-7b\",\\n        \"ComputeResourceRequirements\": {\\n\\t\\t    \"NumberOfAcceleratorDevicesRequired\": 2, \\n\\t\\t\\t\"NumberOfCpuCoresRequired\": 2, \\n\\t\\t\\t\"MinMemoryRequiredInMb\": 1024\\n\\t    }\\n    },\\n    RuntimeConfig={\"CopyCount\": 1},\\n)\\n\\n# Inference component for FLAN-T5 XXL\\nsm_client.create_inference_component(\\n    InferenceComponentName=\"IC-flan-t5-xxl\",\\n    EndpointName=endpoint_name,\\n    VariantName=variant_name,\\n    Specification={\\n        \"ModelName\": \"flan-t5-xxl\",\\n        \"ComputeResourceRequirements\": {\\n\\t\\t    \"NumberOfAcceleratorDevicesRequired\": 2, \\n\\t\\t\\t\"NumberOfCpuCoresRequired\": 1, \\n\\t\\t\\t\"MinMemoryRequiredInMb\": 1024\\n\\t    }\\n    },\\n    RuntimeConfig={\"CopyCount\": 1},\\n) Once the inference components have successfully deployed, you can invoke the models. Run inference To invoke a model on the endpoint, specify the corresponding inference component. import json\\nsm_runtime_client = boto3.client(service_name=\"sagemaker-runtime\")\\npayload = {\"inputs\": \"Why is California a great place to live?\"}', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='141ee8c3-bd51-41ba-93a0-9dcdaa8a7114', embedding=None, metadata={'title': 'Amazon SageMaker adds new inference capabilities to help reduce foundation model deployment costs and latency', 'summary': 'With the new inference capabilities, you can deploy one or more foundation models (FMs) on the same SageMaker endpoint and control how many accelerators and how much memory is reserved for each FM.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='response_dolly = sm_runtime_client.invoke_endpoint(\\n    EndpointName=endpoint_name,\\n    InferenceComponentName = \"IC-dolly-v2-7b\",\\n    ContentType=\"application/json\",\\n    Accept=\"application/json\",\\n    Body=json.dumps(payload),\\n)\\n\\nresponse_flant5 = sm_runtime_client.invoke_endpoint(\\n    EndpointName=endpoint_name,\\n    InferenceComponentName = \"IC-flan-t5-xxl\",\\n    ContentType=\"application/json\",\\n    Accept=\"application/json\",\\n    Body=json.dumps(payload),\\n)\\n\\nresult_dolly = json.loads(response_dolly[\\'Body\\'].read().decode())\\nresult_flant5 = json.loads(response_flant5[\\'Body\\'].read().decode()) Next, you can define separate scaling policies for each model by registering the scaling target and applying the scaling policy to the inference component. Check out the SageMaker Developer Guide for detailed instructions. The new inference capabilities provide per-model CloudWatch metrics and CloudWatch Logs and can be used with any SageMaker-compatible container image across SageMaker CPU- and GPU-based compute instances. Given support by the container image, you can also use response streaming. Now available The new Amazon SageMaker inference capabilities are available today in AWS Regions US East (Ohio, N. Virginia), US West (Oregon), Asia Pacific (Jakarta, Mumbai, Seoul, Singapore, Sydney, Tokyo), Canada (Central), Europe (Frankfurt, Ireland, London, Stockholm), Middle East (UAE), and South America (São Paulo). For pricing details, visit Amazon SageMaker Pricing . To learn more, visit Amazon SageMaker . Get started Log in to the AWS Management Console and deploy your FMs using the new SageMaker inference capabilities today! — Antje', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='6c06606b-d7fb-4ea4-ace4-6f06f06a8cd0', embedding=None, metadata={'title': 'Leverage foundation models for business analysis at scale with Amazon SageMaker Canvas', 'summary': 'This new capability makes it easier for you to evaluate and generate responses from FMs for your specific use case with high accuracy.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, I’m excited to introduce a new capability in Amazon SageMaker Canvas to use foundation models (FMs) from Amazon Bedrock and Amazon SageMaker Jumpstart through a no-code experience. This new capability makes it easier for you to evaluate and generate responses from FMs for your specific use case with high accuracy. Every business has its own set of unique domain-specific vocabulary that generic models are not trained to understand or respond to. The new capability in Amazon SageMaker Canvas bridges this gap effectively. SageMaker Canvas trains the models for you so you don’t need to write any code using our company data so that the model output reflects your business domain and use case such as completing a marketing analysis. For the fine-tuning process, SageMaker Canvas creates a new custom model in your account, and the data used for fine-tuning is not used to train the original FM, ensuring the privacy of your data. Earlier this year, we expanded support for ready-to-use models in Amazon SageMaker Canvas to include foundation models (FMs). This allows you to access, evaluate, and query FMs such as Claude 2, Amazon Titan, and Jurassic-2 (powered by Amazon Bedrock), as well as publicly available models such as Falcon and MPT (powered by Amazon SageMaker JumpStart) through a no-code interface. Extending this experience, we enabled the ability to query the FMs to generate insights from a set of documents in your own enterprise document index, such as Amazon Kendra. While it is valuable to query FMs, customers want to build FMs that generate responses and insights for their use cases. Starting today, a new capability to build FMs addresses this need to generate custom responses. To get started, I open the SageMaker Canvas application and in the left navigation pane, I choose My models. I select the New model button, select Fine-tune foundation model , and select Create . I select the training dataset and can choose up to three models to tune. I choose the input column with the prompt text and the output column with the desired output text. Then, I initiate the fine-tuning process by selecting Fine-tune . Once the fine-tuning process is completed, SageMaker Canvas gives me an analysis of the fine-tuned model with different metrics such as perplexity and loss curves, training loss, validation loss, and more.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='84335d98-4a7d-467f-925a-4bf01434c148', embedding=None, metadata={'title': 'Leverage foundation models for business analysis at scale with Amazon SageMaker Canvas', 'summary': 'This new capability makes it easier for you to evaluate and generate responses from FMs for your specific use case with high accuracy.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Additionally, SageMaker Canvas provides a model leaderboard that gives me the ability to measure and compare metrics around model quality for the generated models. Now, I am ready to test the model and compare responses with the original base model. To test, I select Test in Ready-to-use models from the Analyze page. The fine-tuned model is automatically deployed and is now available for me to chat and compare responses. Now, I am ready to generate and evaluate insights specific to my use case. The icing on the cake was to achieve this without writing a single line of code. Learn more Amazon SageMaker Canvas product page Amazon SageMaker Canvas developer guide Go build! — Irshad PS: Writing a blog post at AWS is always a team effort, even when you see only one name under the post title. In this case, I want to thank Shyam Srinivasan for his technical assistance.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='d7a62892-78db-46c2-ae04-0d045cc42a61', embedding=None, metadata={'title': 'Amazon SageMaker Clarify makes it easier to evaluate and select foundation models (preview)', 'summary': 'You can now use SageMaker Clarify to evaluate, compare, and select FMs in minutes based on metrics such as accuracy, robustness, creativity, factual knowledge, bias, and toxicity.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='I’m happy to share that Amazon SageMaker Clarify now supports foundation model (FM) evaluation (preview). As a data scientist or machine learning (ML) engineer, you can now use SageMaker Clarify to evaluate, compare, and select FMs in minutes based on metrics such as accuracy, robustness, creativity, factual knowledge, bias, and toxicity. This new capability adds to SageMaker Clarify’s existing ability to detect bias in ML data and models and explain model predictions. The new capability provides both automatic and human-in-the-loop evaluations for large language models (LLMs) anywhere, including LLMs available in SageMaker JumpStart , as well as models trained and hosted outside of AWS. This removes the heavy lifting of finding the right model evaluation tools and integrating them into your development environment. It also simplifies the complexity of trying to adopt academic benchmarks to your generative artificial intelligence (AI) use case. Evaluate FMs with SageMaker Clarify With SageMaker Clarify, you now have a single place to evaluate and compare any LLM based on predefined criteria during model selection and throughout the model customization workflow. In addition to automatic evaluation, you can also use the human-in-the-loop capabilities to set up human reviews for more subjective criteria, such as helpfulness, creative intent, and style, by using your own workforce or managed workforce from SageMaker Ground Truth . To get started with model evaluations, you can use curated prompt datasets that are purpose-built for common LLM tasks, including open-ended text generation, text summarization, question answering (Q&A), and classification. You can also extend the model evaluation with your own custom prompt datasets and metrics for your specific use case. Human-in-the-loop evaluations can be used for any task and evaluation metric. After each evaluation job, you receive an evaluation report that summarizes the results in natural language and includes visualizations and examples. You can download all metrics and reports and also integrate model evaluations into SageMaker MLOps workflows. In SageMaker Studio, you can find Model evaluation under Jobs in the left menu. You can also select Evaluate directly from the model details page of any LLM in SageMaker JumpStart. Select Evaluate a model to set up the evaluation job. The UI wizard will guide you through the selection of automatic or human evaluation, model(s), relevant tasks, metrics, prompt datasets, and review teams.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='fd423118-80af-4497-bf8a-55bef5baa7de', embedding=None, metadata={'title': 'Amazon SageMaker Clarify makes it easier to evaluate and select foundation models (preview)', 'summary': 'You can now use SageMaker Clarify to evaluate, compare, and select FMs in minutes based on metrics such as accuracy, robustness, creativity, factual knowledge, bias, and toxicity.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Once the model evaluation job is complete, you can view the results in the evaluation report. In addition to the UI, you can also start with example Jupyter notebooks that walk you through step-by-step instructions on how to programmatically run model evaluation in SageMaker. Evaluate models anywhere with the FMEval open source library To run model evaluation anywhere, including models trained and hosted outside of AWS, use the FMEval open source library . The following example demonstrates how to use the library to evaluate a custom model by extending the ModelRunner class. For this demo, I choose GPT-2 from the Hugging Face model hub and define a custom HFModelConfig and HuggingFaceCausalLLMModelRunner class that works with causal decoder-only models from the Hugging Face model hub such as GPT-2. The example is also available in the FMEval GitHub repo .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='3cfb1e28-fd49-4409-9860-c74304b1cb0d', embedding=None, metadata={'title': 'Amazon SageMaker Clarify makes it easier to evaluate and select foundation models (preview)', 'summary': 'You can now use SageMaker Clarify to evaluate, compare, and select FMs in minutes based on metrics such as accuracy, robustness, creativity, factual knowledge, bias, and toxicity.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='!pip install fmeval\\n\\n# ModelRunners invoke FMs\\nfrom amazon_fmeval.model_runners.model_runner import ModelRunner\\n\\n# Additional imports for custom model\\nimport warnings\\nfrom dataclasses import dataclass\\nfrom typing import Tuple, Optional\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\n@dataclass\\nclass HFModelConfig:\\n    model_name: str\\n    max_new_tokens: int\\n    normalize_probabilities: bool = False\\n    seed: int = 0\\n    remove_prompt_from_generated_text: bool = True\\n\\nclass HuggingFaceCausalLLMModelRunner(ModelRunner):\\n    def __init__(self, model_config: HFModelConfig):\\n        self.config = model_config\\n        self.model = AutoModelForCausalLM.from_pretrained(self.config.model_name)\\n        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)\\n\\n    def predict(self, prompt: str) -> Tuple[Optional[str], Optional[float]]:\\n        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\\n        generations = self.model.generate(\\n            **input_ids,\\n            max_new_tokens=self.config.max_new_tokens,\\n            pad_token_id=self.tokenizer.eos_token_id,\\n        )\\n        generation_contains_input = (\\n            input_ids[\"input_ids\"][0] == generations[0][: input_ids[\"input_ids\"].shape[1]]\\n        ).all()\\n        if self.config.remove_prompt_from_generated_text and not generation_contains_input:\\n            warnings.warn(\\n                \"Your model does not return the prompt as part of its generations. \"\\n                \"`remove_prompt_from_generated_text` does nothing.\"\\n            )\\n        if self.config.remove_prompt_from_generated_text and generation_contains_input:\\n            output = self.tokenizer.batch_decode(generations[:, input_ids[\"input_ids\"].shape[1] :])[0]\\n        else:\\n            output = self.tokenizer.batch_decode(generations, skip_special_tokens=True)[0]\\n\\n        with torch.inference_mode():\\n            input_ids = self.tokenizer(self.tokenizer.bos_token + prompt, return_tensors=\"pt\")[\"input_ids\"]\\n            model_output = self.model(input_ids, labels=input_ids)\\n            probability = -model_output[0].item()\\n\\n        return output, probability Next, create an instance of HFModelConfig and HuggingFaceCausalLLMModelRunner with the model information.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='47fb7411-d1e6-4f9a-a116-36daaa45efac', embedding=None, metadata={'title': 'Amazon SageMaker Clarify makes it easier to evaluate and select foundation models (preview)', 'summary': 'You can now use SageMaker Clarify to evaluate, compare, and select FMs in minutes based on metrics such as accuracy, robustness, creativity, factual knowledge, bias, and toxicity.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='hf_config = HFModelConfig(model_name=\"gpt2\", max_new_tokens=32)\\nmodel = HuggingFaceCausalLLMModelRunner(model_config=hf_config) Then, select and configure the evaluation algorithm. # Let\\'s evaluate the FM for FactualKnowledge\\nfrom amazon_fmeval.fmeval import get_eval_algorithm\\nfrom amazon_fmeval.eval_algorithms.factual_knowledge import FactualKnowledgeConfig\\n\\neval_algorithm_config = FactualKnowledgeConfig(\"<OR>\")\\neval_algorithm = get_eval_algorithm(\"factual_knowledge\", eval_algorithm_config) Let’s first test with one sample. The evaluation score is the percentage of factually correct responses. model_output = model.predict(\"London is the capital of\")[0]\\nprint(model_output)\\n\\neval_algo.evaluate_sample(\\n    target_output=\"UK<OR>England<OR>United Kingdom\", \\n\\tmodel_output=model_output\\n) the UK, and the UK is the largest producer of food in the world.\\n\\nThe UK is the world\\'s largest producer of food in the world.\\n[EvalScore(name=\\'factual_knowledge\\', value=1)] Although it’s not a perfect response, it includes “UK.” Next, you can evaluate the FM using built-in datasets or define your custom dataset. If you want to use a custom evaluation dataset, create an instance of DataConfig : config = DataConfig(\\n    dataset_name=\"my_custom_dataset\",\\n    dataset_uri=\"dataset.jsonl\",\\n    dataset_mime_type=MIME_TYPE_JSONLINES,\\n    model_input_location=\"question\",\\n    target_output_location=\"answer\",\\n)\\n\\neval_output = eval_algorithm.evaluate(\\n    model=model, \\n    dataset_config=config, \\n    prompt_template=\"$feature\", #$feature is replaced by the input value in the dataset \\n    save=True\\n) The evaluation results will return a combined evaluation score across the dataset and detailed results for each model input stored in a local output path. Join the preview FM evaluation with Amazon SageMaker Clarify is available today in public preview in AWS Regions US East (Ohio), US East (N. Virginia), US West (Oregon), Asia Pacific (Singapore), Asia Pacific (Tokyo), Europe (Frankfurt), and Europe (Ireland). The FMEval open source library is available on GitHub. To learn more, visit Amazon SageMaker Clarify. Get started Log in to the AWS Management Console and start evaluating your FMs with SageMaker Clarify today! — Antje', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f915da7f-842f-427b-bd9a-ed58c9f62ad8', embedding=None, metadata={'title': 'Evaluate, compare, and select the best foundation models for your use case in Amazon Bedrock (preview)', 'summary': 'Experiment with different models in the playground environment, and to iterate faster, add automatic evaluations of the models. Then, when you prepare for launch or limited release, you can incorporate human reviews to help ensure quality.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='I’m happy to share that you can now evaluate, compare, and select the best foundation models (FMs) for your use case in Amazon Bedrock . Model Evaluation on Amazon Bedrock is available today in preview. Amazon Bedrock offers a choice of automatic evaluation and human evaluation. You can use automatic evaluation with predefined metrics such as accuracy, robustness, and toxicity. For subjective or custom metrics, such as friendliness, style, and alignment to brand voice, you can set up human evaluation workflows with just a few clicks. Model evaluations are critical at all stages of development.\\xa0As a developer, you now have evaluation tools available for building generative artificial intelligence (AI) applications. You can start by experimenting with different models in the playground environment. To iterate faster, add automatic evaluations of the models. Then, when you prepare for an initial launch or limited release, you can incorporate human reviews to help ensure quality. Let me give you a quick tour of Model Evaluation on Amazon Bedrock. Automatic model evaluation With automatic model evaluation, you can bring your own data or use built-in, curated datasets and pre-defined metrics for specific tasks such as content summarization, question and answering, text classification, and text generation. This takes away the heavy lifting of designing and running your own model evaluation benchmarks. To get started, navigate to the Amazon Bedrock console , then select Model evaluation under Assessment & deployment in the left menu. Create a new model evaluation and choose Automatic . Next, follow the setup dialog to choose the FM you want to evaluate and the type of task, for example, text summarization. Select the evaluation metrics and specify a dataset—either built-in or your own. If you bring your own dataset, make sure it’s in JSON Lines format, and each line contains all of the key-value pairs that you want to evaluate your model with for the model dimension that you want to evaluate.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='b4b7ad0d-9746-42f5-8e74-83d0182dcbff', embedding=None, metadata={'title': 'Evaluate, compare, and select the best foundation models for your use case in Amazon Bedrock (preview)', 'summary': 'Experiment with different models in the playground environment, and to iterate faster, add automatic evaluations of the models. Then, when you prepare for launch or limited release, you can incorporate human reviews to help ensure quality.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='For example, if you want to evaluate the model on a question-answer task, you would format your data as follows (with category being optional): {\"referenceResponse\":\"Cantal\",\"category\":\"Capitals\",\"prompt\":\"Aurillac is the capital of\"}\\n{\"referenceResponse\":\"Bamiyan Province\",\"category\":\"Capitals\",\"prompt\":\"Bamiyan city is the capital of\"}\\n{\"referenceResponse\":\"Abkhazia\",\"category\":\"Capitals\",\"prompt\":\"Sokhumi is the capital of\"}\\n... Then, create and run the evaluation job to understand the model’s task-specific performance. Once the evaluation job is complete, you can review the results in the model evaluation report. Human model evaluation For human evaluation, you can have Amazon Bedrock set up human review workflows with a few clicks. You can bring your own datasets and define custom evaluation metrics, such as relevance, style, or alignment to brand voice. You also have the choice to either leverage your own internal teams as reviewers or engage an AWS managed team. This takes away the tedious effort of building and operating human evaluation workflows. To get started, create a new model evaluation and select Human: Bring your own team or Human: AWS managed team . If you choose an AWS managed team for human evaluation, describe your model evaluation needs, including task type, expertise of the work team, and the approximate number of prompts, along with your contact information. In the next step, an AWS expert will reach out to discuss your model evaluation project requirements in more detail. Upon review, the team will share a custom quote and project timeline. If you choose to bring your own team, follow the setup dialog to choose the FMs you want to evaluate and the type of task, for example, text summarization. Then, select the evaluation metrics, upload your test dataset, and set up the work team.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='06e353bd-db54-4d39-9689-8ab89637f05f', embedding=None, metadata={'title': 'Evaluate, compare, and select the best foundation models for your use case in Amazon Bedrock (preview)', 'summary': 'Experiment with different models in the playground environment, and to iterate faster, add automatic evaluations of the models. Then, when you prepare for launch or limited release, you can incorporate human reviews to help ensure quality.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='For human evaluation, you would format the example data shown before again in JSON Lines format like this (with category and referenceResponse being optional): {\"prompt\":\"Aurillac is the capital of\",\"referenceResponse\":\"Cantal\",\"category\":\"Capitals\"}\\n{\"prompt\":\"Bamiyan city is the capital of\",\"referenceResponse\":\"Bamiyan Province\",\"category\":\"Capitals\"}\\n{\"prompt\":\"Senftenberg is the capital of\",\"referenceResponse\":\"Oberspreewald-Lausitz\",\"category\":\"Capitals\"} Once the human evaluation is completed, Amazon Bedrock generates an evaluation report with the model’s performance against your selected metrics. Things to know Here are a couple of important things to know: Model support – During preview, you can evaluate and compare text-based large language models (LLMs) available on Amazon Bedrock. During preview, you can select one model for each automatic evaluation job and up to two models for each human evaluation job using your own team. For human evaluation using an AWS managed team, you can specify custom project requirements. Pricing – During preview, AWS only charges for the model inference needed to perform the evaluation (processed input and output tokens for on-demand pricing). There will be no separate charges for human evaluation or automatic evaluation. Amazon Bedrock Pricing has all the details. Join the preview Automatic evaluation and human evaluation using your own work team are available today in public preview in AWS Regions US East (N. Virginia) and US West (Oregon). Human evaluation using an AWS managed team is available in public preview in AWS Region US East (N. Virginia). To learn more, visit the Amazon Bedrock Developer Experience web page and check out the User Guide . Get started Log in to the AWS Management Console and start exploring model evaluation in Amazon Bedrock today! — Antje', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='5882a05d-39bf-4ac3-a8d7-e881a8195bd0', embedding=None, metadata={'title': 'Introducing Amazon SageMaker HyperPod, a purpose-built infrastructure for distributed training at scale', 'summary': 'You can now train FMs for weeks or months while SageMaker actively monitors the cluster health and provides automated node and job resiliency.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, we are introducing Amazon SageMaker HyperPod , which helps reducing time to train foundation models (FMs) by providing a purpose-built infrastructure for distributed training at scale. You can now use SageMaker HyperPod to train FMs for weeks or even months while SageMaker actively monitors the cluster health and provides automated node and job resiliency by replacing faulty nodes and resuming model training from a checkpoint. The clusters come preconfigured with SageMaker’s distributed training libraries that help you split your training data and model across all the nodes to process them in parallel and fully utilize the cluster’s compute and network infrastructure. You can further customize your training environment by installing additional frameworks, debugging tools, and optimization libraries. Let me show you how to get started with SageMaker HyperPod. In the following demo, I create a SageMaker HyperPod and show you how to train a Llama 2 7B model using the example shared in the AWS ML Training Reference Architectures GitHub repository. Create and manage clusters As the SageMaker HyperPod admin, you can create and manage clusters using the AWS Management Console or AWS Command Line Interface (AWS CLI) .\\xa0In the console , navigate to Amazon SageMaker, select Cluster management under HyperPod Clusters in the left menu, then choose Create a cluster . In the setup that follows, provide a cluster name and configure instance groups with your instance types of choice and the number of instances to allocate to each instance group. You also need to prepare and upload one or more lifecycle scripts to your Amazon Simple Storage Service (Amazon S3) bucket to run in each instance group during cluster creation. With lifecycle scripts, you can customize your cluster environment and install required libraries and packages. You can find example lifecycle scripts for SageMaker HyperPod in the GitHub repo. Using the AWS CLI You can also use the AWS CLI to create and manage clusters. For my demo, I specify my cluster configuration in a JSON file. I choose to create two instance groups, one for the cluster controller node(s) that I call “controller-group,” and one for the cluster worker nodes that I call “worker-group.” For the worker nodes that will perform model training, I specify Amazon EC2 Trn1 instances powered by AWS Trainium chips.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='1e4b7ca5-610f-45ba-96d9-6b7994f4ed77', embedding=None, metadata={'title': 'Introducing Amazon SageMaker HyperPod, a purpose-built infrastructure for distributed training at scale', 'summary': 'You can now train FMs for weeks or months while SageMaker actively monitors the cluster health and provides automated node and job resiliency.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='// demo-cluster.json\\n[\\n    {\\n        \"InstanceGroupName\": \"controller-group\",\\n        \"InstanceType\": \"ml.m5.xlarge\",\\n        \"InstanceCount\": 1,\\n        \"LifeCycleConfig\": {\\n            \"SourceS3Uri\": \"s3://<your-s3-bucket>/<lifecycle-script-directory>/\",\\n            \"OnCreate\": \"on_create.sh\"\\n            },\\n        \"ExecutionRole\": \"arn:aws:iam::111122223333:role/my-role-for-cluster\",\\n        \"ThreadsPerCore\": 1\\n    },\\n    {\\n        \"InstanceGroupName\": \"worker-group\",\\n        \"InstanceType\": \"ml.trn1.32xlarge\",\\n        \"InstanceCount\": 4,\\n        \"LifeCycleConfig\": {\\n            \"SourceS3Uri\": \"s3://<your-s3-bucket>/<lifecycle-script-directory>/\",\\n            \"OnCreate\": \"on_create.sh\"\\n            },\\n        \"ExecutionRole\": \"arn:aws:iam::111122223333:role/my-role-for-cluster\",\\n        \"ThreadsPerCore\": 1\\n    }\\n] To create the cluster, I run the following AWS CLI command: aws sagemaker create-cluster \\\\\\n    --cluster-name antje-demo-cluster \\\\\\n    --instance-groups file://demo-cluster.json Upon creation, you can use aws sagemaker describe-cluster and aws sagemaker list-cluster-nodes to view your cluster and node details. Note down the cluster ID and instance ID of your controller node. You need that information to connect to your cluster. You also have the option to attach a shared file system, such as Amazon FSx for Lustre . To use FSx for Lustre, you need to set up your cluster with an Amazon Virtual Private Cloud (Amazon VPC) configuration. Here’s an AWS CloudFormation template that shows how to create a SageMaker VPC and how to deploy FSx for Lustre . Connect to your cluster As a cluster user, you need to have access to the cluster provisioned by your cluster admin. With access permissions in place, you can connect to the cluster using SSH to schedule and run jobs. You can use the preinstalled AWS CLI plugin for AWS Systems Manager to connect to the controller node of your cluster. For my demo, I run the following command specifying my cluster ID and instance ID of the control node as the target.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='45da1e8b-a970-4ae5-a77d-b3200141c29a', embedding=None, metadata={'title': 'Introducing Amazon SageMaker HyperPod, a purpose-built infrastructure for distributed training at scale', 'summary': 'You can now train FMs for weeks or months while SageMaker actively monitors the cluster health and provides automated node and job resiliency.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='aws ssm start-session \\\\\\n    --target sagemaker-cluster:ntg44z9os8pn_controller-group-i-05a854e0d4358b59c \\\\\\n    --region us-west-2 Schedule and run jobs on the cluster using Slurm At launch, SageMaker HyperPod supports Slurm for workload orchestration. Slurm is a popular an open source cluster management and job scheduling system. You can install and set up Slurm through lifecycle scripts as part of the cluster creation. The example lifecycle scripts show how. Then, you can use the standard Slurm commands to schedule and launch jobs. Check out the Slurm Quick Start User Guide for architecture details and helpful commands. For this demo, I’m using this example from the AWS ML Training Reference Architectures GitHub repo that\\xa0shows how to train Llama 2 7B on Slurm with Trn1 instances. My cluster is already setup with Slurm, and I have an FSx for Lustre filesystem mounted. Note The Llama 2 model is governed by Meta . You can request access through the Meta request access page . Set up the cluster environment SageMaker HyperPod supports training in a range of environments, including Conda , venv , Docker , and enroot . Following the instructions in the README , I build my virtual environment aws_neuron_venv_pytorch and set up the torch_neuronx and neuronx-nemo-megatron libraries for training models on Trn1 instances. Prepare model, tokenizer, and dataset I follow the instructions to download the Llama 2 model and tokenizer and convert the model into the Hugging Face format. Then, I download and tokenize the RedPajama dataset . As a final preparation step, I pre-compile the Llama 2 model using ahead-of-time (AOT) compilation to speed up model training. Launch jobs on the cluster Now, I’m ready to start my model training job using the sbatch command. sbatch --nodes 4 --auto-resume=1 run.slurm ./llama_7b.sh You can use the squeue command to view the job queue. Once the training job is running, the SageMaker HyperPod resiliency features are automatically enabled. SageMaker HyperPod will automatically detect hardware failures, replace nodes as needed, and resume training from checkpoints if the auto-resume parameter is set, as shown in the preceding command.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='50575e10-3052-4a90-acb2-16d816cb102e', embedding=None, metadata={'title': 'Introducing Amazon SageMaker HyperPod, a purpose-built infrastructure for distributed training at scale', 'summary': 'You can now train FMs for weeks or months while SageMaker actively monitors the cluster health and provides automated node and job resiliency.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='You can view the output of the model training job in the following file: tail -f slurm-run.slurm-<JOB_ID>.out A sample output indicating that model training has started will look like this: Epoch 0:  22%|██▏       | 4499/20101 [22:26:14<77:48:37, 17.95s/it, loss=2.43, v_num=5563, reduced_train_loss=2.470, gradient_norm=0.121, parameter_norm=1864.0, global_step=4512.0, consumed_samples=1.16e+6, iteration_time=16.40]\\nEpoch 0:  22%|██▏       | 4500/20101 [22:26:32<77:48:18, 17.95s/it, loss=2.43, v_num=5563, reduced_train_loss=2.470, gradient_norm=0.121, parameter_norm=1864.0, global_step=4512.0, consumed_samples=1.16e+6, iteration_time=16.40]\\nEpoch 0:  22%|██▏       | 4500/20101 [22:26:32<77:48:18, 17.95s/it, loss=2.44, v_num=5563, reduced_train_loss=2.450, gradient_norm=0.120, parameter_norm=1864.0, global_step=4512.0, consumed_samples=1.16e+6, iteration_time=16.50] To further monitor and profile your model training jobs, you can use SageMaker hosted TensorBoard or any other tool of your choice. Now available SageMaker HyperPod is available today in AWS Regions US East (Ohio), US East (N. Virginia), US West (Oregon), Asia Pacific (Singapore), Asia Pacific (Sydney), Asia Pacific (Tokyo), Europe (Frankfurt), Europe (Ireland), and Europe (Stockholm). Learn more: See Amazon SageMaker HyperPod for pricing information and a list of supported cluster instance types Check out the Developer Guide Visit the AWS Management Console to start training your FMs with SageMaker HyperPod — Antje PS: Writing a blog post at AWS is always a team effort, even when you see only one name under the post title.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='767d5f99-d5f1-4bf7-a752-3ee34ea0e63a', embedding=None, metadata={'title': 'Introducing Amazon SageMaker HyperPod, a purpose-built infrastructure for distributed training at scale', 'summary': 'You can now train FMs for weeks or months while SageMaker actively monitors the cluster health and provides automated node and job resiliency.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='In this case, I want to thank Brad Doran , Justin Pirtle , Ben Snyder , Pierre-Yves Aquilanti , Keita Watanabe , and Verdi March for their generous help with example code and sharing their expertise in managing large-scale model training infrastructures, Slurm, and SageMaker HyperPod.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='7a4b1bf3-012e-4e6e-9bbc-1ac47a1fe1e9', embedding=None, metadata={'title': 'Amazon Titan Image Generator, Multimodal Embeddings, and Text models are now available in Amazon Bedrock', 'summary': 'Amazon Titan models incorporate 25 years of artificial intelligence (AI) and machine learning (ML) innovation at Amazon and offer a range of high-performing image, multimodal, and text model options through a fully managed API.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, we’re introducing two new Amazon Titan multimodal foundation models (FMs): Amazon Titan Image Generator (preview) and Amazon Titan Multimodal Embeddings. I’m also happy to share that Amazon Titan Text Lite and Amazon Titan Text Express are now generally available in Amazon Bedrock . You can now choose from three available Amazon Titan Text FMs, including Amazon Titan Text Embeddings. Amazon Titan models incorporate 25 years of artificial intelligence (AI) and machine learning (ML) innovation at Amazon and offer a range of high-performing image, multimodal, and text model options through a fully managed API. AWS pre-trained these models on large datasets, making them powerful, general-purpose models built to support a variety of use cases while also supporting the responsible use of AI. You can use the base models as is, or you can privately customize them with your own data. To enable access to Amazon Titan FMs, navigate to the Amazon Bedrock console and select Model access on the bottom left menu. On the model access overview page, choose Manage model access and enable access to the Amazon Titan FMs. Let me give you a quick tour of the new models. Amazon Titan Image Generator (preview) As a content creator, you can now use Amazon Titan Image Generator to quickly create and refine images using English natural language prompts. This helps companies in advertising, e-commerce, and media and entertainment to create studio-quality, realistic images in large volumes and at low cost. The model makes it easy to iterate on image concepts by generating multiple image options based on the text descriptions. The model can understand complex prompts with multiple objects and generates relevant images. It is trained on high-quality, diverse data to create more accurate outputs, such as realistic images with inclusive attributes and limited distortions. Titan Image Generator’s image editing features include the ability to automatically edit an image with a text prompt using a built-in segmentation model. The model supports inpainting with an image mask and outpainting to extend or change the background of an image. You can also configure image dimensions and specify the number of image variations you want the model to generate. In addition, you can customize the model with proprietary data to generate images consistent with your brand guidelines or to generate images in a specific style, for example, by fine-tuning the model with images from a previous marketing campaign. Titan Image Generator also mitigates harmful content generation to support the responsible use of AI .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='37d45f3b-c1bf-409c-b593-0e58071c412b', embedding=None, metadata={'title': 'Amazon Titan Image Generator, Multimodal Embeddings, and Text models are now available in Amazon Bedrock', 'summary': 'Amazon Titan models incorporate 25 years of artificial intelligence (AI) and machine learning (ML) innovation at Amazon and offer a range of high-performing image, multimodal, and text model options through a fully managed API.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='All images generated by Amazon Titan contain an invisible watermark, designed to help reduce the spread of misinformation by providing a discreet mechanism to identify AI-generated images. Amazon Titan Image Generator in action You can start using the model in the Amazon Bedrock console by submitting either an English natural language prompt to generate images or by uploading an image for editing. In the following example, I show you how to generate an image with Amazon Titan Image Generator using the AWS SDK for Python (Boto3) . First, let’s have a look at the configuration options for image generation that you can specify in the body of the inference request. For task type, I choose TEXT_IMAGE to create an image from a natural language prompt. import boto3\\nimport json\\n\\nbedrock = boto3.client(service_name=\"bedrock\")\\nbedrock_runtime = boto3.client(service_name=\"bedrock-runtime\")\\n\\n# ImageGenerationConfig Options:\\n#   numberOfImages: Number of images to be generated\\n#   quality: Quality of generated images, can be standard or premium\\n#   height: Height of output image(s)\\n#   width: Width of output image(s)\\n#   cfgScale: Scale for classifier-free guidance\\n#   seed: The seed to use for reproducibility  \\n\\nbody = json.dumps(\\n    {\\n        \"taskType\": \"TEXT_IMAGE\",\\n        \"textToImageParams\": {\\n            \"text\": \"green iguana\",   # Required\\n#           \"negativeText\": \"<text>\"  # Optional\\n        },\\n        \"imageGenerationConfig\": {\\n            \"numberOfImages\": 1,   # Range: 1 to 5 \\n            \"quality\": \"premium\",  # Options: standard or premium\\n            \"height\": 768,         # Supported height list in the docs \\n            \"width\": 1280,         # Supported width list in the docs\\n            \"cfgScale\": 7.5,       # Range: 1.0 (exclusive) to 10.0\\n            \"seed\": 42             # Range: 0 to 214783647\\n        }\\n    }\\n) Next, specify the model ID for Amazon Titan Image Generator and use the InvokeModel API to send the inference request. response = bedrock_runtime.invoke_model(\\n    body=body, \\n    modelId=\"amazon.titan-image-generator-v1\",\\n    accept=\"application/json\", \\n    contentType=\"application/json\"\\n) Then, parse the response and decode the base64-encoded image.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='8662a36f-5b8b-4958-be4e-631e610c3383', embedding=None, metadata={'title': 'Amazon Titan Image Generator, Multimodal Embeddings, and Text models are now available in Amazon Bedrock', 'summary': 'Amazon Titan models incorporate 25 years of artificial intelligence (AI) and machine learning (ML) innovation at Amazon and offer a range of high-performing image, multimodal, and text model options through a fully managed API.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='import base64\\nfrom PIL import Image\\nfrom io import BytesIO\\n\\nresponse_body = json.loads(response.get(\"body\").read())\\nimages = [Image.open(BytesIO(base64.b64decode(base64_image))) for base64_image in response_body.get(\"images\")]\\n\\nfor img in images:\\n    display(img) Et voilà, here’s the green iguana (one of my favorite animals, actually): To learn more about all the Amazon Titan Image Generator features, visit the Amazon Titan product page. (You’ll see more of the iguana over there.) Next, let’s use this image with the new Amazon Titan Multimodal Embeddings model. Amazon Titan Multimodal Embeddings Amazon Titan Multimodal Embeddings helps you build more accurate and contextually relevant multimodal search and recommendation experiences for end users. Multimodal refers to a system’s ability to process and generate information using distinct types of data (modalities). With Titan Multimodal Embeddings, you can submit text, image, or a combination of the two as input. The model converts images and short English text up to 128 tokens into embeddings, which capture semantic meaning and relationships between your data. You can also fine-tune the model on image-caption pairs. For example, you can combine text and images to describe company-specific manufacturing parts to understand and identify parts more effectively. By default, Titan Multimodal Embeddings generates vectors of 1,024 dimensions, which you can use to build search experiences that offer a high degree of accuracy and speed. You can also configure smaller vector dimensions to optimize for speed and price performance. The model provides an asynchronous batch API, and the Amazon OpenSearch Service will soon offer a connector that adds Titan Multimodal Embeddings support for neural search . Amazon Titan Multimodal Embeddings in action For this demo, I create a combined image and text embedding. First, I base64-encode my image, and then I specify either inputText , inputImage , or both in the body of the inference request.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='80093502-19ad-4d8b-9d53-e98c8845cc99', embedding=None, metadata={'title': 'Amazon Titan Image Generator, Multimodal Embeddings, and Text models are now available in Amazon Bedrock', 'summary': 'Amazon Titan models incorporate 25 years of artificial intelligence (AI) and machine learning (ML) innovation at Amazon and offer a range of high-performing image, multimodal, and text model options through a fully managed API.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Maximum image size supported is 2048 x 2048 pixels\\nwith open(\"iguana.png\", \"rb\") as image_file:\\n    input_image = base64.b64encode(image_file.read()).decode(\\'utf8\\')\\n\\n# You can specify either text or image or both\\nbody = json.dumps(\\n    {\\n        \"inputText\": \"Green iguana on tree branch\",\\n        \"inputImage\": input_image\\n    }\\n) Next, specify the model ID for Amazon Titan Multimodal Embeddings and use the InvokeModel API to send the inference request. response = bedrock_runtime.invoke_model(\\n\\tbody=body, \\n\\tmodelId=\"amazon.titan-embed-image-v1\", \\n\\taccept=\"application/json\", \\n\\tcontentType=\"application/json\"\\n) Let’s see the response. response_body = json.loads(response.get(\"body\").read())\\nprint(response_body.get(\"embedding\"))\\n\\t\\n[-0.015633494, -0.011953583, -0.022617092, -0.012395329, 0.03954641, 0.010079376, 0.08505301, -0.022064181, -0.0037248489, ...] I redacted the output for brevity. The distance between multimodal embedding vectors, measured with metrics like cosine similarity or euclidean distance, shows how similar or different the represented information is across modalities. Smaller distances mean more similarity, while larger distances mean more dissimilarity. As a next step, you could build an image database by storing and indexing the multimodal embeddings in a vector store or vector database. To implement text-to-image search, query the database with inputText . For image-to-image search, query the database with inputImage . For image+text-to-image search, query the database with both inputImage and inputText . Amazon Titan Text Amazon Titan Text Lite and Amazon Titan Text Express are large language models (LLMs) that support a wide range of text-related tasks, including summarization, translation, and conversational chatbot systems. They can also generate code and are optimized to support popular programming languages and text formats like JSON and CSV. Titan Text Express – Titan Text Express has a maximum context length of 8,192 tokens and is ideal for a wide range of tasks, such as open-ended text generation and conversational chat, and support within Retrieval Augmented Generation (RAG) workﬂows.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='2c5746c9-83ae-486c-89f0-985d7a936646', embedding=None, metadata={'title': 'Amazon Titan Image Generator, Multimodal Embeddings, and Text models are now available in Amazon Bedrock', 'summary': 'Amazon Titan models incorporate 25 years of artificial intelligence (AI) and machine learning (ML) innovation at Amazon and offer a range of high-performing image, multimodal, and text model options through a fully managed API.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Titan Text Lite – Titan Text Lite has a maximum context length of 4,096 tokens and is a price-performant version that is ideal for English-language tasks. The model is highly customizable and can be fine-tuned for tasks such as article summarization and copywriting. Amazon Titan Text in action For this demo, I ask Titan Text to write an email to my team members suggesting they organize a live stream: “Compose a short email from Antje, Principal Developer Advocate, encouraging colleagues in the developer relations team to organize a live stream to demo our new Amazon Titan V1 models.” body = json.dumps({\\n    \"inputText\": prompt, \\n    \"textGenerationConfig\":{  \\n        \"maxTokenCount\":512,\\n        \"stopSequences\":[],\\n        \"temperature\":0,\\n        \"topP\":0.9\\n    }\\n}) Titan Text FMs support temperature and topP inference parameters to control the randomness and diversity of the response, as well as maxTokenCount and stopSequences to control the length of the response. Next, choose the model ID for one of the Titan Text models and use the InvokeModel API to send the inference request. response = bedrock_runtime.invoke_model(\\n    body=body,\\n\\t# Choose modelID\\n\\t# Titan Text Express: \"amazon.titan-text-express-v1\"\\n\\t# Titan Text Lite: \"amazon.titan-text-lite-v1\"\\n\\tmodelID=\"amazon.titan-text-express-v1\",\\n    accept=\"application/json\", \\n    contentType=\"application/json\"\\n) Let’s have a look at the response. response_body = json.loads(response.get(\\'body\\').read())\\noutputText = response_body.get(\\'results\\')[0].get(\\'outputText\\')\\n\\ntext = outputText[outputText.index(\\'\\\\n\\')+1:]\\nemail = text.strip()\\nprint(email) Subject: Demo our new Amazon Titan V1 models live! Dear colleagues, I hope this email finds you well. I am excited to announce that we have recently launched our new Amazon Titan V1 models, and I believe it would be a great opportunity for us to showcase their capabilities to the wider developer community. I suggest that we organize a live stream to demo these models and discuss their features, benefits, and how they can help developers build innovative applications. This live stream could be hosted on our YouTube channel, Twitch, or any other platform that is suitable for our audience.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e35922d3-969c-478e-a2a9-a47c6c61b043', embedding=None, metadata={'title': 'Amazon Titan Image Generator, Multimodal Embeddings, and Text models are now available in Amazon Bedrock', 'summary': 'Amazon Titan models incorporate 25 years of artificial intelligence (AI) and machine learning (ML) innovation at Amazon and offer a range of high-performing image, multimodal, and text model options through a fully managed API.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='I believe that showcasing our new models will not only increase our visibility but also help us build stronger relationships with developers. It will also provide an opportunity for us to receive feedback and improve our products based on the developer’s needs. If you are interested in organizing this live stream, please let me know. I am happy to provide any support or guidance you may need. Together, let’s make this live stream a success and showcase the power of Amazon Titan V1 models to the world! Best regards, Antje Principal Developer Advocate Nice. I could send this email right away! Availability and pricing Amazon Titan Text FMs are available today in AWS Regions US East (N. Virginia), US West (Oregon), Asia Pacific (Singapore, Tokyo), and Europe (Frankfurt). Amazon Titan Multimodal Embeddings is available today in the AWS Regions US East (N. Virginia) and US West (Oregon). Amazon Titan Image Generator is available in public preview in the AWS Regions US East (N. Virginia) and US West (Oregon). For pricing details, see the Amazon Bedrock Pricing page. Learn more Amazon Bedrock product page Amazon Titan product page Amazon Bedrock User Guide. Go to the AWS Management Console to start building generative AI applications with Amazon Titan FMs on Amazon Bedrock today! — Antje', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='bbbdc559-e2ed-4f0b-a046-6fe0e6f9df03', embedding=None, metadata={'title': 'Amazon Bedrock now provides access to Anthropic’s latest model, Claude 2.1', 'summary': 'The newest model offers an industry-leading 200,000 token context window, reduced rates of hallucination, improved accuracy over long documents, system prompts, and a beta tool use feature for function calling and workflow orchestration.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, we’re announcing the availability of Anthropic’s Claude 2.1 foundation model (FM) in Amazon Bedrock . Last week, Anthropic introduced its latest model, Claude 2.1 , delivering key capabilities for enterprises such as an industry-leading 200,000 token context window (2x the context of Claude 2.0), reduced rates of hallucination, improved accuracy over long documents, system prompts, and a beta tool use feature for function calling and workflow orchestration. With Claude 2.1’s availability in Amazon Bedrock, you can build enterprise-ready generative artificial intelligence (AI) applications using more honest and reliable AI systems from Anthropic. You can now use the Claude 2.1 model provided by Anthropic in the Amazon Bedrock console . Here are some key highlights about the new Claude 2.1 model in Amazon Bedrock: 200,000 token context window – Enterprise applications demand larger context windows and more accurate outputs when working with long documents such as product guides, technical documentation, or financial or legal statements. Claude 2.1 supports 200,000 tokens, the equivalent of roughly 150,000 words or over 500 pages of documents. When uploading extensive information to Claude, you can summarize, perform Q&A, forecast trends, and compare and contrast multiple documents for drafting business plans and analyzing complex contracts. Strong accuracy upgrades – Claude 2.1 has also made significant gains in honesty, with a 2x decrease in hallucination rates, 50 percent fewer hallucinations in open-ended conversation and document Q&A, a 30 percent reduction in incorrect answers, and a 3–4 times lower rate of mistakenly concluding that a document supports a particular claim compared to Claude 2.0. Claude increasingly knows what it doesn’t know and will more likely demur rather than hallucinate. With this improved accuracy, you can build more reliable, mission-critical applications for your customers and employees. System prompts – Claude 2.1 now supports system prompts, a new feature that can improve Claude’s performance in a variety of ways, including greater character depth and role adherence in role-playing scenarios, particularly over longer conversations, as well as stricter adherence to guidelines, rules, and instructions. This represents a structural change, but not a content change from former ways of prompting Claude.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='5416db75-24ac-489d-a94f-b369dd6fc1cb', embedding=None, metadata={'title': 'Amazon Bedrock now provides access to Anthropic’s latest model, Claude 2.1', 'summary': 'The newest model offers an industry-leading 200,000 token context window, reduced rates of hallucination, improved accuracy over long documents, system prompts, and a beta tool use feature for function calling and workflow orchestration.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Tool use for function calling and workflow orchestration –\\xa0Available as a beta feature, Claude 2.1 can now integrate with your existing internal processes, products, and APIs to build generative AI applications. Claude 2.1 accurately retrieves and processes data from additional knowledge sources as well as invokes functions for a given task. \\xa0Claude 2.1 can answer questions by searching databases using private APIs and a web search API, translate natural language requests into structured API calls, or connect to product datasets to make recommendations and help customers complete purchases. Access to this feature is currently limited to select early access partners, with plans for open access in the near future. If you are interested in gaining early access, please contact your AWS account team. To learn more about Claude 2.1’s features and capabilities, visit Anthropic Claude on Amazon Bedrock and the Amazon Bedrock documentation . Claude 2.1 in action To get started with Claude 2.1 in Amazon Bedrock, go to the Amazon Bedrock console . Choose Model access on the bottom left pane, then choose Manage model access on the top right side, submit your use case, and request model access to the Anthropic Claude model. It may take several minutes to get access to models. If you already have access to the Claude model, you don’t need to request access separately for Claude 2.1. To test Claude 2.1 in chat mode, choose Text or Chat under Playgrounds in the left menu pane. Then select Anthropic and then Claude v2.1 . By choosing View API request , you can also access the model via code examples in the AWS Command Line Interface (AWS CLI) and AWS SDKs. Here is a sample of the AWS CLI command: $ aws bedrock-runtime invoke-model \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0 --model-id anthropic.claude-v2:1 \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0 --body {\"prompt\":\"\\\\n\\\\nHuman: Tell me funny joke about outer space\\\\n\\\\nAssistant:\\\\n Why can\\'t you trust atoms? They make up everything!', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='33153c20-1c4a-420e-b36b-23337b4b8d35', embedding=None, metadata={'title': 'Amazon Bedrock now provides access to Anthropic’s latest model, Claude 2.1', 'summary': 'The newest model offers an industry-leading 200,000 token context window, reduced rates of hallucination, improved accuracy over long documents, system prompts, and a beta tool use feature for function calling and workflow orchestration.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\",\"max_tokens_to_sample\":300,\"temperature\":1,\"top_k\":250,\"top_p\":0.999,\"stop_sequences\":[\"\\\\n\\\\nHuman:\"],\"anthropic_version\":\"bedrock-2023-05-31\"} \\\\\\n \\xa0\\xa0\\xa0\\xa0 --cli-binary-format raw-in-base64-out \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0 invoke-model-output.txt You can use system prompt engineering techniques provided by the Claude 2.1 model, where you place your inputs and documents before any questions that reference or utilize that content. Inputs can be natural language text, structured documents, or code snippets using <document> , <papers> , <books> , or <code> tags, and so on. You can also use conversational text, such as chat history, and Retrieval Augmented Generation (RAG) results, such as chunked documents. Here is a system prompt example for support agents to respond to customer questions based on corporate documents. Here are some documents for you to reference for your task:\\n<documents>\\n\\xa0<document index=\"1\">\\n\\xa0 <document_content>\\n\\xa0 (the text content of the document - could be a passage, web page, article, etc)\\n\\xa0\\xa0 </document_content>\\n<document index=\"2\">\\n\\xa0 <source>https://mycompany.repository/userguide/what-is-it.html</source>\\n</document>\\n<document index=\"3\">\\n\\xa0 <source>https://mycompany.repository/docs/techspec.pdf</source>\\n\\xa0</document>\\n...\\n</documents>\\n\\nYou are Larry, and you are a customer advisor with deep knowledge of your company\\'s products. Larry has a great deal of patience with his customers, even when they say nonsense or are sarcastic. Larry\\'s answers are polite but sometimes funny. However, he only answers questions about the company\\'s products and doesn\\'t know much about other questions. Use the provided documentation to answer user questions.\\n\\nHuman: Your product is making a weird stuttering sound when I operate. What might be the problem? To learn more about prompt engineering on Amazon Bedrock, see the Prompt engineering guidelines included in the Amazon Bedrock documentation. You can learn general prompt techniques, templates, and examples for Amazon Bedrock text models, including Claude. Now available Claude 2.1 is available today in the US East (N. Virginia) and US West (Oregon) Regions. You only pay for what you use, with no time-based term commitments for on-demand mode.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f115e19b-e2f9-4b84-b140-e9f76415cf9c', embedding=None, metadata={'title': 'Amazon Bedrock now provides access to Anthropic’s latest model, Claude 2.1', 'summary': 'The newest model offers an industry-leading 200,000 token context window, reduced rates of hallucination, improved accuracy over long documents, system prompts, and a beta tool use feature for function calling and workflow orchestration.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='For text generation models, you are charged for every input token processed and every output token generated. Or you can choose the provisioned throughput mode to meet your application’s performance requirements in exchange for a time-based term commitment. To learn more, see Amazon Bedrock Pricing . Give Anthropic Claude 2.1 a try in Amazon Bedrock console today and send feedback to AWS re:Post for Amazon Bedrock or through your usual AWS Support contacts. — Channy', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='4630587e-7d9d-4d77-9111-3efa3590d9e9', embedding=None, metadata={'title': 'Introducing Amazon Q, a new generative AI-powered assistant (preview)', 'summary': 'You can use Amazon Q in your work to have conversations, solve problems, generate content, gain insights, and take action by connecting to your company’s information repositories, code, data, and enterprise systems.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, we are announcing Amazon Q , a new generative artificial intelligence- (AI)-powered assistant designed for work that can be tailored to your business. You can use Amazon Q to have conversations, solve problems, generate content, gain insights, and take action by connecting to your company’s information repositories, code, data, and enterprise systems. Amazon Q provides immediate, relevant information and advice to employees to streamline tasks, accelerate decision-making and problem-solving, and help spark creativity and innovation at work. Amazon Q offers user-based plans, so you get features, pricing, and options tailored to how you use the product. Amazon Q can adapt its interactions to each individual user based on the existing identities, roles, and permissions of your business. AWS never uses customers’ content from Amazon Q to train the underlying models.\\xa0In other words, your company information remains secure and private. In this post, I’ll give you a quick tour of how you can use Amazon Q for general business use. For information on how developers and IT professionals can use Amazon Q, see: Amazon Q brings generative AI-powered assistance to IT pros and developers (preview) For information on how business analysts can use Amazon Q in QuickSight, see: New Amazon Q in QuickSight uses generative AI assistance for quicker, easier data insights (preview) For information on how contact center agents can use Amazon Q in Connect, see: New generative AI features in Amazon Connect, including Amazon Q, facilitate improved contact center service Amazon Q is your business expert Let’s look at a few examples of how Amazon Q can help business users complete tasks using simple natural language prompts. As a marketing manager, you could ask Amazon Q to transform a press release into a blog post, create a summary of the press release, or create an email draft based on the provided release. Amazon Q searches through your company content, which can include internal style guides, for example, to provide a response appropriate to your company’s brand standards. Then, you could ask Amazon Q to generate tailored social media prompts to promote your story through each of your social media channels. Later, you can ask Amazon Q to analyze the results of your campaign and summarize them for leadership reviews.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='dc78ca16-76ad-4027-9dc3-8dde81756960', embedding=None, metadata={'title': 'Introducing Amazon Q, a new generative AI-powered assistant (preview)', 'summary': 'You can use Amazon Q in your work to have conversations, solve problems, generate content, gain insights, and take action by connecting to your company’s information repositories, code, data, and enterprise systems.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='In the following example, I deployed Amazon Q with access to my AWS News Blog posts from 2023 and called the assistant “AWS Blog Expert.” Coming back to my previous example, let’s assume I’m a marketing manager and want Amazon Q to help me create social media posts for recent company blog posts. I enter the following prompt: “Summarize the key insights from Antje’s recent AWS Weekly Roundup posts and craft a compelling social media post that not only highlights the most important points but also encourages engagement. Consider our target audience and aim for a tone that aligns with our brand identity. The social media post should be concise, informative, and enticing to encourage readers to click through and read the full articles. Please ensure the content is shareable and includes relevant hashtags for maximum visibility.” Behind the scenes, Amazon Q searches the documents in connected data sources and creates a relevant and detailed suggestion for a social media post based on my blog posts. Amazon Q also tells me which document was used to generate the answer. In this case, it is PDF file of the blog posts in question. As an administrator, you can define the context for responses, restrict irrelevant topics, and configure whether to respond only using trusted company information or complement responses with knowledge from the underlying model. Restricting responses to trusted company information helps mitigate hallucinations, a common phenomenon where the underlying model generates responses that sound plausible but are based on misinterpreted or nonexistent data. Amazon Q provides fine-grained access controls that restrict responses to only using data or acting based on the employee’s level of access and provides citations and references to the original sources for fact-checking and traceability. You can choose among 40+ built-in connectors for popular data sources and enterprise systems, including Amazon S3 , Google Drive, Microsoft SharePoint, Salesforce, ServiceNow, and Slack. How to tailor Amazon Q to your business To tailor Amazon Q to your business, navigate to Amazon Q in the console , select Applications in the left menu, and choose Create application . This starts the following workflow. Step 1 . Create application . Provide an application name and create a new or select an existing AWS Identity and Access Management (IAM) service role that Amazon Q is allowed to assume. I call my application AWS-Blog-Expert . Then, choose Create . Step 2 . Select retriever . A retriever pulls data from the index in real time during a conversation.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='1832114e-f855-4f77-bad9-eed14764fd28', embedding=None, metadata={'title': 'Introducing Amazon Q, a new generative AI-powered assistant (preview)', 'summary': 'You can use Amazon Q in your work to have conversations, solve problems, generate content, gain insights, and take action by connecting to your company’s information repositories, code, data, and enterprise systems.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='You can choose between two options: use the Amazon Q native retriever or use an existing Amazon Kendra retriever. The native retriever can connect to the Amazon Q supported data sources. If you already use Amazon Kendra, you can select the existing Amazon Kendra retriever to connect the associated data sources to your Amazon Q application. I select the native retriever option. Then, choose Next . Step 3 . Connect data sources . Amazon Q comes with built-in connectors for popular data sources and enterprise systems. For this demo, I choose Amazon S3 and configure the data source by pointing to my S3 bucket with the PDFs of my blog posts. Once the data source sync is successfully complete and the retriever shows the accurate document count, you can preview the web experience and start a conversation. Note that the data source sync can take from a few minutes to a few hours, depending on the amount and size of data to index. You can also connect plugins that manage access to enterprise systems, including ServiceNow, Jira, Salesforce, and Zendesk. Plugins enable Amazon Q to perform user-requested tasks, such as creating support tickets or analyzing sales forecasts. Preview and deploy web experience In the application overview, choose Preview web experience . This opens the web experience with the conversational interface to chat with the tailored Amazon Q AWS Blog Expert. In the final step, you deploy the Amazon Q web experience. You can integrate your SAML 2.0–compliant external identity provider (IdP) using IAM. Amazon Q can work with any IdP that’s compliant with SAML 2.0. Amazon Q uses service-initiated single sign-on (SSO) to authenticate users. Join the preview Amazon Q is available today in preview in AWS Regions US East (N. Virginia) and US West (Oregon). Visit the product page to learn how Amazon Q can become your expert in your business. Also, check out the Amazon Q Slack Gateway GitHub repository that shows how to make Amazon Q available to users as a Slack Bot application.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='cfa15660-38e3-4f60-b74d-2a1743f67c5f', embedding=None, metadata={'title': 'Introducing Amazon Q, a new generative AI-powered assistant (preview)', 'summary': 'You can use Amazon Q in your work to have conversations, solve problems, generate content, gain insights, and take action by connecting to your company’s information repositories, code, data, and enterprise systems.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Learn more Amazon Q main product page Amazon Q for general business use Read more about Amazon Q Amazon Q brings generative AI-powered assistance to IT pros and developers (preview) Improve developer productivity with generative-AI powered Amazon Q in Amazon CodeCatalyst (preview) Upgrade your Java applications with Amazon Q Code Transformation (preview) New generative AI features in Amazon Connect, including Amazon Q, facilitate improved contact center service New Amazon Q in QuickSight uses generative AI assistance for quicker, easier data insights (preview) — Antje', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='dacd1a81-cfa3-4091-a695-b6f20e2d8f5b', embedding=None, metadata={'title': 'Amazon Q brings generative AI-powered assistance to IT pros and developers (preview)', 'summary': 'With Amazon Q, you minimize the time and effort you need to gain the knowledge required to answer AWS questions, explore new AWS capabilities, learn unfamiliar technologies, and architect solutions that fuel innovation.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, we are announcing the preview of Amazon Q , a new type of generative artificial intelligence (AI) powered assistant that is specifically for work and can be tailored to a customer’s business. Amazon Q brings a set of capabilities to support developers and IT professionals. Now you can use Amazon Q to get started building applications on AWS, research best practices, resolve errors, and get assistance in coding new features for your applications. For example, Amazon Q Code Transformation can perform Java application upgrades now, from version 8 and 11 to version 17. Amazon Q is available in multiple areas of AWS to provide quick access to answers and ideas wherever you work. Here’s a quick look at Amazon Q, including in integrated development environment (IDE): Building applications together with Amazon Q Application development is a journey. It involves a continuous cycle of researching, developing, deploying, optimizing, and maintaining. At each stage, there are many questions—from figuring out the right AWS services to use, to troubleshooting issues in the application code. Trained on 17 years of AWS knowledge and best practices, Amazon Q is designed to help you at each stage of development with a new experience for building applications on AWS. With Amazon Q, you minimize the time and effort you need to gain the knowledge required to answer AWS questions, explore new AWS capabilities, learn unfamiliar technologies, and architect solutions that fuel innovation. Let us show you some capabilities of Amazon Q. 1. Conversational Q&A capability You can interact with the Amazon Q conversational Q&A capability to get started, learn new things, research best practices, and iterate on how to build applications on AWS without needing to shift focus away from the AWS console. To start using this feature, you can select the Amazon Q icon on the right-hand side of the AWS Management Console . For example, you can ask, “What are AWS serverless services to build serverless APIs?” Amazon Q provides concise explanations along with references you can use to follow up on your questions and validate the guidance. You can also use Amazon Q to follow up on and iterate your questions. Amazon Q will show more deep-dive answers for you with references. There are times when we have questions for a use case with fairly specific requirements. With Amazon Q, you can elaborate on your use cases in more detail to provide context.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='4e088ac9-d424-4015-bb62-cc6b5c08da4f', embedding=None, metadata={'title': 'Amazon Q brings generative AI-powered assistance to IT pros and developers (preview)', 'summary': 'With Amazon Q, you minimize the time and effort you need to gain the knowledge required to answer AWS questions, explore new AWS capabilities, learn unfamiliar technologies, and architect solutions that fuel innovation.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='For example, you can ask Amazon Q, “I’m planning to create serverless APIs with 100k requests/day. Each request needs to lookup into the database. What are the best services for this workload?” Amazon Q responds with a list of AWS services you can use and tries to limit the answer results to those that are accurately referenceable and verified with best practices. Here is some additional information that you might want to note: Amazon Q conversational Q&A capability is currently in preview in all commercial AWS Regions. This capability is integrated into the AWS Management Console , AWS Console Mobile Application , AWS Documentation , AWS websites , and Slack and Teams through AWS Chatbot , making it more convenient and easier to find what you need. 2. Optimize Amazon EC2 instance selection Choosing the right Amazon Elastic Compute Cloud (Amazon EC2) instance type for your workload can be challenging with all the options available. Amazon Q aims to make this easier by providing personalized recommendations. To use this feature, you can ask Amazon Q, “Which instance families should I use to deploy a Web App Server for hosting an application?” This feature is also available when you choose to launch an instance in the Amazon EC2 console . In Instance type , you can select Get advice on instance type selection . This will show a dialog to define your requirements. Your requirements are automatically translated into a prompt on the Amazon Q chat panel. Amazon Q returns with a list of suggestions of EC2 instances that are suitable for your use cases. This capability helps you pick the right instance type and settings so your workloads will run smoothly and more cost-efficiently. This capability to provide EC2 instance type recommendations based on your use case is available in preview in all commercial AWS Regions. 3. Troubleshoot and solve errors directly in the console Amazon Q can also help you to solve errors for various AWS services directly in the console. With Amazon Q proposed solutions, you can avoid slow manual log checks or research. Let’s say that you have an AWS Lambda function that tries to interact with an Amazon DynamoDB table. But, for an unknown reason (yet), it fails to run. Now, with Amazon Q, you can troubleshoot and resolve this issue faster by selecting Troubleshoot with Amazon Q . Amazon Q provides concise analysis of the error which helps you to understand the root cause of the problem and the proposed resolution.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='c716275b-ea45-4297-89e4-e2d8c8efbb5b', embedding=None, metadata={'title': 'Amazon Q brings generative AI-powered assistance to IT pros and developers (preview)', 'summary': 'With Amazon Q, you minimize the time and effort you need to gain the knowledge required to answer AWS questions, explore new AWS capabilities, learn unfamiliar technologies, and architect solutions that fuel innovation.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='With this information, you can follow the steps described by Amazon Q to fix the issue. In just a few minutes, you will have the solution to solve your issues, saving significant time without disrupting your development workflow. The Amazon Q capability to help you troubleshoot errors in the console is available in preview in the US West (Oregon) for Amazon Elastic Compute Cloud (Amazon EC2) , Amazon Simple Storage Service (Amazon S3) , Amazon ECS , and AWS Lambda . 4. Network troubleshooting assistance You can also ask Amazon Q to assist you in troubleshooting network connectivity issues caused by network misconfiguration in your current AWS account. For this capability, Amazon Q works with Amazon VPC Reachability Analyzer to check your connections and inspect your network configuration to identify potential issues. This makes it easy to diagnose and resolve AWS networking problems, such as “Why can’t I SSH to my EC2 instance?” or “Why can’t I reach my web server from the Internet?” which you can ask Amazon Q. Then, on the response text, you can select preview experience here , which will provide explanations to help you to troubleshoot network connectivity-related issues. Here are a few things you need to know: This capability is currently available in preview in the US East (N. Virginia). To learn more about the feature and sample questions, see Getting started with Amazon Q network troubleshooting in the AWS Documentation. 5. Integration and conversational capabilities within your IDEs As we mentioned, Amazon Q is also available in supported IDEs. This allows you to ask questions and get help within your IDE by chatting with Amazon Q or invoking actions by typing / in the chat box. To get started, you need to install or update the latest AWS Toolkit and sign in to Amazon CodeWhisperer . Once you’re signed in to Amazon CodeWhisperer, it will automatically activate the Amazon Q conversational capability in the IDE. With Amazon Q enabled, you can now start chatting to get coding assistance. You can ask Amazon Q to describe your source code file. From here, you can improve your application, for example, by integrating it with Amazon DynamoDB. You can ask Amazon Q, “Generate code to save data into DynamoDB table called save_data() accepting data parameter and return boolean status if the operation successfully runs.” Once you’ve reviewed the generated code, you can do a manual copy and paste into the editor.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='5ea1b39f-5ea0-42f5-9888-976e65807cf0', embedding=None, metadata={'title': 'Amazon Q brings generative AI-powered assistance to IT pros and developers (preview)', 'summary': 'With Amazon Q, you minimize the time and effort you need to gain the knowledge required to answer AWS questions, explore new AWS capabilities, learn unfamiliar technologies, and architect solutions that fuel innovation.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='You can also select Insert at cursor to place the generated code into the source code directly. This feature makes it really easy to help you focus on building applications because you don’t have to leave your IDE to get answers and context-specific coding guidance. You can try the preview of this feature in Visual Studio Code and JetBrains IDEs. 6. Feature development capability Another exciting feature that Amazon Q provides is guiding you interactively from idea to building new features within your IDE and Amazon CodeCatalyst . You can go from a natural language prompt to application features in minutes, with interactive step-by-step instructions and best practices, right from your IDE. With a prompt, Amazon Q will attempt to understand your application structure and break down your prompt into logical, atomic implementation steps. To use this capability, you can start by invoking an action command /dev in Amazon Q and describe the task you need Amazon Q to process. Then, from here, you can review, collaborate and guide Amazon Q in the chat for specific areas that need to be implemented. Additional capabilities to help you ship features faster with complete pull requests are available if you’re using Amazon CodeCatalyst. In Amazon CodeCatalyst, you can assign a new or an existing issue to Amazon Q, and it will process an end-to-end development workflow for you. Amazon Q will review the existing code, propose a solution approach, seek feedback from you on the approach, generate merge-ready code, and publish a pull request for review. All you need to do after is to review the proposed solutions from Amazon Q. The following screenshots show a pull request created by Amazon Q in Amazon CodeCatalyst. Here are a couple of things that you should know: Amazon Q feature development capability is currently in preview in Visual Studio Code and Amazon CodeCatalyst To use this capability in IDE, you need to have the Amazon CodeWhisperer Professional tier. Learn more on the Amazon CodeWhisperer pricing page. To learn more about this feature, read: Improve developer productivity with generative-AI powered Amazon Q in Amazon CodeCatalyst (preview) 7. Upgrade applications with Amazon Q Code Transformation With Amazon Q, you can now upgrade an entire application within a few hours by starting a guided code transformation. This capability, called Amazon Q Code Transformation, simplifies maintaining, migrating, and upgrading your existing applications. To start, navigate to the CodeWhisperer section and then select Transform .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='9e29e9de-92cf-4e01-bdb7-5a6556cce73b', embedding=None, metadata={'title': 'Amazon Q brings generative AI-powered assistance to IT pros and developers (preview)', 'summary': 'With Amazon Q, you minimize the time and effort you need to gain the knowledge required to answer AWS questions, explore new AWS capabilities, learn unfamiliar technologies, and architect solutions that fuel innovation.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Amazon Q Code Transformation automatically analyzes your existing codebase, generates a transformation plan, and completes the key transformation tasks suggested by the plan. Some additional information about this feature: Amazon Q Code Transformation is available in preview today in the AWS Toolkit for IntelliJ IDEA and the AWS Toolkit for Visual Studio Code. To use this capability, you need to have the Amazon CodeWhisperer Professional tier during the preview. During preview, you can can upgrade Java 8 and 11 applications to version 17, a Java Long-Term Support (LTS) release. To learn more about this feature, read: Upgrade your Java applications with Amazon Q Code Transformation (preview) Get started with Amazon Q today With Amazon Q, you have an AI expert by your side to answer questions, write code faster, troubleshoot issues, optimize workloads, and even help you code new features. These capabilities simplify every phase of building applications on AWS. Amazon Q lets you engage with AWS Support agents directly from the Q interface if additional assistance is required, eliminating any dead ends in the customer’s self-service experience. The integration with AWS Support is available in the console and will honor the entitlements of your AWS Support plan. Learn more Amazon Q main product page Amazon Q details for IT pros and developers Get started with Amazon Q Read more about Amazon Q Introducing Amazon Q, a new generative AI-powered assistant (preview) Improve developer productivity with generative-AI powered Amazon Q in Amazon CodeCatalyst (preview) Upgrade your Java applications with Amazon Q Code Transformation (preview) New generative AI features in Amazon Connect, including Amazon Q, facilitate improved contact center service New Amazon Q in QuickSight uses generative AI assistance for quicker, easier data insights (preview) — Donnie & Channy On November 29, 2023, the screenshot that showed how to get recommendations for Amazon EC2 instances in Amazon Q was updated to reflect latest improvements in the product.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='b50e161f-d7f9-4774-a9ae-0099c1ef3fa9', embedding=None, metadata={'title': 'Guardrails for Amazon Bedrock helps implement safeguards customized to your use cases and responsible AI policies (preview)', 'summary': 'Promote safe interactions between users and your generative AI applications by implementing safeguards customized to your use cases and responsible AI policies.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='As part of your responsible artificial intelligence (AI) strategy, you can now use Guardrails for Amazon Bedrock (preview) to promote safe interactions between users and your generative AI applications by implementing safeguards customized to your use cases and responsible AI policies. AWS is committed to developing generative AI in a responsible, people-centric way by focusing on education and science and helping developers to integrate responsible AI across the AI lifecycle. With Guardrails for Amazon Bedrock, you can consistently implement safeguards to deliver relevant and safe user experiences aligned with your company policies and principles. Guardrails help you define denied topics and content filters to remove undesirable and harmful content from interactions between users and your applications. This provides an additional level of control on top of any protections built into foundation models (FMs). You can apply guardrails to all large language models (LLMs) in Amazon Bedrock, including fine-tuned models, and Agents for Amazon Bedrock. This drives consistency in how you deploy your preferences across applications so you can innovate safely while closely managing user experiences based on your requirements. By standardizing safety and privacy controls, Guardrails for Amazon Bedrock helps you build generative AI applications that align with your responsible AI goals. Let me give you a quick tour of the key controls available in Guardrails for Amazon Bedrock. Key controls Using Guardrails for Amazon Bedrock, you can define the following set of policies to create safeguards in your applications. Denied topics – You can define a set of topics that are undesirable in the context of your application using a short natural language description. For example, as a developer at a bank, you might want to set up an assistant for your online banking application to avoid providing investment advice. I specify a denied topic with the name “Investment advice” and provide a natural language description, such as “Investment advice refers to inquiries, guidance, or recommendations regarding the management or allocation of funds or assets with the goal of generating returns or achieving specific financial objectives.” Content filters – You can configure thresholds to filter harmful content across hate, insults, sexual, and violence categories. While many FMs already provide built-in protections to prevent the generation of undesirable and harmful responses, guardrails give you additional controls to filter such interactions to desired degrees based on your use cases and responsible AI policies. A higher filter strength corresponds to stricter filtering.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='8c4d9820-bc38-4c46-8772-1b57399ebca4', embedding=None, metadata={'title': 'Guardrails for Amazon Bedrock helps implement safeguards customized to your use cases and responsible AI policies (preview)', 'summary': 'Promote safe interactions between users and your generative AI applications by implementing safeguards customized to your use cases and responsible AI policies.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='PII redaction (in the works) – You will be able to select a set of personally identifiable information (PII) such as name, e-mail address, and phone number, that can be redacted in FM-generated responses or block a user input if it contains PII. Guardrails for Amazon Bedrock integrates with Amazon CloudWatch , so you can monitor and analyze user inputs and FM responses that violate policies defined in the guardrails. Join the preview Guardrails for Amazon Bedrock is available today in limited preview. Reach out through your usual AWS Support contacts if you’d like access to Guardrails for Amazon Bedrock. During preview, guardrails can be applied to all large language models (LLMs) available in Amazon Bedrock, including Amazon Titan Text, Anthropic Claude, Meta Llama 2, AI21 Jurassic, and Cohere Command. You can also use guardrails with custom models as well as Agents for Amazon Bedrock. To learn more, visit the Guardrails for Amazon Bedrock web page. — Antje', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='2d8e634f-ef92-45f6-a069-658b9da0172d', embedding=None, metadata={'title': 'Agents for Amazon Bedrock is now available with improved control of orchestration and visibility into reasoning', 'summary': 'Agents for Amazon Bedrock helps you accelerate generative artificial intelligence (AI) application development by orchestrating multistep tasks.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Back in July, we introduced Agents for Amazon Bedrock in preview. Today, Agents for Amazon Bedrock is generally available. Agents for Amazon Bedrock helps you accelerate generative artificial intelligence (AI) application development by orchestrating multistep tasks. Agents uses the reasoning capability of foundation models (FMs) to break down user-requested tasks into multiple steps. They use the developer-provided instruction to create an orchestration plan and then carry out the plan by invoking company APIs and accessing knowledge bases using Retrieval Augmented Generation (RAG) to provide a final response to the end user. If you’re curious how this works, check out my previous posts on agents that include a primer on advanced reasoning and a primer on RAG . Starting today, Agents for Amazon Bedrock also comes with enhanced capabilities that include improved control of the orchestration and better visibility into the chain of thought reasoning. Behind the scenes, Agents for Amazon Bedrock automates the prompt engineering and orchestration of user-requested tasks, such as managing retail orders or processing insurance claims. An agent automatically builds the orchestration prompt and, if connected to knowledge bases, augments it with your company-specific information and invokes APIs to provide responses to the user in natural language. As a developer, you can use the new trace capability to follow the reasoning that’s used as the plan is carried out. You can view the intermediate steps in the orchestration process and use this information to troubleshoot issues. You can also access and modify the prompt that the agent automatically creates so you can further enhance the end-user experience. You can update this automatically created prompt (or prompt template) to help the FM enhance the orchestration and responses, giving you more control over the orchestration. Let me show you how to view the reasoning steps and how to modify the prompt. View reasoning steps Traces gives you visibility into the agent’s reasoning, known as the chain of thought (CoT). You can use the CoT trace to see how the agent performs tasks step by step. The CoT prompt is based on a reasoning technique called ReAct (synergizing reasoning and acting ). Check out the primer on advanced reasoning in my previous blog post to learn more about ReAct and the specific prompt structure. To get started, navigate to the Amazon Bedrock console and select the working draft of an existing agent. Then, select the Test button and enter a sample user request.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='3a8c6cf9-83f4-43bf-a815-23afbcf0e5da', embedding=None, metadata={'title': 'Agents for Amazon Bedrock is now available with improved control of orchestration and visibility into reasoning', 'summary': 'Agents for Amazon Bedrock helps you accelerate generative artificial intelligence (AI) application development by orchestrating multistep tasks.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='In the agent’s response, select Show trace . The CoT trace shows the agent’s reasoning step-by-step. Open each step to see the CoT details. The enhanced visibility helps you understand the rationale used by the agent to complete the task. As a developer, you can use this information to refine the prompts, instructions, and action descriptions to adjust the agent’s actions and responses when iteratively testing and improving the user experience. Modify agent-created prompts The agent automatically creates a prompt template from the provided instructions. You can update the preprocessing of user inputs, the orchestration plan, and the postprocessing of the FM response. To get started, navigate to the Amazon Bedrock console\\xa0and select the working draft of an existing agent. Then, select the Edit button next to Advanced prompts . Here, you have access to four different types of templates. Preprocessing templates define how an agent contextualizes and categorizes user inputs. The orchestration template equips an agent with short-term memory, a list of available actions and knowledge bases along with their descriptions, as well as few-shot examples of how to break down the problem and use these actions and knowledge in different sequences or combinations. Knowledge base response generation templates deﬁne how knowledge bases will be used and summarized in the response. Postprocessing templates deﬁne how an agent will format and present a final response to the end user. You can either keep using the template defaults or edit and override the template defaults. Things to know Here are a few best practices and important things to know when you’re working with Agents for Amazon Bedrock. Agents perform best when you allow them to focus on a specific task. The clearer the objective (instructions) and the more focused the available set of actions (APIs), the easier it will be for the FM to reason and identify the right steps. If you need agents to cover various tasks, consider creating separate, individual agents. Here are a few additional guidelines: Number of APIs – Use three to five APIs with a couple of input parameters in your agents. API design – Follow general best practices for designing APIs, such as ensuring idempotency. API call validations – Follow best practices of API design by employing exhaustive validation for all API calls. This is particularly important because large language models (LLMs) may generate hallucinated inputs and outputs, and these validations prove helpful during such occurrences.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='3a288219-c2fa-4e40-a21d-54de6fcc0eab', embedding=None, metadata={'title': 'Agents for Amazon Bedrock is now available with improved control of orchestration and visibility into reasoning', 'summary': 'Agents for Amazon Bedrock helps you accelerate generative artificial intelligence (AI) application development by orchestrating multistep tasks.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Availability and pricing Agents for Amazon Bedrock are available today in AWS Regions US East (N. Virginia) and US West (Oregon). You will be charged for the inference calls ( InvokeModel API) made by agents. The InvokeAgent API is not charged separately. Amazon Bedrock Pricing has all the details. Learn more Agents for Amazon Bedrock product page Agents for Amazon Bedrock User Guide Agents for Bedrock in the console — Antje', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='32130e8e-560c-4e9c-998a-83204477404d', embedding=None, metadata={'title': 'Customize models in Amazon Bedrock with your own data using fine-tuning and continued pre-training', 'summary': 'Privately and securely customize foundation models with your own data in Amazon Bedrock to build applications that are specific to your domain, organization, and use case.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, I’m excited to share that you can now privately and securely customize foundation models (FMs) with your own data in Amazon Bedrock to build applications that are specific to your domain, organization, and use case. With custom models, you can create unique user experiences that reflect your company’s style, voice, and services. With fine-tuning , you can increase model accuracy by providing your own task-specific labeled training dataset and further specialize your FMs. With continued pre-training , you can train models using your own unlabeled data in a secure and managed environment with customer managed keys. Continued pre-training helps models become more domain-specific by accumulating more robust knowledge and adaptability—beyond their original training. Let me give you a quick tour of both model customization options. You can create fine-tuning and continued pre-training jobs using the Amazon Bedrock console or APIs. In the console, navigate to Amazon Bedrock , then select Custom models. Fine-tune Meta Llama 2, Cohere Command Light, and Amazon Titan FMs Amazon Bedrock now supports fine-tuning for Meta Llama 2 , Cohere Command Light , as well as Amazon Titan models . To create a fine-tuning job in the console, choose Customize model , then choose Create Fine-tuning job . Here’s a quick demo using the AWS SDK for Python (Boto3) . Let’s fine-tune Cohere Command Light to summarize dialogs. For demo purposes, I’m using the public dialogsum dataset, but this could be your own company-specific data. To prepare for fine-tuning on Amazon Bedrock, I converted the dataset into JSON Lines format and uploaded it to Amazon S3 . Each JSON line needs to have both a prompt and a completion field. You can specify up to 10,000 training data records, but you may already see model performance improvements with a few hundred examples. {\"completion\": \"Mr. Smith\\'s getting a check-up, and Doctor Haw...\", \"prompt\": Summarize the following conversation.\\\\n\\\\n#Pers...\"}\\n{\"completion\": \"Mrs Parker takes Ricky for his vaccines. Dr.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='bb40fadd-4ed0-4838-bd19-dd5eb63b6b4e', embedding=None, metadata={'title': 'Customize models in Amazon Bedrock with your own data using fine-tuning and continued pre-training', 'summary': 'Privately and securely customize foundation models with your own data in Amazon Bedrock to build applications that are specific to your domain, organization, and use case.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='P...\", \"prompt\": \"Summarize the following conversation.\\\\n\\\\n#Pers...\"}\\n{\"completion\": \"#Person1#\\'s looking for a set of keys and asks...\", \"prompt\": \"Summarize the following conversation.\\\\n\\\\n#Pers...\"} I redacted the prompt and completion fields for brevity. You can list available foundation models that support fine-tuning with the following command: import boto3 \\nbedrock = boto3.client(service_name=\"bedrock\")\\nbedrock_runtime = boto3.client(service_name=\"bedrock-runtime\")\\n\\nfor model in bedrock.list_foundation_models(\\n    byCustomizationType=\"FINE_TUNING\")[\"modelSummaries\"]:\\n    for key, value in model.items():\\n        print(key, \":\", value)\\n    print(\"-----\\\\n\") Next, I create a model customization job. I specify the Cohere Command Light model ID that supports fine-tuning, set customization type to FINE_TUNING , and point to the Amazon S3 location of the training data. If needed, you can also adjust the hyperparameters for fine-tuning. # Select the foundation model you want to customize\\nbase_model_id = \"cohere.command-light-text-v14:7:4k\"\\n\\nbedrock.create_model_customization_job(\\n    customizationType=\"FINE_TUNING\",\\n    jobName=job_name,\\n    customModelName=model_name,\\n    roleArn=role,\\n    baseModelIdentifier=base_model_id,\\n    hyperParameters = {\\n        \"epochCount\": \"1\",\\n        \"batchSize\": \"8\",\\n        \"learningRate\": \"0.00001\",\\n    },\\n    trainingDataConfig={\"s3Uri\": \"s3://path/to/train-summarization.jsonl\"},\\n    outputDataConfig={\"s3Uri\": \"s3://path/to/output\"},\\n)\\n\\n# Check for the job status\\nstatus = bedrock.get_model_customization_job(jobIdentifier=job_name)[\"status\"] Once the job is complete, you receive a unique model ID for your custom model. Your fine-tuned model is stored securely by Amazon Bedrock. To test and deploy your model, you need to purchase Provisioned Throughput . Let’s see the results. I select one example from the dataset and ask the base model before fine-tuning, as well as the custom model after fine-tuning, to summarize the following dialog: prompt = \"\"\"Summarize the following conversation.\\\\\\\\n\\\\\\\\n\\n#Person1#: Hello.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='7bc9329b-85b5-4a7e-acd8-95a2fdf340b4', embedding=None, metadata={'title': 'Customize models in Amazon Bedrock with your own data using fine-tuning and continued pre-training', 'summary': 'Privately and securely customize foundation models with your own data in Amazon Bedrock to build applications that are specific to your domain, organization, and use case.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='My name is John Sandals, and I\\'ve got a reservation.\\\\\\\\n\\n#Person2#: May I see some identification, sir, please?\\\\\\\\n\\n#Person1#: Sure. Here you are.\\\\\\\\n\\n#Person2#: Thank you so much. Have you got a credit card, Mr. Sandals?\\\\\\\\n\\n#Person1#: I sure do. How about American Express?\\\\\\\\n\\n#Person2#: Unfortunately, at the present time we take only MasterCard or VISA.\\\\\\\\n\\n#Person1#: No American Express? Okay, here\\'s my VISA.\\\\\\\\n\\n#Person2#: Thank you, sir. You\\'ll be in room 507, nonsmoking, with a queen-size bed. Do you approve, sir?\\\\\\\\n\\n#Person1#: Yeah, that\\'ll be fine.\\\\\\\\n\\n#Person2#: That\\'s great. This is your key, sir. If you need anything at all, anytime, just dial zero.\\\\\\\\n\\\\\\\\n\\nSummary: \"\"\" Use the Amazon Bedrock InvokeModel API to query the models. body = {\\n    \"prompt\": prompt,\\n    \"temperature\": 0.5,\\n    \"p\": 0.9,\\n    \"max_tokens\": 512,\\n}\\n\\nresponse = bedrock_runtime.invoke_model(\\n\\t# Use on-demand inference model ID for response before fine-tuning\\n    # modelId=\"cohere.command-light-text-v14\",\\n\\t# Use ARN of your deployed custom model for response after fine-tuning\\n\\tmodelId=provisioned_custom_model_arn,\\n    modelId=base_model_id, \\n    body=json.dumps(body)\\n) Here’s the base model response before fine-tuning: #Person2# helps John Sandals with his reservation. John gives his credit card information and #Person2# confirms that they take only MasterCard and VISA. John will be in room 507 and #Person2# will be his host if he needs anything. Here’s the response after fine-tuning, shorter and more to the point: John Sandals has a reservation and checks in at a hotel. #Person2# takes his credit card and gives him a key. Continued pre-training for Amazon Titan Text (preview) Continued pre-training on Amazon Bedrock is available today in public preview for Amazon Titan Text models, including Titan Text Express and Titan Text Lite.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='856b6846-229b-4a5c-aa4c-6f5789ff7e01', embedding=None, metadata={'title': 'Customize models in Amazon Bedrock with your own data using fine-tuning and continued pre-training', 'summary': 'Privately and securely customize foundation models with your own data in Amazon Bedrock to build applications that are specific to your domain, organization, and use case.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='To create a continued pre-training job in the console, choose Customize model , then choose Create Continued Pre-training job . Here’s a quick demo again using boto3. Let’s assume you work at an investment company and want to continue pre-training the model with financial and analyst reports to make it more knowledgeable about financial industry terminology. For demo purposes, I selected a collection of Amazon shareholder letters as my training data. To prepare for continued pre-training, I converted the dataset into JSON Lines format again and uploaded it to Amazon S3. Because I’m working with unlabeled data, each JSON line only needs to have the prompt field. You can specify up to 100,000 training data records and usually see positive effects after providing at least 1 billion tokens. {\"input\": \"Dear shareholders: As I sit down to...\"}\\n{\"input\": \"Over the last several months, we to...\"}\\n{\"input\": \"work came from optimizing the conne...\"}\\n{\"input\": \"of the Amazon shopping experience f...\"} I redacted the input fields for brevity. Then, create a model customization job with customization type CONTINUED_PRE_TRAINING that points to the data. If needed, you can also adjust the hyperparameters for continued pre-training. # Select the foundation model you want to customize\\nbase_model_id = \"amazon.titan-text-express-v1\"\\n\\nbedrock.create_model_customization_job(\\n    customizationType=\"CONTINUED_PRE_TRAINING\",\\n    jobName=job_name,\\n    customModelName=model_name,\\n    roleArn=role,\\n    baseModelIdentifier=base_model_id,\\n    hyperParameters = {\\n        \"epochCount\": \"10\",\\n        \"batchSize\": \"8\",\\n        \"learningRate\": \"0.00001\",\\n    },\\n    trainingDataConfig={\"s3Uri\": \"s3://path/to/train-continued-pretraining.jsonl\"},\\n    outputDataConfig={\"s3Uri\": \"s3://path/to/output\"},\\n) Once the job is complete, you receive another unique model ID. Your customized model is securely stored again by Amazon Bedrock. As with fine-tuning, you need to purchase Provisioned Throughput to test and deploy your model. Things to know Here are a couple of important things to know: Data privacy and network security – With Amazon Bedrock, you are in control of your data, and all your inputs and customizations remain private to your AWS account.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='7ba2bff5-3aac-45f5-9ac9-da140a944eaf', embedding=None, metadata={'title': 'Customize models in Amazon Bedrock with your own data using fine-tuning and continued pre-training', 'summary': 'Privately and securely customize foundation models with your own data in Amazon Bedrock to build applications that are specific to your domain, organization, and use case.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Your data, such as prompts, completions, custom models, and data used for fine-tuning or continued pre-training, is not used for service improvement and is never shared with third-party model providers. Your data remains in the AWS Region where the API call is processed. All data is encrypted in transit and at rest. You can use AWS PrivateLink to create a private connection between your VPC and Amazon Bedrock. Billing – Amazon Bedrock charges for model customization, storage, and inference. Model customization is charged per tokens processed. This is the number of tokens in the training dataset multiplied by the number of training epochs. An epoch is one full pass through the training data during customization. Model storage is charged per month, per model. Inference is charged hourly per model unit using provisioned throughput. For detailed pricing information, see Amazon Bedrock Pricing . Custom models and provisioned throughput – Amazon Bedrock allows you to run inference on custom models by purchasing provisioned throughput. This guarantees a consistent level of throughput in exchange for a term commitment. You specify the number of model units needed to meet your application’s performance needs. For evaluating custom models initially, you can purchase provisioned throughput hourly with no long-term commitment. With no commitment, a quota of one model unit is available per provisioned throughput. You can create up to two provisioned throughputs per account. Availability Fine-tuning support on Meta Llama 2, Cohere Command Light, and Amazon Titan Text FMs is available today in AWS Regions US East (N. Virginia) and US West (Oregon). Continued pre-training is available today in public preview in AWS Regions US East (N. Virginia) and US West (Oregon). To learn more, visit the Amazon Bedrock Developer Experience web page and check out the User Guide. Customize FMs with Amazon Bedrock today! — Antje', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='fc7030c3-f10f-4729-a936-b37c4f853017', embedding=None, metadata={'title': 'Knowledge Bases now delivers fully managed RAG experience in Amazon Bedrock', 'summary': 'With a knowledge base, you can securely connect foundation models (FMs) in Amazon Bedrock to your company data for Retrieval Augmented Generation (RAG).'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Back in September, we introduced Knowledge Bases for Amazon Bedrock in preview. Starting today, Knowledge Bases for Amazon Bedrock is generally available. With a knowledge base, you can securely connect foundation models (FMs) in Amazon Bedrock to your company data for Retrieval Augmented Generation (RAG). Access to additional data helps the model generate more relevant, context-specific, and accurate responses without continuously retraining the FM. All information retrieved from knowledge bases comes with source attribution to improve transparency and minimize hallucinations. If you’re curious how this works, check out my previous post that includes a primer on RAG. With today’s launch, Knowledge Bases gives you a fully managed RAG experience and the easiest way to get started with RAG in Amazon Bedrock. Knowledge Bases now manages the initial vector store setup, handles the embedding and querying, and provides source attribution and short-term memory needed for production RAG applications. If needed, you can also customize the RAG workflows to meet specific use case requirements or integrate RAG with other generative artificial intelligence (AI) tools and applications. Fully managed RAG experience Knowledge Bases for Amazon Bedrock manages the end-to-end RAG workflow for you. You specify the location of your data, select an embedding model to convert the data into vector embeddings, and have Amazon Bedrock create a vector store in your account to store the vector data. When you select this option (available only in the console), Amazon Bedrock creates a vector index in Amazon OpenSearch Serverless in your account, removing the need to manage anything yourself. Vector embeddings include the numeric representations of text data within your documents. Each embedding aims to capture the semantic or contextual meaning of the data. Amazon Bedrock takes care of creating, storing, managing, and updating your embeddings in the vector store, and it ensures your data is always in sync with your vector store. Amazon Bedrock now also supports two new APIs for RAG that handle the embedding and querying and provide the source attribution and short-term memory needed for production RAG applications. With the new RetrieveAndGenerate API, you can directly retrieve relevant information from your knowledge bases and have Amazon Bedrock generate a response from the results by specifying a FM in your API call. Let me show you how this works.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='43a967d4-0ad8-4808-b9f4-83abfcba6b85', embedding=None, metadata={'title': 'Knowledge Bases now delivers fully managed RAG experience in Amazon Bedrock', 'summary': 'With a knowledge base, you can securely connect foundation models (FMs) in Amazon Bedrock to your company data for Retrieval Augmented Generation (RAG).'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Use the RetrieveAndGenerate API To give it a try, navigate to the Amazon Bedrock console , create and select a knowledge base, then select Test knowledge base . For this demo, I created a knowledge base that has access to a PDF of Generative AI on AWS . I choose Select Model to specify a FM. Then, I ask, “What is Amazon Bedrock?” Behind the scenes, Amazon Bedrock converts the queries into embeddings, queries the knowledge base, and then augments the FM prompt with the search results as context information and returns the FM-generated response to my question. For multi-turn conversations, Knowledge Bases manages the short-term memory of the conversation to provide more contextual results. Here’s a quick demo of how to use the APIs with the AWS SDK for Python (Boto3) . def retrieveAndGenerate(input, kbId):\\n    return bedrock_agent_runtime.retrieve_and_generate(\\n        input={\\n            \\'text\\': input\\n        },\\n        retrieveAndGenerateConfiguration={\\n            \\'type\\': \\'KNOWLEDGE_BASE\\',\\n            \\'knowledgeBaseConfiguration\\': {\\n                \\'knowledgeBaseId\\': kbId,\\n                \\'modelArn\\': \\'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-instant-v1\\'\\n                }\\n            }\\n        )\\n\\nresponse = retrieveAndGenerate(\"What is Amazon Bedrock?\", \"AES9P3MT9T\")[\"output\"][\"text\"] The output of the RetrieveAndGenerate API includes the generated response, the source attribution, and the retrieved text chunks.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='3d84c16a-0571-481e-bec2-2d4df1dbfec6', embedding=None, metadata={'title': 'Knowledge Bases now delivers fully managed RAG experience in Amazon Bedrock', 'summary': 'With a knowledge base, you can securely connect foundation models (FMs) in Amazon Bedrock to your company data for Retrieval Augmented Generation (RAG).'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"In my demo, the API response looks like this (with some of the output redacted for brevity): { ... \\n    'output': {'text': 'Amazon Bedrock is a managed service from AWS that ...'}, \\n    'citations': \\n        [{'generatedResponsePart': \\n             {'textResponsePart': \\n                 {'text': 'Amazon Bedrock is ...', 'span': {'start': 0, 'end': 241}}\\n             }, \\n\\t      'retrievedReferences': \\n\\t\\t\\t[{'content':\\n                 {'text': 'All AWS-managed service API activity...'}, \\n\\t\\t\\t\\t 'location': {'type': 'S3', 's3Location': {'uri': 's3://data-generative-ai-on-aws/gaia.pdf'}}}, \\n\\t\\t     {'content': \\n\\t\\t\\t      {'text': 'Changing a portion of the image using ...'}, \\n\\t\\t\\t\\t  'location': {'type': 'S3', 's3Location': {'uri': 's3://data-generative-ai-on-aws/gaia.pdf'}}}, ...]\\n        ...}]\\n} The generated response looks like this: Amazon Bedrock is a managed service that offers a serverless experience for generative AI through a simple API. It provides access to foundation models from Amazon and third parties for tasks like text generation, image generation, and building conversational agents. Data processed through Amazon Bedrock remains private and encrypted. Customize RAG workflows If you want to process the retrieved text chunks further, see the relevance scores of the retrievals, or develop your own orchestration for text generation, you can use the new Retrieve API. This API converts user queries into embeddings, searches the knowledge base, and returns the relevant results, giving you more control to build custom workflows on top of the semantic search results. Use the Retrieve API In the Amazon Bedrock console, I toggle the switch to disable Generate responses . Then, I ask again, “What is Amazon Bedrock?” This time, the output shows me the retrieval results with links to the source documents where the text chunks came from. Here’s how to use the Retrieve API with boto3.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='524f25dd-7146-4640-bc2c-92f0db9b8045', embedding=None, metadata={'title': 'Knowledge Bases now delivers fully managed RAG experience in Amazon Bedrock', 'summary': 'With a knowledge base, you can securely connect foundation models (FMs) in Amazon Bedrock to your company data for Retrieval Augmented Generation (RAG).'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='import boto3\\n\\nbedrock_agent_runtime = boto3.client(\\n    service_name = \"bedrock-agent-runtime\"\\n)\\n\\ndef retrieve(query, kbId, numberOfResults=5):\\n    return bedrock_agent_runtime.retrieve(\\n        retrievalQuery= {\\n            \\'text\\': query\\n        },\\n        knowledgeBaseId=kbId,\\n        retrievalConfiguration= {\\n            \\'vectorSearchConfiguration\\': {\\n                \\'numberOfResults\\': numberOfResults\\n            }\\n        }\\n    )\\n\\nresponse = retrieve(\"What is Amazon Bedrock?\", \"AES9P3MT9T\")[\"retrievalResults\"] The output of the Retrieve API includes the retrieved text chunks, the location type and URI of the source data, and the scores of the retrievals. The score helps to determine chunks that match more closely with the query. In my demo, the API response looks like this (with some of the output redacted for brevity): [{\\'content\\': {\\'text\\': \\'Changing a portion of the image using ...\\'},\\n  \\'location\\': {\\'type\\': \\'S3\\',\\n   \\'s3Location\\': {\\'uri\\': \\'s3://data-generative-ai-on-aws/gaia.pdf\\'}},\\n  \\'score\\': 0.7329834},\\n {\\'content\\': {\\'text\\': \\'back to the user in natural language. For ...\\'},\\n  \\'location\\': {\\'type\\': \\'S3\\',\\n   \\'s3Location\\': {\\'uri\\': \\'s3://data-generative-ai-on-aws/gaia.pdf\\'}},\\n  \\'score\\': 0.7331088},\\n...] To further customize your RAG workflows, you can define a custom chunking strategy and select a custom vector store. Custom chunking strategy – To enable effective retrieval from your data, a common practice is to first split the documents into manageable chunks. This enhances the model’s capacity to comprehend and process information more effectively, leading to improved relevant retrievals and generation of coherent responses. Knowledge Bases for Amazon Bedrock manages the chunking of your documents. When you configure the data source for your knowledge base, you can now define a chunking strategy. Default chunking splits data into chunks of up to 200 tokens and is optimized for question-answer tasks. Use default chunking when you are not sure of the optimal chunk size for your data. You also have the option to specify a custom chunk size and overlap with fixed-size chunking.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='7f362af5-9978-4be6-b767-7ad2f60b7073', embedding=None, metadata={'title': 'Knowledge Bases now delivers fully managed RAG experience in Amazon Bedrock', 'summary': 'With a knowledge base, you can securely connect foundation models (FMs) in Amazon Bedrock to your company data for Retrieval Augmented Generation (RAG).'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Use fixed-size chunking if you know the optimal chunk size and overlap for your data (based on file attributes, accuracy testing, and so on). An overlap between chunks in the recommended range of 0–20 percent can help improve accuracy. Higher overlap can lead to decreased relevancy scores. If you select to create one embedding per document, Knowledge Bases keeps each file as a single chunk. Use this option if you don’t want Amazon Bedrock to chunk your data, for example, if you want to chunk your data offline using an algorithm that is specific to your use case. Common use cases include code documentation. Custom vector store – You can also select a custom vector store. The available vector database options include vector engine for Amazon OpenSearch Serverless , Pinecone , and Redis Enterprise Cloud , with support for Amazon Aurora and MongoDB coming soon. To use a custom vector store, you must create a new, empty vector database from the list of supported options and provide the vector database index name as well as index field and metadata field mappings. This vector database will need to be for exclusive use with Amazon Bedrock. Integrate RAG with other generative AI tools and applications If you want to build an AI assistant that can perform multistep tasks and access company data sources to generate more relevant and context-aware responses, you can integrate Knowledge Bases with Agents for Amazon Bedrock . You can also use the Knowledge Bases retrieval plugin for LangChain to integrate RAG workflows into your generative AI applications. Availability Knowledge bases for Amazon Bedrock is available today in AWS Regions US East (N. Virginia) and US West (Oregon). Learn more Knowledge Bases for Amazon Bedrock Knowledge Bases User Guide Amazon Bedrock in the console — Antje', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='9fe4df4f-3ec1-4a69-bc10-acb8a206f164', embedding=None, metadata={'title': 'Amazon Transcribe Call Analytics adds new generative AI-powered call summaries (preview)', 'summary': 'Powered by Amazon Bedrock, this feature helps businesses improve customer experience, and agent and supervisor productivity by automatically summarizing customer service calls.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='We are announcing generative artificial intelligence (AI)-powered call summarization in Amazon Transcribe Call Analytics in preview. Powered by Amazon Bedrock , this feature helps businesses improve customer experience, and agent and supervisor productivity by automatically summarizing customer service calls. Amazon Transcribe Call Analytics provides machine learning (ML)-powered analytics that allows contact centers to understand the sentiment, trends, and policy compliance of customer conversations to improve their experience and identify crucial feedback. A single API call is all it takes to extract transcripts, rich insights, and summaries from your customer conversations. We understand that as a business, you want to maintain an accurate historical record of key conversation points, including action items associated with each conversation. To do this, agents summarize notes after the conversation has ended and enter these in their CRM system, a process that is time-consuming and subject to human error. Now imagine the customer trust erosion that follows when the agent fails to correctly capture and act upon important action items discussed during conversations. How it works Starting today, to assist agents and supervisors with the summarization of customer conversations, Amazon Transcribe Call Analytics will generate a concise summary of a contact center interaction that captures key components such as why the customer called, how the issue was addressed, and what follow-up actions were identified. After completing a customer interaction, agents can directly proceed to help the next customer since they don’t have to summarize a conversation, resulting in reduced customer wait times and improved agent productivity. Further, supervisors can review the summary when investigating a customer issue to get a gist of the conversation, without having to listen to the entire call recording or read the transcript. Exploring Amazon Transcribe Call Analytics in the console To see how this works visually, I first create an Amazon Simple Storage Service (Amazon S3) bucket in the relevant AWS Region . I then upload the audio file to the S3 bucket. To create an analytics job that transcribes the audio and provides additional analytics about the conversation that the customer and the agent were having, I go to the Amazon Transcribe Call Analytics console. I select Post-call Analytics in the left hand navigation bar and then choose Create job. Next I enter a job name making sure to keep the language settings based on the language in the audio file. In the Amazon S3 URI path, I provide the link to the audio file uploaded in the first screenshot shown in this post.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='52d0468c-860d-4a70-893f-02a30583ec9a', embedding=None, metadata={'title': 'Amazon Transcribe Call Analytics adds new generative AI-powered call summaries (preview)', 'summary': 'Powered by Amazon Bedrock, this feature helps businesses improve customer experience, and agent and supervisor productivity by automatically summarizing customer service calls.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='In Role name , I select Create an IAM role which will have access to the Amazon S3 bucket, then choose Next . I enable Generative call summarization, and then choose Create job . After a few minutes, the job’s status will change from In progress to Complete , indicating that it was completed successfully. Select the job, and the next screen will show the transcript and a new tab, Generative call summarization – preview . You can also download the transcript to view the analytics and summary. Now available Generative call summarization in Amazon Transcribe Call Analytics is available today in English in US East (N. Virginia) and US West (Oregon). With generative call summarization in Amazon Transcribe Call Analytics, you pay as you go and are billed monthly based on tiered pricing. For more information, see Amazon Transcribe pricing . Learn more : Amazon Transcribe Call Analytics product page Amazon Transcribe Call Analytics developer guide – Veliswa', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='6494afb3-3897-4cf5-8b3d-5e99e7163aff', embedding=None, metadata={'title': 'Build generative AI apps using AWS Step Functions and Amazon Bedrock', 'summary': 'Step Functions provides two new optimized API actions for Amazon Bedrock: InvokeModel and CreateModelCustomizationJob.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today we are announcing two new optimized integrations for AWS Step Functions with Amazon Bedrock . Step Functions is a visual workflow service that helps developers build distributed applications, automate processes, orchestrate microservices, and create data and machine learning (ML) pipelines. In September, we made available Amazon Bedrock, the easiest way to build and scale generative artificial intelligence (AI) applications with foundation models (FMs). Bedrock offers a choice of foundation models from leading providers like AI21 Labs, Anthropic, Cohere, Stability AI, and Amazon, along with a broad set of capabilities that customers need to build generative AI applications, while maintaining privacy and security. You can use Amazon Bedrock from the AWS Management Console , AWS Command Line Interface (AWS CLI) , or AWS SDKs . The new Step Functions optimized integrations with Amazon Bedrock allow you to orchestrate tasks to build generative AI applications using Amazon Bedrock, as well as to integrate with over 220 AWS services . With Step Functions, you can visually develop, inspect, and audit your workflows. Previously, you needed to invoke an AWS Lambda function to use Amazon Bedrock from your workflows, adding more code to maintain them and increasing the costs of your applications. Step Functions provides two new optimized API actions for Amazon Bedrock: InvokeModel – This integration allows you to invoke a model and run the inferences with the input provided in the parameters. Use this API action to run inferences for text, image, and embedding models. CreateModelCustomizationJob – This integration creates a fine-tuning job to customize a base model. In the parameters, you specify the foundation model and the location of the training data. When the job is completed, your custom model is ready to be used. This is an asynchronous API, and this integration allows Step Functions to run a job and wait for it to complete before proceeding to the next state. This means that the state machine execution will pause while the create model customization job is running and will resume automatically when the task is complete. The InvokeModel API action accepts requests and responses that are up to 25 MB. However, Step Functions has a 256 kB limit on state payload input and output. In order to support larger payloads with this integration, you can define an Amazon Simple Storage Service (Amazon S3) bucket where the InvokeModel API reads data from and writes the result to.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='43071db6-ba5f-4353-ac85-35e080625cb3', embedding=None, metadata={'title': 'Build generative AI apps using AWS Step Functions and Amazon Bedrock', 'summary': 'Step Functions provides two new optimized API actions for Amazon Bedrock: InvokeModel and CreateModelCustomizationJob.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='These configurations can be provided in the parameters section of the API action configuration parameters section. How to get started with Amazon Bedrock and AWS Step Functions Before getting started, ensure that you create the state machine in a Region where Amazon Bedrock is available. For this example, use US East (N. Virginia), us-east-1 . From the AWS Management Console, create a new state machine. Search for “bedrock,” and the two available API actions will appear. Drag the InvokeModel to the state machine. You can now configure that state in the menu on the right. First, you can define which foundation model you want to use. Pick a model from the list, or get the model dynamically from the input. Then you need to configure the model parameters. You can enter the inference parameters in the text box or load the parameters from Amazon S3. If you keep scrolling in the API action configuration, you can specify additional configuration options for the API, such as the S3 destination bucket. When this field is specified, the API action stores the API response in the specified bucket instead of returning it to the state output. Here, you can also specify the content type for the requests and responses. When you finish configuring your state machine, you can create and run it. When the state machine runs, you can visualize the execution details, select the Amazon Bedrock state, and check its inputs and outputs. Using Step Functions, you can build state machines as extensively as you need, combining different services to solve many problems. For example, you can use Step Functions with Amazon Bedrock to create applications using prompt chaining. This is a technique for building complex generative AI applications by passing multiple smaller and simpler prompts to the FM instead of a very long and detailed prompt. To build a prompt chain, you can create a state machine that calls Amazon Bedrock multiple times to get an inference for each of the smaller prompts. You can use the parallel state to run all these tasks in parallel and then use an AWS Lambda function that unifies the responses of the parallel tasks into one response and generates a result. Available now AWS Step Functions optimized integrations for Amazon Bedrock are limited to the AWS Regions where Amazon Bedrock is available. You can get started with Step Functions and Amazon Bedrock by trying out a sample project from the Step Functions console . – Marcia', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='73c84a5e-f388-440a-b6fc-977ceae831f0', embedding=None, metadata={'title': 'Amazon CodeWhisperer offers new AI-powered code remediation, IaC support, and integration with Visual Studio', 'summary': 'These new enhancements deliver more automation, security, efficiency, and accelerated code delivery for customers, and provides this support in more places where developers work.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, we’re announcing the general availability of artificial intelligence (AI)-powered code remediation and infrastructure as code (IaC) support for Amazon CodeWhisperer , an AI-powered productivity tool for the IDE and command line. Amazon CodeWhisperer is also now available in Visual Studio, in preview. These new enhancements to Amazon CodeWhisperer help to enable faster and more efficient software development by offloading undifferentiated work and delivering more automation, security, efficiency, and accelerated code delivery for customers, and provides this support in more places where developers love to work. AI-powered code remediation – Since its launch, Amazon CodeWhisperer has identified hard-to-find security vulnerabilities with built-in security scans. It now provides generative AI-powered code suggestions to help remediate identified security and code quality issues. Built-in security scanning is designed to detect issues such as exposed credentials and log injection. Generative AI-powered code suggestions are designed to remediate the identified vulnerabilities, and are tailored to your application code so that you can quickly accept fixes with confidence. When a security scan is completed in CodeWhisperer, you are presented with code suggestions that you can simply accept to close the identified vulnerabilities quickly. Generative AI-powered code suggestions speed up the process of addressing security issues, so you can focus on higher-value work instead of manually reviewing code line by line to find the correct solution. You do not need to perform any additional setup in Amazon CodeWhisperer to start using this capability. Security scanning is available for Java, Python, JavaScript, and now available for TypeScript, C#, AWS CloudFormation (YAML, JSON), AWS CDK (TypeScript, Python), and HashiCorp Terraform (HCL). Code suggestions to remediate vulnerabilities are currently available for code written in Java, Python, and JavaScript. Infrastructure as code (IaC) – Amazon CodeWhisperer announces support for IaC, now encompassing AWS CloudFormation (YAML, JSON), AWS CDK (Typescript, Python), and HashiCorp Terraform (HCL). This update enhances the efficiency of IaC script development, allowing developers and DevOps teams to write infrastructure code seamlessly. With support for multiple IaC languages, CodeWhisperer promotes collaboration and consistency across diverse teams. This marks a significant advancement in cloud infrastructure development, offering a more streamlined and productive coding experience for users.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='8a441444-a01b-4a34-ba65-10822ca8865c', embedding=None, metadata={'title': 'Amazon CodeWhisperer offers new AI-powered code remediation, IaC support, and integration with Visual Studio', 'summary': 'These new enhancements deliver more automation, security, efficiency, and accelerated code delivery for customers, and provides this support in more places where developers work.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Visual Studio – Amazon CodeWhisperer is now available in Visual Studio 2022 (preview). Developers can build applications faster with real-time code suggestions for C#. Get started with the Individual Tier for free by installing the AWS Toolkit extension and signing in with an AWS Builder ID . CodeWhisperer also helps developers code responsibly by flagging code suggestions that may resemble publicly available code. CodeWhisperer will provide the repository URL and license when code similar to public code. Finally, Amazon CodeWhisperer recently previewed (11/20) a new time-saving capability for the command line interface. Now, Amazon CodeWhisperer adds typeahead code completions and inline documentation for hundreds of popular CLIs like Git, npm, AWS CLI, and Docker. It also adds the ability for you to translate natural language to shell code. For more details, read Introducing Amazon CodeWhisperer for command line . Learn more Amazon CodeWhisperer Go build! — Irshad', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='2c7cba48-4ba6-4768-ab90-a293b5b516e1', embedding=None, metadata={'title': 'Amazon CloudWatch Application Signals for automatic instrumentation of your applications (preview)', 'summary': 'You get a pre-built, standardized dashboard showing the most important metrics, such as volume of requests, availability, latency, and more, for the performance of your applications.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='One of the challenges with distributed systems is that they are made up of many interdependent services, which add a degree of complexity when you are trying to monitor their performance. Determining which services and APIs are experiencing high latencies or degraded availability requires manually putting together telemetry signals. This can result in time and effort establishing the root cause of any issues with the system due to the inconsistent experiences across metrics, traces, logs, real user monitoring, and synthetic monitoring. You want to provide your customers with continuously available and high-performing applications. At the same time, the monitoring that assures this must be efficient, cost-effective, and without undiﬀerentiated heavy lifting. Amazon CloudWatch Application Signals helps you automatically instrument applications based on best practices for application performance. There is no manual effort, no custom code, and no custom dashboards. You get a pre-built, standardized dashboard showing the most important metrics, such as volume of requests, availability, latency, and more, for the performance of your applications. In addition, you can define Service Level Objectives (SLOs) on your applications to monitor specific operations that matter most to your business. An example of an SLO could be to set a goal that a webpage should render within 2000 ms 99.9 percent of the time in a rolling 28-day interval. Application Signals automatically correlates telemetry across metrics, traces, logs, real user monitoring, and synthetic monitoring to speed up troubleshooting and reduce application disruption. By providing an integrated experience for analyzing performance in the context of your applications, Application Signals gives you improved productivity with a focus on the applications that support your most critical business functions. My personal favorite is the collaboration between teams that’s made possible by Application Signals. I started this post by mentioning that distributed systems are made up of many interdependent services. On the Service Map, which we will look at later in this post, if you, as a service owner, identify an issue that’s caused by another service, you can send a link to the owner of the other service to efficiently collaborate on the triage tasks. Getting started with Application Signals You can easily collect application and container telemetry when creating new Amazon EKS clusters in the Amazon EKS console by enabling the new Amazon CloudWatch Observability EKS add-on. Another option is to enable for existing Amazon EKS Clusters or other compute types directly in the Amazon CloudWatch console.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='c9afe0e4-8a68-4a79-ba87-da8d784f1660', embedding=None, metadata={'title': 'Amazon CloudWatch Application Signals for automatic instrumentation of your applications (preview)', 'summary': 'You get a pre-built, standardized dashboard showing the most important metrics, such as volume of requests, availability, latency, and more, for the performance of your applications.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='After enabling Application Signals via the Amazon EKS add-on or Custom option for other compute types , Application Signals automatically discovers services and generates a standard set of application metrics such as volume of requests and latency spikes or availability drops for APIs and dependencies, to name a few. All of the services discovered and their golden metrics (volume of requests, latency, faults and errors) are then automatically displayed on the Services page and the Service Map . The Service Map gives you a visual deep dive to evaluate the health of a service, its operations, dependencies, and all the call paths between an operation and a dependency. The list of services that are enabled in Application Signals will also show in the services dashboard, along with operational metrics across all of your services and dependencies to easily spot anomalies. The Application column is auto-populated if the EKS cluster belongs to an application that’s tagged in AppRegistry . The Hosted In column automatically detects which EKS pod, cluster, or namespace combination the service requests are running in, and you can select one to go directly to Container Insights for detailed container metrics such as CPU or memory utilization, to name a few. Team collaboration with Application Signals Now, to expand on the team collaboration that I mentioned at the beginning of this post. Let’s say you consult the services dashboard to do sanity checks and you notice two SLO issues for one of your services named pet-clinic-frontend . Your company maintains a set of SLOs, and this is the view that you use to understand how the applications are performing against the objectives. For the services that are tagged in AppRegistry all teams have a central view of the definition and ownership of the application. Further navigation to the service map gives you even more details on the health of this service. At this point you make the decision to send the link to the pet-clinic-frontend service to Sarah whose details you found in the AppRegistry. Sarah is the person on-call for this service. The link allows you to efficiently collaborate with Sarah because it’s been curated to land directly on the triage view that is contextualized based on your discovery of the issue. Sarah notices that the POST /api/customer/owners latency has increased to 2k ms for a number of requests and as the service owner, dives deep to arrive at the root cause.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='eec46e2b-d189-45ba-87b8-98c92c4ad056', embedding=None, metadata={'title': 'Amazon CloudWatch Application Signals for automatic instrumentation of your applications (preview)', 'summary': 'You get a pre-built, standardized dashboard showing the most important metrics, such as volume of requests, availability, latency, and more, for the performance of your applications.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Clicking into the latency graph returns a correlated list of traces that correspond directly to the operation, metric, and moment in time, which helps Sarah to find the exact traces that may have led to the increase in latency. Sarah uses Amazon CloudWatch Synthetics and Amazon CloudWatch RUM and has enabled the X-Ray active tracing integration to automatically see the list of relevant canaries and pages correlated to the service. This integrated view now helps Sarah gain multiple perspectives in the performance of the application and quickly troubleshoot anomalies in a single view. Available now Amazon CloudWatch Application Signals is available in preview and you can start using it today in the following AWS Regions : US East (N. Virginia), US East (Ohio), US West (Oregon), Europe (Ireland), Asia Pacific (Sydney), and Asia Pacific (Tokyo). To learn more, visit the Amazon CloudWatch user guide and the One Observability Workshop . You can submit your questions to AWS re:Post for Amazon CloudWatch , or through your usual AWS Support contacts. – Veliswa', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='b676e31d-9616-4b97-80b7-047f194cb310', embedding=None, metadata={'title': 'New myApplications in the AWS Management Console simplifies managing your application resources', 'summary': 'Now, you can more easily manage and monitor the cost, health, security posture, and performance of your applications on AWS from a widget in the AWS Management Console.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, we are announcing the general availability of myApplications supporting application operations, a new set of capabilities that help you get started with your applications on AWS, operate them with less effort, and move faster at scale. With myApplications in the AWS Management Console , you can more easily manage and monitor the cost, health, security posture, and performance of your applications on AWS. The myApplications experience is available in the Console Home , where you can access an Applications widget that lists the applications in an account. Now, you can create your applications more easily using the Create application wizard, connecting resources in your AWS account from one view in the console. The created application will automatically display in myApplications, and you can take action on your applications. When you choose your application in the Applications widget in the console, you can see an at-a-glance view of key application metrics widgets in the applications dashboard. Here you can find, debug operational issues, and optimize your applications. With a single action on the applications dashboard, you can dive deeper to act on specific resources in the relevant services, such as Amazon CloudWatch for application performance, AWS Cost Explorer for cost and usage, and AWS Security Hub for security findings. Getting started with myApplications To get started, on the AWS Management Console Home , choose Create application in the Applications widget. In the first step, input your application name and description. In the next step, you can add your resources. Before you can search and add resources, you should turn on and set up AWS Resource Explorer , a managed capability that simplifies the search and discovery of your AWS resources across AWS Regions. Choose Add resources and select the resources to add to your applications. You can also search by keyword, tag, or AWS CloudFormation stack to integrate groups of resources to manage the full lifecycle of your application. After confirming, your resources are added, new awsApplication tags applied, and the myApplications dashboard will be automatically generated. Now, let’s see which widgets can be useful. The Application summary widget displays the name, description, and tag so you know which application you are working on. The Cost and usage widget visualizes your AWS resource costs and usage from AWS Cost Explorer , including the application’s current and forecasted month-end costs, top five billed services, and a monthly application resource cost trend chart. You can monitor spend, look for anomalies, and click to take action where needed.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='038df053-5057-4baa-9eb3-933f73d15cf2', embedding=None, metadata={'title': 'New myApplications in the AWS Management Console simplifies managing your application resources', 'summary': 'Now, you can more easily manage and monitor the cost, health, security posture, and performance of your applications on AWS from a widget in the AWS Management Console.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='The Compute widget summarizes of application compute resources, information about which are in alarm, and trend charts from CloudWatch showing basic metrics such as Amazon EC2 instance CPU utilization and AWS Lambda invocations. You also can assess application operations, look for anomalies, and take action. The Monitoring and Operations widget displays alarms and alerts for resources associated with your application, service level objectives (SLOs), and standardized application performance metrics from CloudWatch Application Signals . You can monitor ongoing issues, assess trends, and quickly identify and drill down on any issues that might impact your application. The Security widget shows the highest priority security findings identified by AWS Security Hub . Findings are listed by severity and service, so you can monitor their security posture and click to take action where needed. The DevOps widget summarizes operational insights from AWS System Manager Application Manager , such as fleet management, state management, patch management, and configuration management status so you can assess compliance and take action. You can also use the Tagging widget to assist you in reviewing and applying tags to your application. Now available You can enjoy this new myApplications capability, a new application-centric experience to easily manage and monitor applications on AWS. myApplications capability is available in the following AWS Regions: US East (Ohio, N. Virginia), US West (N. California, Oregon), South America (São Paulo), Asia Pacific (Hyderabad, Jakarta, Mumbai, Osaka, Seoul, Singapore, Sydney, Tokyo), Europe (Frankfurt, Ireland, London, Paris, Stockholm), Middle East (Bahrain) Regions. AWS Premier Tier Services Partners— Escala24x7 , IBM , Tech Mahindra , and Xebia will support application operations with complementary features and services. Give it a try now in the AWS Management Console and send feedback to AWS re:Post for AWS Management Console , using the feedback link on the myApplications dashboard, or through your usual AWS Support contacts. — Channy', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='b4114b94-f6d1-4465-a4a4-c1d89b99e7fb', embedding=None, metadata={'title': 'Use Amazon CloudWatch to consolidate hybrid, multicloud, and on-premises metrics', 'summary': 'You can now consolidate metrics from your hybrid, multicloud, and on-premises data sources using Amazon CloudWatch and process them in a consistent, unified fashion.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='You can now consolidate metrics from your hybrid, multicloud , and on-premises data sources using Amazon CloudWatch and process them in a consistent, unified fashion. You can query, visualize, and alarm on any and all of the metrics, regardless of their source. In addition to giving you a unified view, this new feature will help you to identify trends and issues that span multiple parts and aspects of your infrastructure. When I first heard about this new feature, I thought, “Wait, I can do that with PutMetricData , what’s the big deal?” Quite a bit, as it turns out. PutMetricData stores the metrics in CloudWatch, but this cool new feature fetches them on demand, directly from the source. Instead of storing data, you select and configure connectors that pull data from Amazon Managed Service for Prometheus , generic Prometheus , Amazon OpenSearch Service , Amazon RDS for MySQL , Amazon RDS for PostgreSQL , CSV files stored in Amazon Simple Storage Service (Amazon S3) , and Microsoft Azure Monitor . Each connector is a AWS Lambda function that is deployed from a AWS CloudFormation template. CloudWatch invokes the appropriate Lambda functions as needed and makes use of the returned metrics immediately — they are not buffered or kept around. Creating and using connectors To get started I open the CloudWatch Console, click All metrics, and activate the Multi source query tab, then I click Create and manage data sources : And then I do it again: Then I choose a data source type: CloudWatch will then prompt me for the details that it needs to create and set up the connector for my data source. For example, if I select Amazon RDS – MySQL , I give my data source a name, choose the RDS database instance, and specify the connection info: When I click Create data source , a Lambda function, a Lambda Permission, an IAM role, a Secrets Manager Secret, a Log Group, and a AWS CloudFormation Stack will be created in my account: Then, when I am ready to reference the data source and make use of the metrics that it provides, I enter a SQL query that returns timestamps and values for the metric: Inside the Lambda function The code for the Custom – getting started template is short, simple, and easy to understand.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='994ad667-e423-4e82-bc7c-0acc545d27e4', embedding=None, metadata={'title': 'Use Amazon CloudWatch to consolidate hybrid, multicloud, and on-premises metrics', 'summary': 'You can now consolidate metrics from your hybrid, multicloud, and on-premises data sources using Amazon CloudWatch and process them in a consistent, unified fashion.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='It implements handlers for two events: DescribeGetMetricData – This handler returns a string that includes the name of the connector, default values for the arguments to the other handler, and a text description in Markdown format that is displayed in the custom data source query builder in the CloudWatch console. GetMetricData – This handler returns a metric name, 1-dimensional array of timestamps and metric values, all for a time range that is provided as arguments to the handler. If you spend a few minutes examining this code you should be able to see how to write functions to connect to your own data sources. Things to know Here are a couple of things to keep in mind about this powerful new feature: Regions – You can create and use data connectors in all commercial AWS Regions; a connector that is running in one region can connect to and retrieve data from services and endpoints in other regions and other AWS accounts. Pricing – There is no extra charge for the connectors. You pay for the invocations of the Lambda functions and for any other AWS infrastructure that you create. — Jeff ;', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='913cfe06-e402-46dc-b661-e480167163f0', embedding=None, metadata={'title': 'Use natural language to query Amazon CloudWatch logs and metrics (preview)', 'summary': 'To make it easy to interact with your operational data, Amazon CloudWatch is introducing natural language query generation for Logs and Metrics Insights.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='To make it easy to interact with your operational data, Amazon CloudWatch is introducing today natural language query generation for Logs and Metrics Insights. With this capability, powered by generative artificial intelligence (AI) , you can describe in English the insights you are looking for, and a Logs or Metrics Insights query will be automatically generated. This feature provides three main capabilities for CloudWatch Logs and Metrics Insights: Generate new queries from a description or a question to help you get started easily. Query explanation to help you learn the language including more advanced features. Refine existing queries using guided iterations. Let’s see how these work in practice with a few examples. I’ll cover logs first and then metrics. Generate CloudWatch Logs Insights queries with natural language In the CloudWatch console , I select Log Insights in the Logs section. I then select the log group of an AWS Lambda function that I want to investigate. I choose the Query generator button to open a new Prompt field where I enter what I need using natural language: Tell me the duration of the 10 slowest invocations Then, I choose Generate new query . The following Log Insights query is automatically generated: fields @timestamp, @requestId, @message, @logStream, @duration \\n| filter @type = \"REPORT\" and @duration > 1000\\n| sort @duration desc\\n| limit 10 I choose Run query to see the results. I find that now there’s too much information in the output. I prefer to see only the data I need, so I enter the following sentence in the Prompt and choose Update query . Show only timestamps and latency The query is updated based on my input and only the timestamp and duration are returned: fields @timestamp, @duration \\n| filter @type = \"REPORT\" and @duration > 1000\\n| sort @duration desc\\n| limit 10 I run the updated query and get a result that is easier for me to read. Now, I want to know if there are any errors in the log. I enter this sentence in the Prompt and generate a new query: Count the number of ERROR messages As requested, the generated query is counting the messages that contain the ERROR string: fields @message\\n| filter @message like /ERROR/\\n| stats count() I run the query and find out that there are more errors than I expected. I need more information.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='b494c257-1e39-4b8f-9626-45469319af15', embedding=None, metadata={'title': 'Use natural language to query Amazon CloudWatch logs and metrics (preview)', 'summary': 'To make it easy to interact with your operational data, Amazon CloudWatch is introducing natural language query generation for Logs and Metrics Insights.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='I use this prompt to update the query and get a better distribution of the errors: Show the errors per hour The updated query uses the bin() function to group the result in one hour intervals. fields @timestamp, @message\\n| filter @message like /ERROR/\\n| stats count(*) by bin(1h) Let’s see a more advanced query about memory usage. I select the log groups of a few Lambda functions and type: Show invocations with the most over-provisioned memory grouped by log stream Before generating the query, I choose the gear icon to toggle the options to include my prompt and an explanation as comment. Here’s the result (I split the explanation over multiple lines for readability): # Show invocations with the most over-provisioned memory grouped by log stream\\n\\nfields @logStream, @memorySize/1000/1000 as memoryMB, @maxMemoryUsed/1000/1000 as maxMemoryUsedMB, (@memorySize/1000/1000 - @maxMemoryUsed/1000/1000) as overProvisionedMB \\n| stats max(overProvisionedMB) as maxOverProvisionedMB by @logStream \\n| sort maxOverProvisionedMB desc\\n\\n# This query finds the amount of over-provisioned memory for each log stream by\\n# calculating the difference between the provisioned and maximum memory used.\\n# It then groups the results by log stream and calculates the maximum\\n# over-provisioned memory for each log stream. Finally, it sorts the results\\n# in descending order by the maximum over-provisioned memory to show\\n# the log streams with the most over-provisioned memory. Now, I have the information I need to understand these errors. On the other side, I also have EC2 workloads. How are those instances running? Let’s look at some metrics. Generate CloudWatch Metrics Insights queries with natural language In the CloudWatch console , I select All metrics in the Metrics section. Then, in the Multi source query tab, I use the Editor . If you prefer, the Query generator is available also in the Builder . I choose Query generator like before. Then, I enter what I need using plain English: Which 10 EC2 instances have the highest CPU utilization? I choose Generate new query and get a result using the Metrics Insights syntax.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='06c58f7d-d1e3-4df7-ae9d-44af64a6b10a', embedding=None, metadata={'title': 'Use natural language to query Amazon CloudWatch logs and metrics (preview)', 'summary': 'To make it easy to interact with your operational data, Amazon CloudWatch is introducing natural language query generation for Logs and Metrics Insights.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='SELECT AVG(\"CPUUtilization\")\\nFROM SCHEMA(\"AWS/EC2\", InstanceId)\\nGROUP BY InstanceId\\nORDER BY AVG() DESC\\nLIMIT 10 To see the graph, I choose Run . Well, it looks like my EC2 instances are not doing much. This result shows how those instances are using the CPU, but what about storage? I enter this in the prompt and choose Update query : How about the most EBS writes? The updated query replaces the average CPU utilization with the sum of bytes written to all EBS volumes attached to the instance. It keeps the limit to only show the top 10 results. SELECT SUM(\"EBSWriteBytes\")\\nFROM SCHEMA(\"AWS/EC2\", InstanceId)\\nGROUP BY InstanceId\\nORDER BY SUM() DESC\\nLIMIT 10 I run the query and, by looking at the result, I have a better understanding of how storage is being used by my EC2 instances. Try entering some requests and run the generated queries over your logs and metrics to see how this works with your data. Things to know Amazon CloudWatch natural language query generation for logs and metrics is available in preview in the US East (N. Virginia) and US West (Oregon) AWS Regions . There is no additional cost for using natural language query generation during the preview. You only pay for the cost of running the queries according to CloudWatch pricing . Generated queries are produced by generative AI and dependent on factors including the data selected and available in your account. For these reasons, your results may vary. When generating a query, you can include your original request and an explanation of the query as comments. To do so, choose the gear icon in the bottom right corner of the query edit window and toggle those options. This new capability can help you generate and update queries for logs and metrics, saving you time and effort. This approach allows engineering teams to scale their operations without worrying about specific data knowledge or query expertise. Use natural language to analyze your logs and metrics with Amazon CloudWatch. — Danilo', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='1df752de-d17b-4f5b-8a5b-fed48d06603a', embedding=None, metadata={'title': 'New Amazon CloudWatch log class for infrequent access logs at a reduced price', 'summary': 'This new log class offers a tailored set of capabilities at a lower cost for infrequently accessed logs, enabling customers to consolidate all their logs in one place in a cost-effective manner.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Amazon CloudWatch Logs announces today a new log class called Infrequent Access. This new log class offers a tailored set of capabilities at a lower cost for infrequently accessed logs, enabling customers to consolidate all their logs in one place in a cost-effective manner. As customers’ applications continue to scale and grow, so does the volume of logs generated. To limit the increase of logging costs, many customers are forced to make hard trade-offs. For example, some customers limit the logs generated by their applications, which can hinder the visibility of the application, or choose a different solution for different log types, which adds complexity and inefficiencies in managing different logging solutions. For instance, customers may send logs needed for real-time analytics and alerting to CloudWatch Logs and send more detailed logs needed for debugging and troubleshooting to a lower-cost solution that doesn’t have as many features as CloudWatch. In the end, these workarounds can impact the observability of the application, because customers need to navigate across multiple solutions to see their logs. The Infrequent Access log class allows you to build a holistic observability solution using CloudWatch by centralizing all your logs in one place to ingest, query, and store your logs in a cost-efficient way. Infrequent Access is 50 percent lower per GB ingestion price than Standard log class. It provides a tailored set of capabilities for customers that don’t need advanced features like Live Tail , metric extraction, alarming, or data protection that the Standard log class provides. With Infrequent Access, you can still get the benefits of fully managed ingestion, storage, and the ability to deep dive using CloudWatch Logs Insights . The following table shows a side-by-side comparison of the features that the new Infrequent Access and the Standard log classes have. Feature Infrequent Access log class Standard log class Fully managed ingestion and storage Available Available Cross-account Available Available Encryption with KMS Available Available Logs Insights Available Available Subscription filters / Export to S3 Not available Available GetLogEvents / FilterLogEvents Not available Available Contributor , Container , and Lambda Insights Not available Available Metric filter and alerting Not available Available Data protection Not available Available Embedded metric format (EMF) Not available Available Live Tail Not available Available When to use the new Infrequent Access log class Use the Infrequent Access log class when you have a new workload that doesn’t require advanced features provided by the Standard log class.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='a92b180c-256e-447d-bd37-ff272b041617', embedding=None, metadata={'title': 'New Amazon CloudWatch log class for infrequent access logs at a reduced price', 'summary': 'This new log class offers a tailored set of capabilities at a lower cost for infrequently accessed logs, enabling customers to consolidate all their logs in one place in a cost-effective manner.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='One important consideration is that when you create a log group with a specific log class, you cannot change that log group log class afterward. The Infrequent Access log class is suitable for debug logs or web server logs because they are quite verbose and rarely require any of the advanced functionality that the Standard log class provides. Another good workload for the Infrequent Access log class is an Internet of Things (IoT) fleet sending detailed logs that are only accessed for after the fact forensic analysis after the event. In addition, the Infrequent Access log class is a good choice for workloads where logs need to be stored for compliance because they will be queried infrequently. Getting started To get started using the new Infrequent Access log class, create a new log group in the CloudWatch Logs console and select the new Infrequent Access log class. You can create logs groups with the new Infrequent Access log class not only from the AWS Management Console but also from the AWS Command Line Interface (AWS CLI) , AWS CloudFormation , AWS Cloud Development Kit (AWS CDK) , and AWS SDKs . Once you have the new log group created, you can start using it in your workloads. For this example, I will configure a web application to send debug logs to this log group. After a while of the web application executes for a while, you can go back to the log group, where you see a new log stream. When you select a log stream, you will be directed to CloudWatch Logs Insights. Using the same familiar CloudWatch Logs Insights experience you get with Standard Class, you can create queries and search those logs to find relevant information, and you can analyze all the logs quickly in one place. Available now The new Infrequent Access log class is now available in all AWS Regions except the China and GovCloud Regions. You can start using it and enjoy a more cost-effective way to collect, store, and analyze your logs in a fully managed experience. To learn more about the new log class, you can check the CloudWatch Logs user guide dedicated page for the Infrequent Access log class . – Marcia', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='4022ddc6-f3d4-4706-b920-012ff2ca8b65', embedding=None, metadata={'title': 'Zonal autoshift automatically shifts your traffic away from Availability Zones when we detect potential issues', 'summary': 'The new capability of Amazon Route 53 Application Recovery Controller shifts your workload’s traffic away from an Availability Zone when it identifies a potential failure and shifts it back once the failure is resolved.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today we’re launching zonal autoshift, a new capability of Amazon Route 53 Application Recovery Controller that you can enable to automatically and safely shift your workload’s traffic away from an Availability Zone when AWS identifies a potential failure affecting that Availability Zone and shift it back once the failure is resolved. When deploying resilient applications, you typically deploy your resources across multiple Availability Zones in a Region. Availability Zones are distinct groups of physical data centers at a meaningful distance apart (typically miles) to make sure that they have diverse power, connectivity, network devices, and flood plains. To help you protect against an application’s errors, like a failed deployment, an error of configuration, or an operator error, we introduced last year the ability to manually or programmatically trigger a zonal shift. This enables you to shift the traffic away from one Availability Zone when you observe degraded metrics in that zone. It does so by configuring your load balancer to direct all new connections to infrastructure in healthy Availability Zones only. This allows you to preserve your application’s availability for your customers while you investigate the root cause of the failure. Once fixed, you stop the zonal shift to ensure the traffic is distributed across all zones again. Zonal shift works at the Application Load Balancer (ALB) or Network Load Balancer (NLB) level only when cross-zone load balancing is turned off, which is the default for NLB. In a nutshell, load balancers offer two levels of load balancing. The first level is configured in the DNS. Load balancers expose one or more IP addresses for each Availability Zone, offering a client-side load balancing between zones. Once the traffic hits an Availability Zone, the load balancer sends traffic to registered healthy targets, typically an Amazon Elastic Compute Cloud (Amazon EC2) instance. By default, ALBs send traffic to targets across all Availability Zones. For zonal shift to properly work, you must configure your load balancers to disable cross-zone load balancing. When zonal shift starts, the DNS sends all traffic away from one Availability Zone, as illustrated by the following diagram. Manual zonal shift helps to protect your workload against errors originating from your side. But when there is a potential failure in an Availability Zone, it is sometimes difficult for you to identify or detect the failure. Detecting an issue in an Availability Zone using application metrics is difficult because, most of the time, you don’t track metrics per Availability Zone.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='acc34781-9218-4ced-851d-06983fc5b47e', embedding=None, metadata={'title': 'Zonal autoshift automatically shifts your traffic away from Availability Zones when we detect potential issues', 'summary': 'The new capability of Amazon Route 53 Application Recovery Controller shifts your workload’s traffic away from an Availability Zone when it identifies a potential failure and shifts it back once the failure is resolved.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Moreover, your services often call dependencies across Availability Zone boundaries, resulting in errors seen in all Availability Zones. With modern microservice architectures, these detection and recovery steps must often be performed across tens or hundreds of discrete microservices, leading to recovery times of multiple hours. Customers asked us if we could take the burden off their shoulders to detect a potential failure in an Availability Zone. After all, we might know about potential issues through our internal monitoring tools before you do. With this launch, you can now configure zonal autoshift to protect your workloads against potential failure in an Availability Zone. We use our own AWS internal monitoring tools and metrics to decide when to trigger a network traffic shift. The shift starts automatically; there is no API to call. When we detect that a zone has a potential failure, such as a power or network disruption, we automatically trigger an autoshift of your infrastructure’s NLB or ALB traffic, and we shift the traffic back when the failure is resolved. Obviously, shifting traffic away from an Availability Zone is a delicate operation that must be carefully prepared. We built a series of safeguards to ensure we don’t degrade your application availability by accident. First, we have internal controls to ensure we shift traffic away from no more than one Availability Zone at a time. Second, we practice the shift on your infrastructure for 30 minutes every week. You can define blocks of time when you don’t want the practice to happen, for example, 08:00–18:00, Monday through Friday. Third, you can define two Amazon CloudWatch alarms to act as a circuit breaker during the practice run: one alarm to prevent starting the practice run at all and one alarm to monitor your application health during a practice run. When either alarm triggers during the practice run, we stop it and restore traffic to all Availability Zones. The state of application health alarm at the end of the practice run indicates its outcome: success or failure. According to the principle of shared responsibility , you have two responsibilities as well. First you must ensure there is enough capacity deployed in all Availability Zones to sustain the increase of traffic in remaining Availability Zones after traffic has shifted. We strongly recommend having enough capacity in remaining Availability Zones at all times and not relying on scaling mechanisms that could delay your application recovery or impact its availability. When zonal autoshift triggers, AWS Auto Scaling might take more time than usual to scale your resources.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='6259b247-f474-40ba-8414-327ed0fab15d', embedding=None, metadata={'title': 'Zonal autoshift automatically shifts your traffic away from Availability Zones when we detect potential issues', 'summary': 'The new capability of Amazon Route 53 Application Recovery Controller shifts your workload’s traffic away from an Availability Zone when it identifies a potential failure and shifts it back once the failure is resolved.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Pre-scaling your resource ensures a predictable recovery time for your most demanding applications. Let’s imagine that to absorb regular user traffic, your application needs six EC2 instances across three Availability Zones (2×3 instances). Before configuring zonal autoshift, you should ensure you have enough capacity in the remaining Availability Zones to absorb the traffic when one Availability Zone is not available. In this example, it means three instances per Availability Zone (3×3 = 9 instances with three Availability Zones in order to keep 2×3 = 6 instances to handle the load when traffic is shifted to two Availability Zones). In practice, when operating a service that requires high reliability, it’s normal to operate with some redundant capacity online for eventualities such as customer-driven load spikes, occasional host failures, etc. Topping up your existing redundancy in this way both ensures you can recover rapidly during an Availability Zone issue but can also give you greater robustness to other events. Second, you must explicitly enable zonal autoshift for the resources you choose. AWS applies zonal autoshift only on the resources you chose. Applying a zonal autoshift will affect the total capacity allocated to your application. As I just described, your application must be prepared for that by having enough capacity deployed in the remaining Availability Zones. Of course, deploying this extra capacity in all Availability Zones has a cost. When we talk about resilience, there is a business tradeoff to decide between your application availability and its cost. This is another reason why we apply zonal autoshift only on the resources you select. Let’s see how to configure zonal autoshift To show you how to configure zonal autoshift, I deploy my now-famous TicTacToe web application using a CDK script . I open the Route 53 Application Recovery Controller page of the AWS Management Console . On the left pane, I select Zonal autoshift . Then, on the welcome page, I select Configure zonal autoshift for a resource . I select the load balancer of my demo application. Remember that currently, only load balancers with cross-zone load balancing turned off are eligible for zonal autoshift. As the warning on the console reminds me, I also make sure my application has enough capacity to continue to operate with the loss of one Availability Zone.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='6148053d-227a-4fd3-b376-cd2a87f9a593', embedding=None, metadata={'title': 'Zonal autoshift automatically shifts your traffic away from Availability Zones when we detect potential issues', 'summary': 'The new capability of Amazon Route 53 Application Recovery Controller shifts your workload’s traffic away from an Availability Zone when it identifies a potential failure and shifts it back once the failure is resolved.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='I scroll down the page and configure the times and days I don’t want AWS to run the 30-minute practice. At first, and until I’m comfortable with autoshift, I block the practice 08:00–18:00, Monday through Friday. Pay attention that hours are expressed in UTC, and they don’t vary with daylight saving time . You may use a UTC time converter application for help. While it is safe for you to exclude business hours at the start, we recommend configuring the practice run also during your business hours to ensure capturing issues that might not be visible when there is low or no traffic on your application. You probably most need zonal autoshift to work without impact at your peak time, but if you have never tested it, how confident are you? Ideally, you don’t want to block any time at all, but we recognize that’s not always practical. Further down on the same page, I enter the two circuit breaker alarms. The first one prevents the practice from starting. You use this alarm to tell us this is not a good time to start a practice run. For example, when there is an issue ongoing with your application or when you’re deploying a new version of your application to production. The second CloudWatch alarm gives the outcome of the practice run. It enables zonal autoshift to judge how your application is responding to the practice run. If the alarm stays green, we know all went well. If either of these two alarms triggers during the practice run, zonal autoshift stops the practice and restores the traffic to all Availability Zones. Finally, I acknowledge that a 30-minute practice run will run weekly and that it might reduce the availability of my application. Then, I select Create . And that’s it. After a few days, I see the history of the practice runs on the Zonal shift history for resource tab of the console. I monitor the history of my two circuit breaker alarms to stay confident everything is correctly monitored and configured. It’s not possible to test an autoshift itself. It triggers automatically when we detect a potential issue in an Availability Zone. I asked the service team if we could shut down an Availability Zone to test the instructions I shared in this post; they politely declined my request :-). To test your configuration, you can trigger a manual shift, which behaves identically to an autoshift.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='5f887115-dd6d-4d2d-9eef-fe6338eadf0f', embedding=None, metadata={'title': 'Zonal autoshift automatically shifts your traffic away from Availability Zones when we detect potential issues', 'summary': 'The new capability of Amazon Route 53 Application Recovery Controller shifts your workload’s traffic away from an Availability Zone when it identifies a potential failure and shifts it back once the failure is resolved.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='A few more things to know Zonal autoshift is now available at no additional cost in all AWS Regions, except for China and GovCloud. We recommend applying the crawl, walk, run methodology. First, you get started with manual zonal shifts to acquire confidence in your application. Then, you turn on zonal autoshift configured with practice runs outside of your business hours. Finally, you modify the schedule to include practice zonal shifts during your business hours. You want to test your application response to an event when you least want it to occur. We also recommend that you think holistically about how all parts of your application will recover when we move traffic away from one Availability Zone and then back. The list that comes to mind (although certainly not complete) is the following. First, plan for extra capacity as I discussed already. Second, think about possible single points of failure in each Availability Zone, such as a self-managed database running on a single EC2 instance or a microservice that leaves in a single Availability Zone, and so on. I strongly recommend using managed databases, such as Amazon DynamoDB or Amazon Aurora for applications requiring zonal shifts. These have built-in replication and fail-over mechanisms in place. Third, plan the switch back when the Availability Zone will be available again. How much time do you need to scale your resources? Do you need to rehydrate caches? You can learn more about resilient architectures and methodologies with this great series of articles from my colleague Adrian . Finally, remember that only load balancers with cross-zone load balancing turned off are currently eligible for zonal autoshift. To turn off cross-zone load balancing from a CDK script, you need to remove stickinessCookieDuration and add load_balancing.cross_zone.enabled=false on the target group. Here is an example with CDK and Typescript: // Add the auto scaling group as a load balancing\\n    // target to the listener.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='53074182-7510-4075-b699-fb041619f28b', embedding=None, metadata={'title': 'Zonal autoshift automatically shifts your traffic away from Availability Zones when we detect potential issues', 'summary': 'The new capability of Amazon Route 53 Application Recovery Controller shifts your workload’s traffic away from an Availability Zone when it identifies a potential failure and shifts it back once the failure is resolved.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='const targetGroup = listener.addTargets(\\'MyApplicationFleet\\', {\\n      port: 8080,\\n      // for zonal shift, stickiness & cross-zones load balancing must be disabled\\n      // stickinessCookieDuration: Duration.hours(1),\\n      targets: [asg]\\n    });    \\n    // disable cross zone load balancing\\n    targetGroup.setAttribute(\"load_balancing.cross_zone.enabled\", \"false\"); Now it’s time for you to select your applications that would benefit from zonal autoshift. Start by reviewing your infrastructure capacity in each Availability Zone and then define the circuit breaker alarms. Once you are confident your monitoring is correctly configured, go and enable zonal autoshift . -- seb', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='b319e934-636f-4d4c-83c0-8cae6c5f57a5', embedding=None, metadata={'title': 'Mutual authentication for Application Load Balancer reliably verifies certificate-based client identities', 'summary': 'With this new feature, you can now offload client authentication to Application Load Balancer, ensuring only trusted clients communicate with backend applications.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, we are announcing support for mutually authenticating clients that present X509 certificates to Application Load Balancer . With this new feature, you can now offload client authentication to the load balancer, ensuring only trusted clients communicate with their backend applications. This new capability is built on S2N , AWS’s open source Transport Layer Security (TLS) implementation that provides strong encryption and protections against zero-day vulnerabilities, which developers can trust. Mutual authentication (mTLS) is commonly used for business-to-business (B2B) applications such as online banking, automobile, or gaming devices to authenticate devices using digital certificates. Companies typically use it with a private certificate authority (CA) to authenticate their clients before granting access to data and services. Customers have implemented mutual authentication using self-created or third-party solutions that require additional time and management overhead. These customers spend their engineering resources to build the functionality into their backend, update their code to keep up with the latest security patches, and invest heavily in infrastructure to create and rotate certificates. With mutual authentication on Application Load Balancer, you have a fully managed, scalable, and cost-effective solution that enables you to use your developer resources to focus on other critical projects. Your ALB will authenticate clients with revocation checks and pass client certificate information to the target, which can be used for authorization by applications. Getting started with mutual authentication on ALB To enable mutual authentication on ALB, choose Create Application Load Balancer by the ALB wizard on Amazon EC2 console . When you select HTTPS in the Listeners and routing section , you can see more settings such as security policy, default server certificate, and a new client certificate handling option to support mutual authentication. With Mutual authentication (mTLS) enabled, you can configure how listeners handle requests that present client certificates. This includes how your Application Load Balancer authenticates certificates and the amount of certificate metadata that is sent to your backend targets. Mutual authentication has two options. The Passthrough option sends all the client certificate chains received from the client to your backend application using HTTP headers. The mTLS-enabled Application Load Balancer gets the client certificate in the handshake, establishes a TLS connection, and then sends whatever it gets in HTTPS headers to the target application. The application will need to verify the client certificate chain to authenticate the client.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='30815767-bfda-4bb2-892c-3566ba4789de', embedding=None, metadata={'title': 'Mutual authentication for Application Load Balancer reliably verifies certificate-based client identities', 'summary': 'With this new feature, you can now offload client authentication to Application Load Balancer, ensuring only trusted clients communicate with backend applications.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='With the Verify with trust store option, Application Load Balancer and client verify each other’s identity and establish a TLS connection to encrypt communication between them. We introduce a new trust store feature, and you can upload any CA bundle with roots and/or intermediate certificates generated by AWS Private Certificate Authority or any other third party CA\\xa0as the source of trust to validate your client certificates. It requires selecting an existing trust store or creating a new one. Trust stores contain your CAs, trusted certificates, and, optionally, certificate revocation lists (CRLs). The load balancer uses a trust store to perform mutual authentication with clients. To use this option and create a new trust store, choose Trust Stores in the left menu of the Amazon EC2 console and choose Create trust store . You can choose a CA certificate bundle in PEM format and, optionally, CRLs from your Amazon Simple Storage Service (Amazon S3) bucket. A CA certificate bundle is a group of CA certificates (root or intermediate) used by a trust store. CRLs can be used when a CA revokes client certificates that have been compromised, and you need to reject those revoked certificates. You can replace a CA bundle, and add or remove CRLs from the trust store after creation. You can use the AWS Command Line Interface (AWS CLI) with new APIs such as create-trust-store to upload CA information, configure the mutual-authentication-mode on the Application Load Balancer listener, and send user certificate information to targets. $ aws elbv2 create-trust-store --name my-tls-name \\\\\\n    --ca-certificates-bundle-s3-bucket channy-certs \\\\\\n    --ca-certificates-bundle-s3-key Certificates.pem \\\\\\n    --ca-certificates-bundle-s3-object-version <version>\\n>> arn:aws:elasticloadbalancing:root:file1\\n$ aws elbv2 create-listener --load balancer-arn <value> \\\\\\n    --protocol HTTPS \\\\\\n    --port 443 \\\\\\n    --mutual-authentication Mode=verify,\\n      TrustStoreArn=<arn:aws:elasticloadbalancing:root:file1> If you already have your own private CA, such as AWS Private CA, third-party CA, or self-signed CA, you can upload their CA bundle or CRLs to the Application Load Balancer trust store to enable mutual authentication.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e82ce1dd-6854-413a-afd8-7a5b68f5683a', embedding=None, metadata={'title': 'Mutual authentication for Application Load Balancer reliably verifies certificate-based client identities', 'summary': 'With this new feature, you can now offload client authentication to Application Load Balancer, ensuring only trusted clients communicate with backend applications.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='To test the mutual authentication on Application Load Balancer, follow the step-by-step instructions to make a self-signed CA bundle and client certificate using OpenSSL, upload them to the Amazon S3 bucket, and use them with an ELB trust store. You can use curl with the --key and --cert parameters to send the client certificate as part of the request: $ curl --key my_client.key --cert my_client.pem https://api.yourdomain.com Mutual authentication can fail if a client presents an invalid or expired certificate, fails to present a certificate, cannot find a trust chain, or if any links in the trust chain have expired, or the certificate is on the revocation list. Application Load Balancer will close the connections whenever it fails to authenticate a client and will record new connection logs that capture detailed information about requests sent to your load balancer. Each log contains information such as the client’s IP address, handshake latency, TLS cipher used, and client certificate details. You can use these connection logs to analyze request patterns and troubleshoot issues. To learn more, see Mutual authentication on Application Load Balancer in the AWS documentation. Now available Mutual authentication on Application Load Balancer is now available in all commercial AWS Regions where Application Load Balancer is available, except China. With no upfront costs or commitments required, you only pay for what you use. To learn more, see the Elastic Load Balancing pricing page. Give it a try now and send feedback to AWS re:Post for Amazon EC2 or through your usual AWS Support contacts. Learn more: Application Load Balancer product page — Channy', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='258ddfaf-5001-4139-9428-4b3f3b064d77', embedding=None, metadata={'title': 'External endpoints and testing of task states now available in AWS Step Functions', 'summary': 'Now AWS Step Functions HTTPS endpoints let you integrate third-party APIs and external services to your workflows.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Now AWS Step Functions HTTPS endpoints let you integrate third-party APIs and external services to your workflows. HTTPS endpoints provide a simpler way of making calls to external APIs and integrating with existing SaaS providers, like Stripe for handling payments, GitHub for code collaboration and repository management, and Salesforce for sales and marketing insights. Before this launch, customers needed to use an AWS Lambda function to call the external endpoint, handling authentication and errors directly from the code. Also, we are announcing a new capability to test your task states individually without the need to deploy or execute the state machine. AWS Step Functions is a visual workflow service that makes it easy for developers to build distributed applications, automate processes, orchestrate microservices, and create data and machine learning (ML) pipelines. Step Functions integrates with over 220 AWS services and provides features that help developers build, such as built-in error handling, real-time and auditable workflow execution history, and large-scale parallel processing. HTTPS endpoints HTTPS endpoints are a new resource for your task states that allow you to connect to third-party HTTP targets outside AWS. Step Functions invokes the HTTP endpoint, deliver a request body, headers, and parameters, and get a response from the third-party services. You can use any preferred HTTP method, such as GET or POST. HTTPS endpoints use Amazon EventBridge connections to manage the authentication credentials for the target. This defines the authorization type used, which can be a basic authentication with a username and password, an API key, or OAuth. EventBridge connections use AWS Secrets Manager to store the secret. This keeps the secrets out of the state machine, reducing the risks of accidentally exposing your secrets in logs or in the state machine definition. Getting started with HTTPS endpoints To get started with HTTPS endpoints, first you need to create an EventBridge connection . Then you need to create a new AWS Identity and Access Management (IAM) role and give permissions so your state machine can access the connection resource, get the secret from Secrets Manager, and get permissions to invoke an HTTP endpoint.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='18460208-3404-4f5a-aa2f-0b53a0dc6658', embedding=None, metadata={'title': 'External endpoints and testing of task states now available in AWS Step Functions', 'summary': 'Now AWS Step Functions HTTPS endpoints let you integrate third-party APIs and external services to your workflows.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Here are the policies that you need to include in your state machine execution role: {\\n    \"Version\": \"2012-10-17\",\\n    \"Statement\": [\\n        {\\n            \"Effect\": \"Allow\",\\n            \"Action\": [\\n                \"secretsmanager:GetSecretValue\",\\n                \"secretsmanager:DescribeSecret\"\\n            ],\\n            \"Resource\": \"arn:aws:secretsmanager:*:*:secret:events!connection/*\"\\n        }\\n    ]\\n}\\n{\\n    \"Version\": \"2012-10-17\",\\n    \"Statement\": [\\n        {\\n            \"Sid\": \"RetrieveConnectionCredentials\",\\n            \"Effect\": \"Allow\",\\n            \"Action\": [\\n                \"events:RetrieveConnectionCredentials\"\\n            ],\\n            \"Resource\": [\\n                \"arn:aws:events:us-east-2:123456789012:connection/oauth_connection/aeabd89e-d39c-4181-9486-9fe03e6f286a\"\\n            ]\\n        }\\n    ]\\n}\\n{\\n    \"Version\": \"2012-10-17\",\\n    \"Statement\": [\\n        {\\n            \"Sid\": \"InvokeHTTPEndpoint\",\\n            \"Effect\": \"Allow\",\\n            \"Action\": [\\n                \"states:InvokeHTTPEndpoint\"\\n            ],\\n            \"Resource\": [\\n                \"arn:aws:states:us-east-2:123456789012:stateMachine:myStateMachine\"\\n            ]\\n        }\\n    ]\\n} After you have everything ready, you can create your state machine. In your state machine, add a new task state to call a third-party API. You can configure the API endpoint to point to the third-party URL you need, set the correct HTTP method , pick the connection Amazon Resource Name (ARN) for the connection you created previously as the authentication for that endpoint, and provide a request body if needed. In addition, all these parameters can be set dynamically at runtime from the state JSON input. Now, making external requests with Step Functions is easy, and you can take advantage of all the configurations that Step Functions provides to handle errors , such as retries for transient errors or momentary service unavailability, and redrive for errors that require longer investigation or resolution time. Test state To accelerate feedback cycles, we are also announcing a new capability to test individual states. This new feature allows you to test states independently from the execution of your workflow. This is particularly useful for testing endpoints configuration.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='16cdf86e-9f1c-46cf-9367-b2842dfdfa4f', embedding=None, metadata={'title': 'External endpoints and testing of task states now available in AWS Step Functions', 'summary': 'Now AWS Step Functions HTTPS endpoints let you integrate third-party APIs and external services to your workflows.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='You can change the input and test the different scenarios without the need to deploy your workflow or execute the whole state machine. This new feature is available in all task, choice, and pass states. You will see the testing capability in the Step Functions Workflow Studio when you select a task. When you choose the Test state , you will be redirected to a different view where you can test the task state. You can test that the state machine role has the right permissions, the endpoint you want to call is correctly configured, and verify that the data manipulations work as expected. Availability Now, with all the features that Step Functions provides, it’s never been easier to build state machines that can solve a wide variety of problems, like payment flows, workflows with manual inputs, and integration to legacy systems. Using Step Functions HTTPS endpoints, you can directly integrate with popular payment platforms while ensuring that your users’ credit cards are only charged once and errors are handled automatically. In addition, you can test this new integration even before you deploy the state machine using the new test state feature. These new features will be rolled out in all AWS Regions except Asia Pacific (Hyderabad), Asia Pacific (Melbourne), AWS Israel (Tel Aviv), China, and GovCloud Regions before the end of December. To get started you can try the “Generate Invoices using Stripe” sample project from Step Functions in the AWS Managment Console or check out the AWS Step Functions Developer Guide to learn more. — Marcia A correction was made on December 11, 2023 : An earlier version of this post misstated that “all the new features are available in all AWS Regions Asia Pacific (Hyderabad), Asia Pacific (Melbourne), AWS Israel (Tel Aviv), China, and GovCloud Regions”. The correct information is that the “These new features will be rolled out in all AWS Regions except Asia Pacific (Hyderabad), Asia Pacific (Melbourne), AWS Israel (Tel Aviv), China, and GovCloud Regions before the end of December”.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='9016061d-3302-46b2-8e71-b91c70d55d8e', embedding=None, metadata={'title': 'Announcing new diagnostic tools for AWS Partner-Led Support (PLS) participants', 'summary': 'The AWS Partner-Led Support program now has access to the same tools that AWS Support Engineers use to assist AWS customers.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='We have added a set of diagnostic tools that will give participants in the AWS Partner-Led Support program access to diagnostic tools that will empower them to do an even better job of supporting their customers. Intro to AWS Partner-Led Support This AWS Partner Network (APN) program enables AWS Partners to act as the customer’s sole point of contact for technical support. Customers contact their support partner for technical assistance instead of directly contacting AWS. In many cases the partner can resolve the issue directly. If the partner cannot do this, they get guidance from AWS via their AWS Support plan. Diagnostic tools These are the same tools that AWS Support Engineers use to assist AWS customers. When a customer contacts their partner for support, the partner will federate into the customer’s AWS account. Then they will use the new diagnostic tools to access the customer metadata that will help them to identify and diagnose the issue. The tools are enabled by a set of IAM roles set up by the customer. The tools can access and organize metadata and CloudWatch metrics, but they cannot access customer data and they cannot make any changes to any of the customer’s AWS resources. Here is a small sample of the types of information that partners will be able to access: EC2 Capacity Reservations Lambda Functions List GuardDuty Findings Load Balancer Responses RDS and Redshift Clusters Each tool operates on a list of regions selected when the tool is run, all invocations of each tool are logged and are easily accessible for review, and the output from each invocation can be directed to one of several different regions. The tools can be invoked from the AWS Management Console , with API access available in order to support in-house tools, automation, and integration. Learn more The service is available today for partners that have joined the Partner-Led Support program. For more information, see the AWS Partner Led Support page. If you are a current AWS Partner and would like to learn more about this program with an eye toward qualifying and participating, please visit AWS Partner Central . Learn more about AWS Diagnostic Tools here . — Jeff ;', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ff4791e6-f1cf-4ff4-b595-247e7f7fbf37', embedding=None, metadata={'title': 'Three new capabilities for Amazon Inspector broaden the realm of vulnerability scanning for workloads', 'summary': 'Amazon Inspector introduces a new set of open source plugins and an API, continuous monitoring for your Amazon EC2 instances, and generative AI-powered assisted code remediation for your AWS Lambda functions.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, Amazon Inspector adds three new capabilities to increase the realm of possibilities when scanning your workloads for software vulnerabilities: Amazon Inspector introduces a new set of open source plugins and an API allowing you to assess your container images for software vulnerabilities at build time directly from your continuous integration and continuous delivery (CI/CD) pipelines wherever they are running. Amazon Inspector can now continuously monitor your Amazon Elastic Compute Cloud (Amazon EC2) instances without installing an agent or additional software (in preview). Amazon Inspector uses generative artificial intelligence (AI) and automated reasoning to provide assisted code remediation for your AWS Lambda functions. Amazon Inspector is a vulnerability management service that continually scans your AWS workloads for known software vulnerabilities and unintended network exposure. Amazon Inspector automatically discovers and scans running EC2 instances, container images in Amazon Elastic Container Registry (Amazon ECR) and within your CI/CD tools, and Lambda functions. We all know engineering teams often face challenges when it comes to promptly addressing vulnerabilities. This is because of the tight release deadlines that force teams to prioritize development over tackling issues in their vulnerability backlog. But it’s also due to the complex and ever-evolving nature of the security landscape. As a result, a study showed that organizations take 250 days on average to resolve critical vulnerabilities . It is therefore crucial to identify potential security issues early in the development lifecycle to prevent their deployment into production. Detecting vulnerabilities in your AWS Lambda functions code Let’s start close to the developer with Lambda functions code. In November 2022 and June 2023 , Amazon Inspector added the capability to scan your function’s dependencies and code. Today, we’re adding generative AI and automated reasoning to analyze your code and automatically create remediation as code patches. Amazon Inspector can now provide in-context code patches for multiple classes of vulnerabilities detected during security scans. Amazon Inspector extends the assessment of your code for security issues like injection flaws, data leaks, weak cryptography, or missing encryption. Thanks to generative AI, Amazon Inspector now provides suggestions how to fix it. It shows affected code snippets in context with suggested remediation. Here is an example. I wrote a short snippet of Python code with a hardcoded AWS secret key. Never do that! def create_session_noncompliant():\\n    import boto3\\n    # Noncompliant: uses hardcoded secret access key.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e1500b8f-471e-4b7f-b9c7-d47f5eeb2cfd', embedding=None, metadata={'title': 'Three new capabilities for Amazon Inspector broaden the realm of vulnerability scanning for workloads', 'summary': 'Amazon Inspector introduces a new set of open source plugins and an API, continuous monitoring for your Amazon EC2 instances, and generative AI-powered assisted code remediation for your AWS Lambda functions.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='sample_key = \"AjWnyxxxxx45xxxxZxxxX7ZQxxxxYxxx1xYxxxxx\"\\n    boto3.session.Session(aws_secret_access_key=sample_key)\\n    return response I deploy the code. This triggers the assessment. I open the AWS Management Console and navigate to the Amazon Inspector page. In the Findings section, I find the vulnerability. It gives me the Vulnerability location and the Suggested remediation in a plain natural language explanation but also in diff text and graphical formats. Detecting vulnerabilities in your container CI/CD pipeline Now, let’s move to your CI/CD pipelines when building containers. Until today, Amazon Inspector was able to assess container images once they were built and stored in Amazon Elastic Container Registry (Amazon ECR). Starting today, Amazon Inspector can detect security issues much sooner in the development process by assessing container images during their build within CI/CD tools. Assessment results are returned in near real-time directly to the CI/CD tool’s dashboard. There is no need to enable Amazon Inspector to use this new capability. We provide ready-to-use CI/CD plugins for Jenkins and JetBrain’s TeamCity , with more to come. There is also a new API ( inspector-scan ) and command ( inspector-sbomgen ) available from our AWS SDKs and AWS Command Line Interface (AWS CLI) . This new API allows you to integrate Amazon Inspector in the CI/CD tool of your choice. Upon execution, the plugin runs a container extraction engine on the configured resource and generates a CycloneDX-compatible software bill of materials (SBOM) . Then, the plugin sends the SBOM to Amazon Inspector for analysis. The plugin receives the result of the scan in near real-time. It parses the response and generates outputs that Jenkins or TeamCity uses to pass or fail the execution of the pipeline. To use the plugin with Jenkins, I first make sure there is a role attached to the EC2 instance where Jenkins is installed, or I have an AWS access key and secret access key with permissions to call the Amazon Inspector API. I install the plugin directly from Jenkins ( Jenkins Dashboard > Manage Jenkins > Plugins ) Then, I add an Amazon Inspector Scan step in my pipeline. I configure the step with the IAM Role I created (or an AWS access key and secret access key when running on premises), my Docker Credentials , the AWS Region , and the Image Id .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f46b5110-639d-4a35-9ddb-b6d4bec4a6b7', embedding=None, metadata={'title': 'Three new capabilities for Amazon Inspector broaden the realm of vulnerability scanning for workloads', 'summary': 'Amazon Inspector introduces a new set of open source plugins and an API, continuous monitoring for your Amazon EC2 instances, and generative AI-powered assisted code remediation for your AWS Lambda functions.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='When Amazon Inspector detects vulnerabilities, it reports them to the plugin. The build fails, and I can view the details directly in Jenkins. The SBOM generation understands packages or applications for popular operating systems, such as Alpine, Amazon Linux, Debian, Ubuntu, and Red Hat packages. It also detects packages for Go, Java, NodeJS, C#, PHP, Python, Ruby, and Rust programming languages. Detecting vulnerabilities on Amazon EC2 without installing agents (in preview) Finally, let’s talk about agentless inspection of your EC2 instances. Currently, Amazon Inspector uses AWS Systems Manager and the AWS Systems Manager Agent (SSM Agent) to collect information about the inventory of your EC2 instances. To ensure Amazon Inspector can communicate with your instances, you have to ensure three conditions. First, a recent version of the SSM Agent is installed on the instance. Second, the SSM Agent is started. And third, you attached an IAM role to the instance to allow the SSM Agent to communicate back to the SSM service. This seems fair and simple. But it is not when considering large deployments across multiple OS versions, AWS Regions, and accounts, or when you manage legacy applications. Each instance launched that doesn’t satisfy these three conditions is a potential security gap in your infrastructure. With agentless scanning (in preview), Amazon Inspector doesn’t require the SSM Agent to scan your instances. It automatically discovers existing and new instances and schedules a vulnerability assessment for them. It does so by taking a snapshot of the instance’s EBS volumes and analyzing the snapshot. This technique has the extra advantage of not consuming any CPU cycle or memory on your instances, leaving 100 percent of the (virtual) hardware available for your workloads. After the analysis, Amazon Inspector deletes the snapshot. To get started, enable hybrid scanning under EC2 scanning settings in the Amazon Inspector section of the AWS Management Console . Hybrid mode means Amazon Inspector continues to use the SSM Agent–based scanning for instances managed by SSM and automatically switches to agentless for instances that are not managed by SSM. Under Account management , I can verify the list of scanned instances. I can see which instances are scanned with the SSM Agent and which are not. Under Findings , I can filter by vulnerability, by account, by instance, and so on. I select by instance and select the agentless instance I want to review.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='938d4dda-c836-4785-9b2a-870ad031534d', embedding=None, metadata={'title': 'Three new capabilities for Amazon Inspector broaden the realm of vulnerability scanning for workloads', 'summary': 'Amazon Inspector introduces a new set of open source plugins and an API, continuous monitoring for your Amazon EC2 instances, and generative AI-powered assisted code remediation for your AWS Lambda functions.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='For that specific instance, Amazon Inspector lists more than 200 findings, sorted by severity. As usual, I can see the details of a finding to understand what the risk is and how to mitigate it. Pricing and availability Amazon Inspector code remediation for Lambda functions is available in ten Regions: US East (Ohio, N. Virginia), US West (Oregon), Asia Pacific (Singapore, Sydney, Tokyo), and Europe (Frankfurt, Ireland, London, Stockholm). It is available at no additional cost. Amazon Inspector agentless vulnerability scanning for Amazon EC2 is available in preview in three AWS Regions: US East (N. Virginia), US West (Oregon), and Europe (Ireland). The new API to scan containers at build time is available in the 21 AWS Regions where Amazon Inspector is available today . There are no upfront or subscription costs. We charge on-demand based on the volume of activity. There is a price per EC2 instance or container image scan. As usual, the Amazon Inspector pricing page has the details. Start today by adding the Jenkins or TeamCity agent to your containerized application CI/CD pipelines or activate the agentless Amazon EC2 inspection. Now go build ! -- seb', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='cfa1f540-35e2-4575-bbea-a698a54d1b4b', embedding=None, metadata={'title': 'AWS Control Tower adds 65 new controls', 'summary': 'With this launch, we’ve added a set of purpose-built controls to help you meet your digital sovereignty requirements.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, we added to AWS Control Tower a set of 65 purpose-built controls to help you meet your digital sovereignty requirements. Digital sovereignty is the control of your digital assets: where the data resides, where it flows, and who has control over it. Since the creation of the AWS Cloud 17 years ago, we have been committed to giving you control over your data. In November last year, we launched the AWS Digital Sovereignty Pledge , our commitment to offering all AWS customers the most advanced set of sovereignty controls and features available in the cloud. Since then, we have announced several steps in that direction. The AWS Nitro System has been validated by an independent third party to confirm that it contains no mechanism that allows anyone at AWS to access your data on AWS hosts. We launched AWS Dedicated Local Zones , a piece of infrastructure that is fully managed by AWS and built for exclusive use by a customer or community and placed in a customer-specified location or data center. And more recently, we announced the construction of a new independent sovereign Region in Europe . The introduction of AWS Control Tower controls that support digital sovereignty is an additional step in our roadmap of capabilities for data residency, granular access restriction, encryption, and resilience. AWS Control Tower offers a simple and efficient way to set up and govern a secure, multi-account AWS environment. It establishes a landing zone that is based on best-practices blueprints, and it enables governance using controls you can choose from a prepackaged list. The landing zone is a well-architected , multi-account baseline that follows AWS best practices . Controls implement governance rules for security, compliance, and operations. The level of control required for digital assets greatly varies across industries and countries. Customers operating in highly regulated sectors might have the obligation to keep their data in a specific country or region, such as the European Union. Others might have obligations related to data encryption and where the encryption keys are kept, and so on. Furthermore, digital sovereignty requirements evolve rapidly, making it challenging to define and implement all the required controls. Many customers have told us they are concerned that they will have to choose between the full power of AWS and a feature-limited sovereign cloud solution that could hamper their ability to innovate, transform, and grow. We firmly believe that you shouldn’t have to make this choice.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='eb0b16b7-5467-4963-8c51-ae52de5306d3', embedding=None, metadata={'title': 'AWS Control Tower adds 65 new controls', 'summary': 'With this launch, we’ve added a set of purpose-built controls to help you meet your digital sovereignty requirements.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='AWS Control Tower helps reduce the time it takes to define, implement, and manage controls required to govern where your data is stored, transferred, and processed at scale. AWS Control Tower offers you a consolidated view of the controls enabled, your compliance status, and controls evidence across your multiple accounts. This information is available on the console and by calling our APIs. As requirements and AWS services evolve, AWS Control Tower provides you with updated controls to help you continually manage your digital sovereignty needs. Here are a couple of examples of the controls we added: Operator access – Require that an Amazon Elastic Compute Cloud (Amazon EC2) dedicated host uses an AWS Nitro instance type. Controlling access to your data – Require that an Amazon Elastic Block Store (Amazon EBS) snapshot cannot be publicly restorable. Encryption at rest and in transit , including advanced key management strategies – Require an EC2 instance to use an AWS Nitro instance type that supports encryption in-transit between instances when created using the AWS::EC2::Instance resource type. It also requires that an Amazon Relational Database Service (Amazon RDS) database instance has encryption at rest configured to use an AWS KMS key that you specify for supported engine types. These are just four examples from three categories. We’ve added 65 new controls, with over 245+ controls available under the digital sovereignty category grouping. The full list is available in the AWS Control Tower documentation . One of the technical mechanisms AWS Control Tower uses to prevent accidental data storage or flow in a Region is the Region deny control . This parameter allows system administrators to deny access to AWS services and operations in selected AWS Regions. Until today, Region deny control could only be applied for an entire landing zone and all its organizational units (OUs) and accounts. With this launch, you can configure a new Region deny control at the organizational unit level and select the services and IAM principals to allow based on your unique business needs. Let’s see how to get started For this demo, let’s imagine that I want to restrict access to AWS services in a set of Regions. I open the AWS Management Console and navigate to the AWS Control Tower page . On the left navigation pane, under Control Library , I select Categories > Groups > Digital Sovereignty . I can review the list of controls available. I locate and select the control I want to enable: Deny access to AWS based on the requested AWS Region for an organizational unit .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='08ca37a2-fd8d-44d6-a105-a012ed7d9de3', embedding=None, metadata={'title': 'AWS Control Tower adds 65 new controls', 'summary': 'With this launch, we’ve added a set of purpose-built controls to help you meet your digital sovereignty requirements.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='There is a description of the control and a list of frameworks it applies to ( NIST 800 and PCI DSS ). I select Enable control . On the next page, I select the Organizational units (OU) for which I want to enable this control. I select the AWS Regions where I will allow access. All Regions left unchecked will have their access denied once the control is enforced. Then, I review the service control policy (SCP). It contains a Deny statement to prevent access to the services or APIs listed. Optionally, I can add NotActions . This is a list of exceptions. The services or APIs listed under NotActions are authorized. In this example, I deny everything excepted three APIs: sqs:SendMessage , ec2:StartInstances , and s3:GetObject . On the last page, I add a list of IAM principals (users or roles) that will be exempted from the control. This is an exception list. I also tag my control as usual with AWS resources. On the last screen (not shown here), I review all my parameters and select Enable control . I can verify the list of OU for which the control is enabled under the OUs enabled tab. The summary page shows all Regions, APIs, and IAM principals enabled for this OU. All the rest is denied. I can update the parameters at any time. Pricing and availability AWS Control Tower is available in all commercial Regions and in US GovCloud. There is no additional charge to use AWS Control Tower. However, when you set up AWS Control Tower, you will begin to incur costs for AWS services configured to set up your landing zone and mandatory controls. Certain AWS services, such as Organizations and AWS IAM Identity Center, come at no additional charge. However, you will pay for services such as AWS Service Catalog , AWS CloudTrail , AWS Config , Amazon CloudWatch , Amazon Simple Notification Service (Amazon SNS) , Amazon Simple Storage Service (Amazon S3) , and Amazon Virtual Private Cloud (Amazon VPC) based on your usage of these services. You only pay for what you use, as you use it. The AWS Control Tower pricing page has the details . The new AWS Control Tower controls alleviate the burden of identifying and deploying safeguards to meet your digital sovereignty requirements.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='3d4983a3-72b4-4380-a2f4-d22a0716fa5c', embedding=None, metadata={'title': 'AWS Control Tower adds 65 new controls', 'summary': 'With this launch, we’ve added a set of purpose-built controls to help you meet your digital sovereignty requirements.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='This set of controls is fully managed, and we will update them as AWS services and digital sovereignty requirements evolve over time. Go and configure the AWS Control Tower controls that help support your digital sovereignty requirements today. -- seb', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='dfee1c7d-238e-4599-af70-7e8b3a7df5e0', embedding=None, metadata={'title': 'Amazon Detective adds new capabilities to accelerate and improve your cloud security investigations', 'summary': 'Amazon Detective adds four new capabilities to help you save time and strengthen your security operations.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, Amazon Detective adds four new capabilities to help you save time and strengthen your security operations. First, Detective investigations for IAM help security analysts investigate AWS Identity and Access Management (IAM) objects, such as users and roles, for indicators of compromise (IoCs) to determine potential involvement in known tactics from the MITRE ATT&CK framework . These automatic investigations are available in the Detective section of the AWS Management Console and through a new API to automate your analysis or incident response or to send these findings to other systems, such as AWS Security Hub or your SIEM . Second, Detective finding group summaries uses generative artificial intelligence (AI) to enrich its investigations. It automatically analyzes finding groups and provides insights in natural language to accelerate security investigations. It provides a plain language title based on the analysis of the finding group with relevant summarized insights, such as describing the activity that initiated the event and its impact, if any. Finding group summaries handles the heavy lifting of analyzing the finding group built across multiple AWS data sources, making it easier and faster to investigate unusual or suspicious activity. In addition to these two new capabilities that I describe in this post, Detective adds another two capabilities not covered here: Detective now supports security investigations for threats detected by Amazon GuardDuty ECS Runtime Monitoring . Detective now integrates with Amazon Security Lake , enabling security analysts to query and retrieve logs stored in Security Lake. Amazon Detective makes it easier to analyze, investigate, and quickly identify the root cause of security findings or suspicious activities. Detective uses machine learning (ML), statistical analysis, and graph theory to help you visualize and conduct faster and more efficient security investigations. Detective automatically collects logs data and events from sources like AWS CloudTrail logs, Amazon Virtual Private Cloud (Amazon VPC) Flow Logs , Amazon GuardDuty findings, Amazon Elastic Kubernetes Service (Amazon EKS) audit logs, and AWS security findings. Detective maintains up to a year of aggregated data for analysis and investigations. Cloud security professionals often find threat hunting and incident investigations to be resource-intensive and time-consuming. They must manually gather and analyze data from various sources to identify potential IAM-related threats. IAM investigations are particularly challenging due to dynamic cloud permissions and credentials. Analysts need to piece together data from different systems, including audit logs, entitlement reports, and CloudTrail events, which can be dispersed. Cloud permissions are often granted on-demand or through automation scripts, making authorization changes hard to track.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='205bc30c-a336-41b2-b2e4-2b3dfa8143a0', embedding=None, metadata={'title': 'Amazon Detective adds new capabilities to accelerate and improve your cloud security investigations', 'summary': 'Amazon Detective adds four new capabilities to help you save time and strengthen your security operations.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Reconstructing activity timelines and identifying irregular entitlements can take hours or days, depending on complexity. Limited visibility into legacy systems and incomplete logs further complicates IAM investigations, making it difficult to obtain a definitive understanding of unauthorized access. Detective investigations for IAM triage findings and surface only the most critical, suspicious issues, allowing security analysts to focus on high-level investigations. It automatically analyzes resources in your AWS environment to identify potential indicators of compromise or suspicious activity using machine learning and threat intelligence. This allows analysts to identify patterns and comprehend which resources are impacted by security events, offering a proactive approach to threat identification and mitigation. The investigations are not only available in the console; you can use the new StartInvestigation API to automate a remediation workflow or collect information about all IP involved or AWS resources compromised. You can also use the API to feed the data to other systems to build a consolidated view of your security posture. Finding group summaries evaluates the connections between security events across an environment and provides insights in natural language that link related threats, compromised resources, and malicious actor behavior. This narrative offers security analysts a comprehensive overview of security incidents that goes beyond individual service reports. By grouping and contextualizing data from multiple sources, finding group summaries identifies threats that might go unnoticed when insights are isolated. This approach improve the speed and efficiency of investigations and responses. Security analysts can utilize finding group summaries to gain a holistic understanding of security events and their interrelationships, helping them make informed decisions regarding containment and remediation. Let’s see these two capabilities in action In this demo, I start with Detective investigations for IAM in the Detective section of the console . The Detective dashboard shows me the number of investigations done and the number of IAM roles and users involved in suspicious activities. From there, I drill down the list of investigations. And I select one specific investigation to get the details. There is a summary first. I scroll down the page to see what IP addresses are involved and for what type of activities. This example shows me a physical impossibility: the same IP was used in a short time from two different places, Australia and Japan. The most interesting section of the page, in my opinion, is the mappings to tactics, techniques, and procedures (TTP). All TTPs are classified according to their severity. The console shows the techniques and actions used. When selecting a specific TTP, I can see the details in the right pane.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='20297ade-5fcc-422f-9c79-1bfdca464b67', embedding=None, metadata={'title': 'Amazon Detective adds new capabilities to accelerate and improve your cloud security investigations', 'summary': 'Amazon Detective adds four new capabilities to help you save time and strengthen your security operations.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='In this example, the suspicious IP address has been involved in more than 2,000 failed attempts to change the trusted policy of an IAM role. Finally, I navigate to the Indicators tab to see the list of indicators. On the other side, finding group summaries is available under Finding groups . I select a finding group to receive a natural language explanation of the findings and risks involved. Pricing and availability These two new capabilities are now available to all AWS customers. Detective investigations for IAM is available in all AWS Regions where Detective is available . Finding group summaries is available in five AWS Regions: US East (N. Virginia), US West (Oregon), Asia Pacific (Singapore, Tokyo), and Europe (Frankfurt). Learn all the details about Amazon Detective and get started today . -- seb', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='38add857-bb05-466d-a4a9-93ce2113db2a', embedding=None, metadata={'title': 'Detect runtime security threats in Amazon ECS and AWS Fargate, new in Amazon GuardDuty', 'summary': 'The new capability helps detect potential runtime security issues in Amazon Elastic Container Service (Amazon ECS) clusters running on both AWS Fargate and Amazon Elastic Compute Cloud (Amazon EC2).'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, we’re announcing Amazon GuardDuty ECS Runtime Monitoring to help detect potential runtime security issues in Amazon Elastic Container Service (Amazon ECS) clusters running on both AWS Fargate and Amazon Elastic Compute Cloud (Amazon EC2) . GuardDuty combines machine learning (ML), anomaly detection, network monitoring, and malicious file discovery against various AWS data sources. When threats are detected, GuardDuty generates security findings and automatically sends them to AWS Security Hub , Amazon EventBridge , and Amazon Detective . These integrations help centralize monitoring for AWS and partner services, initiate automated responses, and launch security investigations. GuardDuty ECS Runtime Monitoring helps detect runtime events such as file access, process execution, and network connections that might indicate runtime threats. It checks hundreds of threat vectors and indicators and can produce over 30 different finding types. For example, it can detect attempts of privilege escalation, activity generated by crypto miners or malware, or activity suggesting reconnaissance by an attacker. This is in addition to GuardDuty‘s primary detection categories . GuardDuty ECS Runtime Monitoring uses a managed and lightweight security agent that adds visibility into individual container runtime behaviors. When using AWS Fargate, there is no need for you to install, configure, manage, or update the agent. We take care of that for you. This simplifies the management of your clusters and reduces the risk of leaving some tasks without monitoring. It also helps to improve your security posture and pass regulatory compliance and certification for runtime threats. GuardDuty ECS Runtime Monitoring findings are visible directly in the console. You can configure GuardDuty to also send its findings to multiple AWS services or to third-party monitoring systems connected to your security operations center (SOC) . With this launch, Amazon Detective now receives security findings from GuardDuty ECS Runtime Monitoring and includes them in its collection of data for analysis and investigations. Detective helps to analyze, investigate, and quickly identify the root cause of potential security issues or suspicious activities. It collects log data from AWS resources and uses machine learning, statistical analysis, and graph theory to build a linked set of data that enables you to easily conduct security investigations. Configure GuardDuty ECS Runtime Monitoring on AWS Fargate For this demo, I choose to show the experience provided for AWS Fargate. When using Amazon ECS, you must ensure your EC2 instances have the GuardDuty agent installed.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='78301546-efe2-4403-a439-58239220472b', embedding=None, metadata={'title': 'Detect runtime security threats in Amazon ECS and AWS Fargate, new in Amazon GuardDuty', 'summary': 'The new capability helps detect potential runtime security issues in Amazon Elastic Container Service (Amazon ECS) clusters running on both AWS Fargate and Amazon Elastic Compute Cloud (Amazon EC2).'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='You can install the agent manually, bake it into your AMI, or use GuardDuty‘s provided AWS Systems Manager document to install it (go to Systems Manager in the console, select Documents, and then search for GuardDuty). The documentation has more details about installing the agent on EC2 instances . When operating from a GuardDuty administrator account, I can enable GuardDuty ECS Runtime Monitoring at the organization level to monitor all ECS clusters in all organizations’ AWS accounts. In this demo, I use the AWS Management Console to enable Runtime Monitoring . Enabling GuardDuty ECS Runtime Monitoring in the console has an effect on all your clusters. When I want GuardDuty to automatically deploy the GuardDuty ECS Runtime Monitoring agent on Fargate, I enable GuardDuty agent management . To exclude individual clusters from automatic management, I can tag them with GuardDutyManaged=false . I make sure I tag my clusters before enabling ECS Runtime Monitoring in the console. When I don’t want to use the automatic management option, I can leave the option disabled and selectively choose the clusters to monitor with the tag GuardDutyManaged=true . The Amazon ECS or AWS Fargate cluster administrator must have authorization to manage tags on the clusters. The IAM TaskExecutionRole you attach to tasks must have permissions to download the GuardDuty agent from a private ECR repository. This is done automatically when you use the AmazonECSTaskExecutionRolePolicy managed IAM policy . Here is my view of the console when the Runtime Monitoring and agent management are enabled. I can track the deployment of the security agent by assessing the Coverage statistics across all the ECS clusters. Once monitoring is enabled, there is nothing else to do. Let’s see what findings it detects on my simple demo cluster. Check out GuardDuty ECS runtime security findings When GuardDuty ECS Runtime Monitoring detects potential threats, they appear in a list like this one. I select a specific finding to view more details about it. Things to know By default, a Fargate task is immutable. GuardDuty won’t deploy the agent to monitor containers on existing tasks. If you want to monitor containers for already running tasks, you must stop and start the tasks after enabling GuardDuty ECS Runtime Monitoring. Similarly, when using Amazon ECS services , you must force a new deployment to ensure tasks are restarted with the agent.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='a6298fcb-c99f-4687-9925-e81a6d802a91', embedding=None, metadata={'title': 'Detect runtime security threats in Amazon ECS and AWS Fargate, new in Amazon GuardDuty', 'summary': 'The new capability helps detect potential runtime security issues in Amazon Elastic Container Service (Amazon ECS) clusters running on both AWS Fargate and Amazon Elastic Compute Cloud (Amazon EC2).'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='As I mentioned already, be sure the tasks have IAM permissions to download the GuardDuty monitoring agent from Amazon ECR. We designed the GuardDuty agent to have little impact on performance, but you should plan for it in your Fargate task sizing calculations . When you choose automatic agent management, GuardDuty also creates a VPC endpoint to allow the agent to communicate with GuardDuty APIs. When—just like me—you create your cluster with a CDK or CloudFormation script with the intention to delete the cluster after a period of time (for example, in a continuous integration scenario), bear in mind that the VPC endpoint must be deleted manually to allow CloudFormation to delete your stack. Pricing and availability You can now use GuardDuty ECS Runtime Monitoring on AWS Fargate and Amazon EC2 instances. For a full list of Regions where GuardDuty ECS Runtime Monitoring is available, visit our Region-specific feature availability page. You can try GuardDuty ECS Runtime Monitoring for free for 30 days. When you enable GuardDuty for the first time, you have to explicitly enable GuardDuty ECS Runtime Monitoring. At the end of the trial period, we charge you per vCPU per hour of the monitoring agents. The GuardDuty pricing page has all the details. Get insights about the threats to your container and enable GuardDuty ECS Runtime Monitoring today . -- seb', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='c63940db-79ea-4aa3-a508-970f2f1bcc58', embedding=None, metadata={'title': 'Vector engine for Amazon OpenSearch Serverless is now available', 'summary': 'The vector engine makes it easy for you to build modern machine learning-augmented search experiences and generative generative AI applications without needing to manage the underlying vector database infrastructure.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today we are announcing the general availability of the vector engine for Amazon OpenSearch Serverless with new features. In July 2023, we introduced the preview release of the vector engine for Amazon OpenSearch Serverless, a simple, scalable, and high-performing similarity search capability. The vector engine makes it easy for you to build modern machine learning (ML) augmented search experiences and generative artificial intelligence (generative AI) applications without needing to manage the underlying vector database infrastructure. You can now store, update, and search billions of vector embeddings with thousands of dimensions in milliseconds. The highly performant similarity search capability of vector engine enables generative AI-powered applications to deliver accurate and reliable results with consistent milliseconds-scale response times. The vector engine also enables you to optimize and tune results with hybrid search by combining vector search and full-text search in the same query, removing the need to manage and maintain separate data stores or a complex application stack. The vector engine provides a secure, reliable, scalable, and enterprise-ready platform to cost eﬀectively build a prototyping application and then seamlessly scale to production. You can now get started in minutes with the vector engine by creating a specialized vector engine–based collection, which is a logical grouping of embeddings that works together to support a workload. The vector engine uses OpenSearch Compute Units (OCUs), compute capacity unit, to ingest and run similarity search queries. One OCU can handle up to 2 million vectors for 128 dimensions or 500,000 for 768 dimensions at 99 percent recall rate. The vector engine built on OpenSearch Serverless is a highly available service by default. It requires a minimum of four OCUs (2 OCUs for the ingest, including primary and standby, and 2 OCUs for the search with two active replicas across Availability Zones) for the first collection in an account. All subsequent collections using the same AWS Key Management Service (AWS KMS) key can share those OCUs. What’s new at GA? Since the preview, the vector engine for Amazon OpenSearch Serverless became one of the vector database options in the knowledge base of Amazon Bedrock to build generative AI applications using a Retrieval Augmented Generation (RAG) concept.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e71384b8-8c68-4b23-a546-5e9611d5a340', embedding=None, metadata={'title': 'Vector engine for Amazon OpenSearch Serverless is now available', 'summary': 'The vector engine makes it easy for you to build modern machine learning-augmented search experiences and generative generative AI applications without needing to manage the underlying vector database infrastructure.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Here are some new or improved features for this GA release: Disable redundant replica (development and test focused) option As we announced in our preview blog post , this feature eliminates the need to have redundant OCUs in another Availability Zone solely for availability purposes. A collection can be deployed with two OCUs – one for indexing and one for search. This cuts the costs in half compared to default deployment with redundant replicas. The reduced cost makes this configuration suitable and economical for development and testing workloads. With this option, we will still provide durability guarantees since the vector engine persists all the data in Amazon S3 , but single-AZ failures would impact your availability. If you want to disable a redundant replica, uncheck Enable redundancy when creating a new vector search collection. Fractional OCU for the development and test focused option Support for fractional OCU billing for development and test focused workloads (that is, no redundant replica option) reduces the floor price for vector search collection. The vector engine will initially deploy smaller 0.5 OCUs while providing the same capabilities at lower scale and will scale up to a full OCU and beyond to meet your workload demand. This option will further reduce the monthly costs when experimenting with using the vector engine. Automatic scaling for a billion scale With vector engine’s seamless auto-scaling, you no longer have to reindex for scaling purposes. At preview, we were supporting about 20 million vector embeddings. With the general availability of vector engine, we have raised the limits to support a billion vector scale. Now available The vector engine for Amazon OpenSearch Serverless is now available in all AWS Regions where Amazon OpenSearch Serverless is available. To get started, you can refer to the following resources: Introducing the vector engine for Amazon OpenSearch Serverless, now in preview Try semantic search with the Amazon OpenSearch Service vector engine Amazon OpenSearch Service’s vector database capabilities explained Using OpenSearch as a vector database Getting started with Amazon OpenSearch Serverless documentation Demo video: Amazon OpenSearch Service for vector search Demo video: Empowering search: OpenSearch and bulk vector search Give it a try and send feedback to AWS re:Post for Amazon OpenSearch Service or through your usual AWS support contacts. — Channy', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='84048959-669f-4a27-820c-1e2e501dbcd9', embedding=None, metadata={'title': 'AWS Lambda functions now scale 12 times faster when handling high-volume requests', 'summary': 'Each synchronously invoked Lambda function now scales by 1,000 concurrent executions every 10 seconds until the aggregate concurrency across all functions reaches the account’s concurrency limit.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Now AWS Lambda scales up to 12 times faster. Each synchronously invoked Lambda function now scales by 1,000 concurrent executions every 10 seconds until the aggregate concurrency across all functions reaches the account’s concurrency limit. In addition, each function within an account now scales independently from each other, no matter how the functions are invoked . These improvements come at no additional cost, and you don’t need to do any configuration in your existing functions. Building scalable and high-performing applications can be challenging with traditional architectures, often requiring over-provisioning of compute resources or complex caching solutions for peak demands and unpredictable traffic. Many developers choose Lambda because it scales on-demand when applications face unpredictable traffic. Before this update, Lambda functions could initially scale at the account level by 500–3,000 concurrent executions (depending on the Region) in the first minute, followed by 500 concurrent executions every minute until the account’s concurrency limit is reached. Because this scaling limit was shared between all the functions in the same account and Region, if one function experienced an influx of traffic, it could affect the throughput of other functions in the same account. This increased engineering efforts to monitor a few functions that could burst beyond the account limits, causing a noisy neighbor scenario and reducing the overall concurrency of other functions in the same account. Now, with these scaling improvements, customers with highly variable traffic can reach concurrency targets faster than before. For instance, a news site publishing a breaking news story or an online store running a flash sale would experience a significant influx of visitors. Thanks to these improvements, they can now scale 12 times faster than before. In addition, customers that use services such as Amazon Athena and Amazon Redshift with scalar Lambda-based UDFs to perform data enrichment or data transformations will see benefits from these improvements. These services rely on batching data and passing it in chunks to Lambda, simultaneously invoking multiple parallel functions. The enhanced concurrency scaling behavior ensures Lambda can rapidly scale and service level agreement (SLA) requirements are met. How does this work in practice? The following graph shows a function receiving requests and processing them every 10 seconds. The account concurrency limit is set to 7,000 concurrent requests and is shared between all the functions in the same account. Each function scaling-up rate is fixed to 1,000 concurrent executions every 10 seconds.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='2fb19c91-5724-4256-8d30-a92937a42231', embedding=None, metadata={'title': 'AWS Lambda functions now scale 12 times faster when handling high-volume requests', 'summary': 'Each synchronously invoked Lambda function now scales by 1,000 concurrent executions every 10 seconds until the aggregate concurrency across all functions reaches the account’s concurrency limit.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='This rate is independent from other functions in the same account, making it easier for you to predict how this function will scale and throttle the requests if needed. 09:00:00 – The function has been running for a while, and there are already 1,000 concurrent executions that are being processed. 09:00:10 – Ten seconds later, there is a new burst of 1,000 new requests. This function can process them with no problem because the function can scale up to 1,000 concurrent executions every 10 seconds. 09:00:20 – The same happens here: a thousand new requests. 09:00:30 – The function now receives 1,500 new requests. Because the maximum scale-up capacity for a function is 1,000 requests per 10 seconds, 500 of those requests will get throttled. 09:01:00 – At this time, the function is already processing 4,500 concurrent requests. But there is a burst of 3,000 new requests. Lambda processes 1,000 of the new requests and throttles 2,000 because the function can scale up to 1,000 requests every 10 seconds. 09:01:10 – After 10 seconds, there is another burst of 2,000 requests, and the function can now process 1,000 more requests. However, the remaining 1,000 requests get throttled because the function can scale to 1,000 requests every 10 seconds. 09:01:20 – Now the function is processing 6,500 concurrent requests, and there are 1,000 incoming requests. The first 500 of those requests get processed, but the other 500 get throttled because the function reached the account concurrency limit of 7,000 requests. It’s important to remember that you can raise the account concurrency limit by creating a support ticket in the AWS Management Console . In the case of having more than one function in your account, the functions scale independently until the total account concurrency limit is reached. After that, all new invocations will be throttled. Availability These scaling improvements will be enabled by default for all functions. Starting on November 26 through mid-December, AWS is gradually rolling out these scaling improvements to all AWS Regions except China and GovCloud Regions. If you want to learn more about Lambda’s new scaling behavior, read the Lambda scaling behavior documentation page .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='1bf174fc-a1d9-48e4-97eb-1cb7b752d4b0', embedding=None, metadata={'title': 'AWS Lambda functions now scale 12 times faster when handling high-volume requests', 'summary': 'Each synchronously invoked Lambda function now scales by 1,000 concurrent executions every 10 seconds until the aggregate concurrency across all functions reaches the account’s concurrency limit.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='– Marcia', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='328396ef-17a8-4d27-90a6-cc13cecefa94', embedding=None, metadata={'title': 'Announcing the new Amazon S3 Express One Zone high performance storage class', 'summary': 'The new Amazon S3 Express One Zone storage class is designed to deliver up to 10x better performance than the S3 Standard storage class and is a great fit for your most frequently accessed data and your most demanding applications.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Update (January 24, 2024) – The original version of this post used the s3 sync command to copy objects from my local directory to the storage bucket. The newest version of this command does not support copying to directory buckets, so I have revised the example to use s3 cp instead. — Jeff; The new Amazon S3 Express One Zone storage class is designed to deliver up to 10x better performance than the S3 Standard storage class while handling hundreds of thousands of requests per second with consistent single-digit millisecond latency, making it a great fit for your most frequently accessed data and your most demanding applications. Objects are stored and replicated on purpose built hardware within a single AWS Availability Zone, allowing you to co-locate storage and compute (Amazon EC2, Amazon ECS, and Amazon EKS) resources to further reduce latency. Amazon S3 Express One Zone With very low latency between compute and storage, the Amazon S3 Express One Zone storage class can help to deliver a significant reduction in runtime for data-intensive applications, especially those that use hundreds or thousands of parallel compute nodes to process large amounts of data for AI/ML training, financial modeling, media processing, real-time ad placement, high performance computing, and so forth. These applications typically keep the data around for a relatively short period of time, but access it very frequently during that time. This new storage class can handle objects of any size, but is especially awesome for smaller objects. This is because for smaller objects the time to first byte is very close to the time for last byte. In all storage systems, larger objects take longer to stream because there is more data to download during the transfer, and therefore the storage latency has less impact on the total time to read the object. As a result, smaller objects receive an outsized benefit from lower storage latency compared to large objects. Because of S3 Express One Zone’s consistent very low latency, small objects can be read up to 10x faster compared to S3 Standard. The extremely low latency provided by Amazon S3 Express One Zone, combined with request costs that are 50% lower than for the S3 Standard storage class, means that your Spot and On-Demand compute resources are used more efficiently and can be shut down earlier, leading to an overall reduction in processing costs.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='af951bf4-1922-49df-b12e-03291751d1f2', embedding=None, metadata={'title': 'Announcing the new Amazon S3 Express One Zone high performance storage class', 'summary': 'The new Amazon S3 Express One Zone storage class is designed to deliver up to 10x better performance than the S3 Standard storage class and is a great fit for your most frequently accessed data and your most demanding applications.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Each Amazon S3 Express One Zone directory bucket exists in a single Availability Zone that you choose, and can be accessed using the usual set of S3 API functions: CreateBucket , PutObject , GetObject , ListObjectsV2 , and so forth. The buckets also support a carefully chosen set of S3 features including byte-range fetches , multi-part upload , multi-part copy , presigned URLs , and Access Analyzer for S3 . You can upload objects directly, write code that uses CopyObject , or use S3 Batch Operations , In order to reduce latency and to make this storage class as efficient & scalable as possible, we are introducing a new bucket type, a new authentication model, and a bucket naming convention: New bucket type – The new directory buckets are specific to this storage class, and support hundreds of thousands of requests per second. They have a hierarchical namespace and store object key names in a directory-like manner. The path delimiter must be “ / “, and any prefixes that you supply to ListObjectsV2 must end with a delimiter. Also, list operations return results without first sorting them, so you cannot do a “start after” retrieval. New authentication model – The new CreateSession function returns a session token that grants access to a specific bucket for five minutes. You must include this token in the requests that you make to other S3 API functions that operate on the bucket or the objects in it, with the exception of CopyObject , which requires IAM credentials. The newest versions of the AWS SDKs handle session creation automatically. Bucket naming – Directory bucket names must be unique within their AWS Region, and must specify an Availability Zone ID in a specially formed suffix. If my base bucket name is jbarr and it exists in Availability Zone use1-az5 (Availability Zone 5 in the US East (N. Virginia) Region) the name that I supply to CreateBucket would be jbarr--use1-az5--x-s3 . Although the bucket exists within a specific Availability Zone, it is accessible from the other zones in the region, and there are no data transfer charges for requests from compute resources in one Availability Zone to directory buckets in another one in the same region. Amazon S3 Express One Zone in action Let’s put this new storage class to use. I will focus on the command line, but AWS Management Console and API access are also available. My EC2 instance is running in my us-east-1f Availability Zone.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='1ebaebad-3cbf-4bee-97d2-7b510585293e', embedding=None, metadata={'title': 'Announcing the new Amazon S3 Express One Zone high performance storage class', 'summary': 'The new Amazon S3 Express One Zone storage class is designed to deliver up to 10x better performance than the S3 Standard storage class and is a great fit for your most frequently accessed data and your most demanding applications.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='I use jq to map this value to an Availability Zone Id: $ aws ec2 describe-availability-zones --output json | \\\\\\n  jq -r  \\'.AvailabilityZones[] | select(.ZoneName == \"us-east-1f\") | .ZoneId\\'\\nuse1-az5 I create a bucket configuration ( s3express-bucket-config.json ) and include the Id: {\\n        \"Location\" :\\n        {\\n                \"Type\" : \"AvailabilityZone\",\\n                \"Name\" : \"use1-az5\"\\n        },\\n        \"Bucket\":\\n        {\\n                \"DataRedundancy\" : \"SingleAvailabilityZone\",\\n                \"Type\"           : \"Directory\"\\n        }\\n} After installing the newest version of the AWS Command Line Interface (AWS CLI) , I create my directory bucket: $ aws s3api create-bucket --bucket jbarr--use1-az5--x-s3 \\\\\\n  --create-bucket-configuration file://s3express-bucket-config.json \\\\\\n  --region us-east-1\\n-------------------------------------------------------------------------------------------\\n|                                       CreateBucket                                      |\\n+----------+------------------------------------------------------------------------------+\\n|  Location|  https://jbarr--use1-az5--x-s3.s3express-use1-az5.us-east-1.amazonaws.com/   |\\n+----------+------------------------------------------------------------------------------+ Then I can use the directory bucket as the destination for other CLI commands as usual (the second aws is the directory where I unzipped the AWS CLI): $ aws s3 cp aws s3://jbarr--use1-az5--x-s3 --recursive When I list the directory bucket’s contents, I see that the StorageClass is EXPRESS_ONEZONE : $ aws s3api list-objects-v2 --bucket jbarr--use1-az5--x-s3 --output json | \\\\\\n  jq -r \\'.Contents[] | {Key: .Key, StorageClass: .StorageClass}\\'\\n...\\n{\\n  \"Key\": \"install\",\\n  \"StorageClass\": \"EXPRESS_ONEZONE\"\\n}\\n... The Management Console for S3 shows General purpose buckets and Directory buckets on separate tabs: I can import the contents of an existing bucket (or a prefixed subset of the contents) into a directory bucket using the Import button, as seen above. I select a source bucket, click Import, and enter the parameters that will be used to generate an inventory of the source bucket and to create and an S3 Batch Operations job.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='9b58f0ab-06a9-4177-9714-f58292cc0a4c', embedding=None, metadata={'title': 'Announcing the new Amazon S3 Express One Zone high performance storage class', 'summary': 'The new Amazon S3 Express One Zone storage class is designed to deliver up to 10x better performance than the S3 Standard storage class and is a great fit for your most frequently accessed data and your most demanding applications.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='The job is created and begins to execute: Things to know Here are some important things to know about this new S3 storage class: Regions – Amazon S3 Express One Zone is available in the US East (N. Virginia), US West (Oregon), Asia Pacific (Tokyo), and Europe (Stockholm) Regions, with plans to expand to others over time. Other AWS Services – You can use Amazon S3 Express One Zone with other AWS services including Amazon SageMaker Model Training , Amazon Athena , Amazon EMR , and AWS Glue Data Catalog to accelerate your machine learning and analytics workloads. You can also use Mountpoint for Amazon S3 to process your S3 objects in file-oriented fashion. Pricing – Pricing, like the other S3 storage classes, is on a pay-as-you-go basis. You pay $0.16/GB/month in the US East (N. Virginia) Region, with a one-hour minimum billing time for each object, and additional charges for certain request types. You pay an additional per-GB fee for the portion of any request that exceeds 512 KB. For more information, see the Amazon S3 Pricing page. Durability – In the unlikely case of the loss or damage to all or part of an AWS Availability Zone, data in a One Zone storage class may be lost. For example, events like fire and water damage could result in data loss. Apart from these types of events, our One Zone storage classes use similar engineering designs as our Regional storage classes to protect objects from independent disk, host, and rack-level failures, and each are designed to deliver 99.999999999% data durability. SLA – Amazon S3 Express One Zone is designed to deliver 99.95% availability with an availability SLA of 99.9%; for information see the Amazon S3 Service Level Agreement page. This new storage class is available now and you can start using it today! Learn more Amazon S3 Express One Zone — Jeff ;', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='9fe5a72e-f259-40f0-b380-c5708e43a777', embedding=None, metadata={'title': 'Amazon EBS Snapshots Archive is now available with AWS Backup', 'summary': 'This feature lets you transition your infrequently accessed Amazon EBS Snapshots to low-cost archive, long-term storage.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today we announce the availability of Amazon Elastic Block Store (Amazon EBS) Snapshots Archive with AWS Backup . Previously available only in the Amazon EC2 console or Amazon Data Lifecycle Manager , this feature gives you the ability to transition your infrequently accessed Amazon EBS Snapshots to low-cost archive, long-term storage of your rarely-accessed snapshots that do not need frequent or fast retrieval. Amazon EBS Snapshots Archive in the AWS Backup console Snapshots Archive with AWS Backup is only available for snapshots with a backup frequency of one month or longer (28-day cron expression) and a retention of more than 90 days. This is a protective measure to ensure that you don’t archive snapshots, such as hourly snapshots that wouldn’t benefit from the transition to the cold tier. The ability to archive Amazon EBS Snapshots is a new parameter of the Lifecycle section of the AWS Backup Plans. You must explicitly opt into moving your Amazon EBS Snapshots to cold storage, because this has different properties of our existing cold storage including: Always converting an incremental backup to a full backup. Longer recovery time objective (RTO) (up to 72 hours). Limitations on the frequency of backups that can be transitioned to cold storage (monthly or greater). Time in warm storage indicates how long the backups will remain in warm storage before they are transitioned to cold storage. Total retention period is the total time the backups will be retained by AWS Backup, and its value is the sum of both warm and cold storage. For backups in cold storage, the minimum retention period is 90 days. This is why the default total retention is 98 days (8 days in warm + 90 days in cold). The bar graph shows the total retention of your backups and where the backups will reside during that time. In the example shown in this graph, 8 days is in warm storage (red bar), and 90 days is in cold storage (blue bar). To restore or use the archived Amazon EBS snapshot today (outside of AWS Backup), you have to follow a two-step process: Temporarily or permanently restore the snapshot from archive to standard tier. Once it’s in standard tier, call the CreateVolume API from the standard tier.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ff1c5978-a706-4393-9008-e06e00b0d921', embedding=None, metadata={'title': 'Amazon EBS Snapshots Archive is now available with AWS Backup', 'summary': 'This feature lets you transition your infrequently accessed Amazon EBS Snapshots to low-cost archive, long-term storage.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='With this announcement, using either the AWS Backup console or the API to restore the archived Amazon EBS snapshot in AWS Backup, the following restore workflow applies: Enter the number of days you want to temporarily restore your snapshot from cold to standard tier. Choose your volume configuration. The end result will be a restored EBS volume. You will not have to manually move the snapshot from cold to standard tier, then restore the volume, this will be done automatically for you. Now available Amazon EBS Snapshots Archive with AWS Backup is available for you today in all AWS Regions except China and AWS GovCloud (US). As usual, you pay as you go, with no minimum or fixed fees. There are two metrics that influence Amazon EBS Snapshots Archive billing: data storage and data retrieval. You are charged for a 90-day period at minimum. This means that if you delete a snapshot archive or permanently restore it less than 90 days after creation, then we charge for the full 90-day period. The AWS Backup pricing page has the details . – Veliswa', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='549a762f-67ac-40ef-bd25-caaab0ac9ae1', embedding=None, metadata={'title': 'Replication failback and increased IOPS are new for Amazon EFS', 'summary': 'Replication failback makes it easier to synchronize when performing disaster recovery, and Amazon EFS now supports up to 250,000 read IOPS and up to 50,000 write IOPS per file system.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, Amazon Elastic File System (Amazon EFS) has introduced two new capabilities: Replication failback – Failback support for EFS replication makes it easier and more cost-effective to synchronize changes between EFS file systems when performing disaster recovery (DR) workflows. You can now quickly replicate incremental changes from your secondary back to your primary file system after disaster events and other DR-related activities. Increased IOPS – Amazon EFS now supports up to 250,000 read IOPS and up to 50,000 write IOPS per file system, making it easier to run more IOPS-heavy workloads at any scale for virtual servers, containers, and serverless functions that require shared storage. Let’s see more in depth how these work in practice. Introducing Amazon EFS replication failback With Amazon EFS replication, you can create a replica of your file system in the same or in another AWS Region . When replication is enabled, Amazon EFS automatically keeps the primary (source) and secondary (destination) file systems synchronized. To help you meet your compliance and business continuity goals, EFS replication is designed to provide a recovery point objective (RPO) and a recovery time objective (RTO) measured in minutes. Now, with failback support, you can respond to disaster recovery (DR) events, conduct planned business continuity tests, and manage other DR-related activities with greater speed and cost efficiency. Failback support allows you to switch the direction of replication between the primary and secondary file systems. EFS replication keeps the two file systems in sync by copying only incremental changes, eliminating the need to make full copies of your data or use a self-managed, custom solution to complete a recovery workflow. Using Amazon EFS replication failback I have a file system replicated to another Region. As part of a periodic DR test, I want to switch to using the secondary file system and then revert back to the primary file system, preserving all the changes made on the secondary file system. To do so, I can use EFS Replication failback in just a few steps. First, I delete the replication from the primary (source) to the secondary (destination) file system. After this, the secondary file system becomes writable. To do so, in the Amazon EFS console , I check I am in the correct Region and select the secondary file system. In the Replication tab, I choose Delete replication and confirm deletion.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='6484118f-9919-45fb-b4b0-cf801414e294', embedding=None, metadata={'title': 'Replication failback and increased IOPS are new for Amazon EFS', 'summary': 'Replication failback makes it easier to synchronize when performing disaster recovery, and Amazon EFS now supports up to 250,000 read IOPS and up to 50,000 write IOPS per file system.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='I can also start from the primary file system. In that case,\\xa0the Delete replication link in the Replication tab opens a new browser tab and asks to confirm deletion like before. I can now use the secondary file system and change its data as needed. To go back to using the primary file system, I create a “reverse replication” from the secondary to the primary file system. To do so, I check I am in the correct Region and select the secondary file system. In the Replication tab, I choose Create replication and the new option Replicate to existing file system . Then, I select the Region of the primary file system and use the console to browse the EFS file systems in that Region and choose the primary one. The console warns me that Replication overwrite protection is enabled for the primary file system. I follow the Disable protection link to open a new browser tab and edit the primary file system to disable replication overwrite protection. Now, I go back to the browser tab where I am creating the failback replication from the secondary to the primary file system. I refresh the protection check and choose to create the replication. In the following dialog, I confirm that I want Amazon EFS to write to the primary file system. To know when the primary file system is back in sync, I check the Last synced timestamp in the Replication tab, which indicates that all changes made to the source file system before that time are replicated to the destination. Optionally, I can look at the TimeSinceLastSync metric (expressed in minutes) in Amazon CloudWatch to understand how data is being replicated. When the primary file system is back in sync, I delete the replication from the secondary to the primary file system. To complete the restore of the original configuration, I again create the replication from the primary to the secondary file system. Increased IOPS per file system The Amazon EFS team has been able to increase IOPS again! The last time they did it was just a few months back . Starting today, an EFS file system can handle up to 50,000 write IOPS (a 2x improvement) and up to 250,000 read IOPS (a 4.5x improvement) when working with frequently-accessed data from a high-performance cache managed by Amazon EFS. You can monitor the percentage utilization of your file system’s available IOPS using the PercentIOLimit CloudWatch metric.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='866ba544-444f-4ff0-96a1-651517dc4219', embedding=None, metadata={'title': 'Replication failback and increased IOPS are new for Amazon EFS', 'summary': 'Replication failback makes it easier to synchronize when performing disaster recovery, and Amazon EFS now supports up to 250,000 read IOPS and up to 50,000 write IOPS per file system.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='This metric considers the maximum IOPS for writes and uncached reads, including combinations of the two. Reads from the cache are not included in the PercentIOLimit metric. With these performance improvements, you can run even more IOPS-demanding workloads on Amazon EFS, such as machine learning (ML) training, fine-tuning, and inference. Other use cases that can benefit from the increased IOPS are data science user shares, SaaS applications, and media processing. Things to know EFS replication failback is available in all AWS Regions where EFS is available. There are no additional costs for using replication failback. You pay for the usual replication and file system changes as described in Amazon EFS pricing . The increased IOPS limits are immediately available for all file systems\\xa0using the Elastic Throughput mode in all Regions where EFS is available. You don’t need to do anything to benefit from these performance improvements. To achieve the maximum IOPS, your application needs sufficient parallelization. For example, using multiple clients and distributing the load across a large number of files. For more information, see the performance tips in the user guide . Learn more Amazon EFS product page — Danilo', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='83f30958-7e78-4fc4-94b0-e0a2b743ace6', embedding=None, metadata={'title': 'Automatic restore testing and validation now available in AWS Backup', 'summary': 'With this feature, you can automate the entire restore testing process and avoid surprises later by determining now whether you can successfully recover using your backups in the event of a data loss such as ransomware.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Performing automatic game day testing of all your critical resources is an important step in determining that you are prepared to respond to ransomware or any data loss event. This gives you the opportunity to take appropriate corrective actions based on the results and monitor results such as success or failure from these tests. Ultimately, you will be able to ascertain if the restore times meet your expected organization’s recovery time objective (RTO) goals, helping you develop improved recovery strategies. Today, we’re announcing restore testing, a new capability in AWS Backup that allows you to perform restore testing of your AWS resources across storage, compute, and databases. With this feature, you can automate the entire restore testing process and avoid surprises later by determining now whether you can successfully recover using your backups in the event of a data loss such as ransomware. As an additional option, to demonstrate compliance with your organizational and regulatory data governance requirements, you can use the restore job results. How it works Restore testing in AWS Backup supports restore testing of resources for which the recovery points are created by AWS Backup, and the following services are supported: Amazon Elastic Block Store (Amazon EBS) , Amazon Elastic Compute Cloud (Amazon EC2) , Amazon Aurora , Amazon Relational Database Service (Amazon RDS) , Amazon Elastic File Store (Amazon EFS) , Amazon Simple Storage Service (Amazon S3) , Amazon DynamoDB , Amazon FSx , Amazon DocumentDB , and Amazon Neptune . You can get started with restore testing from the AWS Backup console, AWS CLI, or AWS SDK. Earlier, I created EC2 instances and a backup of these instances. Then, I created my restore testing plan in the AWS Backup console. In this General section, I enter the name of the plan, a test frequency, a Start time, and a Start within. Start time sets the time for the test to begin, for example, if you have a daily test frequency set, you specify what time the plan will run each day. Start within is the period of time in which the restore test is designated to begin. AWS Backup makes a best effort to commence all designated restore jobs during the Start within time window. You have a choice to keep this very minimal or very large based on your preference. In the Recovery point selection section, I specify the vaults that the recovery points should come from, and a timeframe of eligible recovery points as part of this restore testing plan.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='0595e9a4-13d6-41e9-b665-e8d452e1ac69', embedding=None, metadata={'title': 'Automatic restore testing and validation now available in AWS Backup', 'summary': 'With this feature, you can automate the entire restore testing process and avoid surprises later by determining now whether you can successfully recover using your backups in the event of a data loss such as ransomware.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='I left the criteria for a recovery point at the default selection. I also didn’t opt to include recovery points generated by point-in-time recovery (PITR) in this restore testing plan. Tagging is optional so for the purposes of this test I didn’t add a tag. I was then finished with setup, and it was time for me to choose Create restore testing plan to proceed with creating this restore testing plan. Once the restore testing plan has been created, it is time to assign resources. I start by specifying the IAM role that AWS Backup will assume when running the restore test. In terms of retention period before cleanup, I kept the default selection of deleting the restored resources immediately, to optimize costs. Alternatively, by specifying a retention period I could have also configured to integrate my own tests (for example, AWS Lambda) using Amazon EventBridge (CloudWatch Events) and send back validation status using the new PutRestoreValidationResult API so that it is reported in the restore job. I have EC2 instances that I created and backed up earlier, and I specify that this plan is for Amazon EC2 resource types. I include all protected resources of this EC2 resource type in the selection scope. I have very few resources, so I didn’t add the optional tags. I opted to use the default instance type for the restore. I also didn’t specify any additional parameters. It’s then time to choose Assign resources . Once the resources have been assigned, all information related to the restore testing plan will be presented in a summarized form where you’ll be able to see when the restore testing jobs have executed. Once I have enough restores performed over time, I can also view the Restore time history for every resource restored from the Protected resources tab. Now available Restore testing in AWS Backup is available in all AWS Regions where AWS Backup is available except AWS China Regions, AWS GovCloud (US), and Israel (Tel Aviv). To learn more, visit the AWS Backup user guide . You can submit your questions to AWS re:Post for AWS Backup or through your usual AWS Support contacts. — Veliswa', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='d717b9fc-014b-4398-b6d7-98cae3beb94b', embedding=None, metadata={'title': 'Optimize your storage costs for rarely-accessed files with Amazon EFS Archive', 'summary': 'We’ve added a new storage class for Amazon Elastic File System optimized for long-lived data that is rarely accessed.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, we are introducing EFS Archive , a new storage class for Amazon Elastic File System (Amazon EFS) optimized for long-lived data that is rarely accessed. With this launch, Amazon EFS supports three Regional storage classes: EFS Standard – Powered by SSD storage and designed to deliver submillisecond latency for active data. EFS Infrequent Access (EFS IA) – Cost-optimized for data accessed only a few times a quarter, and that doesn’t need the submillisecond latencies of EFS Standard. EFS Archive – Cost-optimized for long-lived data accessed a few times a year or less and offering similar performance to EFS IA. All Regional storage classes deliver gigabytes-per-second throughput and hundreds of thousands of IOPS performance and are designed for eleven nines of durability. You don’t need to manually pick and choose a storage class for your file systems because EFS lifecycle management can automatically migrate files across storage classes based on their access patterns. This allows you to have a single shared file system that contains files processed in very different ways: from active latency-sensitive to cold rarely-accessed data. Many datasets have subsets of data that are valuable for generating insights but aren’t often used. With EFS Archive, you can store rarely accessed data cost-effectively while keeping it in the same shared file system as other data. This simplified storage approach allows end users and applications to collaborate on large shared datasets in one place, making it easier and quicker to set up and scale analytics workloads. Using EFS Archive, you can optimize costs for workloads with large file-based datasets that contain a mix of active and inactive data such as user shares, machine learning (ML) training datasets, SaaS applications, and data retained for regulatory compliance like financial transactions and medical records. Let’s see how this works in practice. Using EFS Archive storage To use the new EFS Archive storage class, I need to configure lifecycle management for the file system. In the Amazon EFS console , I select one of my file systems and choose Edit . To use EFS Archive storage, the file system Throughput mode must be Elastic . Elastic Throughput is the recommended choice for most workloads because it is designed to provide applications with as much throughput as they need with pay-as-you-use pricing. Now, I configure Lifecycle management to transition files into EFS IA or EFS Archive based on my workload’s access patterns.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='8e6a1cf1-db9b-4065-b994-e8825b989539', embedding=None, metadata={'title': 'Optimize your storage costs for rarely-accessed files with Amazon EFS Archive', 'summary': 'We’ve added a new storage class for Amazon Elastic File System optimized for long-lived data that is rarely accessed.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='My workloads rarely use files older than one month. Files older than a quarter are not used by normal activities but need to be kept for a longer time. Based on these considerations, I select to automatically transition files to EFS IA after 30 days and to EFS Archive after 90 days since the last access. These are the default settings for new file systems. When one of my old files is accessed, it’s usually an indicator that is being used in a new analysis, so it’ll become active again for some period. For this reason, I use the option to transition files back to Standard storage on their first access in IA or Archive storage. I save changes, and that’s it! This file system will now automatically use different storage classes based on how files are being processed by my applications. Things to know EFS Archive is available today in all AWS Regions where Amazon EFS is offered, excluding those based in China. To offer a more cost-optimized experience for colder, rarely-accessed files, EFS Archive offers 50 percent lower storage cost than EFS IA with a three times higher request charge when data is accessed. For more information, see Amazon EFS pricing . You can use EFS Archive with existing file systems by configuring the file system lifecycle policies . New file systems are created by default with a lifecycle policy that automatically transitions files to EFS IA after 30 days and to EFS Archive after 90 days since the last access. Optimize your storage costs by configuring lifecycle management for your Amazon EFS file systems. — Danilo', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ab9bf323-7d15-4890-9794-73c83f648b5f', embedding=None, metadata={'title': 'Announcing on-demand data replication for Amazon FSx for OpenZFS', 'summary': 'Now you have the capability to send a snapshot from one file system to another file system in your account.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Update: December 19, 2023 – You can now send snapshots of your file systems to file systems in another AWS Region or AWS account. The text of this post has been updated to reflect the extra benefits unlocked by these two new capabilities. Today we’re adding to Amazon FSx for OpenZFS the capability to send a snapshot from a file system to another file system in your account including across accounts and/or AWS Regions. You can trigger the copy with one single API call or CLI command, and we take care of the rest. You don’t need to manage complex VPC peering configuration between AWS accounts or use commands like rsync and monitor the state of the transfer. The service takes care of the low-level infrastructure on your behalf. It manages potential network interruptions and retries automatically until the transfer completes. It transfers data incrementally at block level using OpenZFS’s native send and receive capabilities. This new capability helps you to maintain compliance by, for example, simplifying the preparation of disaster recovery plans across Regions, agility by, for example, allowing quicker and easier creation of testing and development environments, and performance improvements by simplifying the management of read replicas to provide scale-out performance and geographic data distribution. Amazon FSx for OpenZFS is a fully managed file storage service that lets you launch, run, and scale fully managed file systems built on the open source OpenZFS file system. FSx for OpenZFS makes it easy to migrate your on-premises ZFS file servers without changing your applications or how you manage data and to build new high-performance, data-intensive applications on the cloud. Snapshots are one of the most powerful features of ZFS file systems. A snapshot is a read-only copy of a file system or volume. Snapshots can be created almost instantly and initially consume no additional disk space within the storage pool. When a snapshot is created, its space is initially shared between the snapshot and the file system and possibly with previous snapshots. As the file system changes, space that was previously shared becomes unique to the snapshot. The snapshot consumes incremental disk space by continuing to reference the old data and so prevents the space from being freed. Snapshots can be rolled back on-demand and almost instantly, even on very large file systems. Snapshots can also be cloned to form new volumes. Snapshots are block-level copies.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='2a576095-cfdc-4c8f-b904-3e36476332a8', embedding=None, metadata={'title': 'Announcing on-demand data replication for Amazon FSx for OpenZFS', 'summary': 'Now you have the capability to send a snapshot from one file system to another file system in your account.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='They are more efficient to transfer than traditional file-level copies, where the system must sometimes traverse millions of files to detect the ones that changed. Transferring an incremental snapshot is also more efficient than transferring an incremental file-based copy because snapshots are incremental at block level. They only contain blocks modified since the last snapshot. On-demand replication of ZFS snapshots allows the transfer of terabytes of data using the native send and receive capability of OpenZFS without having to worry about the underlying infrastructure. We detect and manage network interruptions and other types of errors for you, making it easier for you to replicate data across file systems. There are four main use cases where you might want to use this new capability. Developers and quality assurance (QA) engineers might send on-demand snapshots to development and testing environments. It allows them to work with production data, ensuring accurate testing and development outcomes. The use of recent snapshots as consistent starting points for testing enhances the efficiency of the development and testing processes. Data engineers might use on-demand replication to run parallel experiments on a dataset. Imagine your application processes a large dataset. You want to run multiple versions of your data processing algorithm on the same base dataset to find the best tuning for your use case. With on-demand data replication, you can create multiple identical copies of your file system and run each experiment in parallel. System administrators might use on-demand replication of snapshots to transfer data to a hot standby in the context of disaster recovery. Restoring a ZFS file system that contains a replicated snapshot is faster than restoring a backup, because the file system is already live and populated with the replicated data. The possibility of quickly rolling back to a previous snapshot adds additional flexibility and security during the disaster recovery process. Technical professionals can utilize on-demand snapshot copies to address latency and data access challenges in multi-Region scenarios, for example, a mobile application serving users in various Regions. Assets such as images or configuration files must be accessed with low latency. By copying ZFS snapshots to remote file systems nightly, these assets become readily available in multiple Regions, ensuring swift user access to the necessary resources. Let’s see how it works To prepare this demo, I use the FSx for OpenZFS section of the AWS Management Console . First, I create two Amazon FSx for OpenZFS volumes.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='97e9f4c1-508e-41bc-99ca-740d97ec5d31', embedding=None, metadata={'title': 'Announcing on-demand data replication for Amazon FSx for OpenZFS', 'summary': 'Now you have the capability to send a snapshot from one file system to another file system in your account.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Then, I mount the two file systems on one Amazon Linux instance ( /zfs-filesystem1 and /zfs-filesystem2 ). I prepare a file on the first volume, and I expect to find the same file on the second volume after an on-demand replication. To synchronize data between my two volumes, I navigate to t he snapshot section of the console . Then I select Copy snapshot and update volume . I also have the option to copy the snapshot to a new ZFS volume. On the Copy snapshot and update volume page, I select the destination File system and Volume . I also confirm the source snapshot. I choose the Source snapshot copy strategy , either requesting a full copy or an incremental copy. When ready, I select Update . After a while—how long depends on the amount of data to transfer—I observe a new snapshot listed on the destination volume. In my demo scenario, it just takes a few seconds. I return to my Linux instance and list the content available in my second mount point /zfs-snapshot . I am happy to see my cow ASCII art on the second file system 🎉🐮. Alternatively, I can automate on-demand transfers using the new FSx APIs: CopySnapshotAndUpdateVolume and CopySnapshotAndCreateVolume . To set up an ongoing periodic replication, I use the provided CloudFormation template to create an automated replication schedule. When deployed, the system periodically takes a snapshot of the volume on the source file system and performs an incremental replication to a volume on the destination file system. For example, I could schedule replication to a development file system to happen once every 15 minutes for testing purposes. Pricing and availability This new capability is available in all AWS Regions where FSx for OpenZFS is available. It comes at no additional cost. AWS charges the usual fees for network data transfer between Availability Zones or Regions. You pay standard FSx for OpenZFS charges for the amount of storage used by the remote file system. The new on-demand replication for Amazon FSx for OpenZFS allows you to efficiently transfer incremental file system snapshots to a new volume on your account or a different account, in the same or a different AWS Region. It allows system administrators to configure disaster recovery scenarios and developers and QA engineers to work with copies of production data. It also gives low-latency access to data to your global customer base.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='cb499499-f765-45a2-8782-1fb2f8bffaad', embedding=None, metadata={'title': 'Announcing on-demand data replication for Amazon FSx for OpenZFS', 'summary': 'Now you have the capability to send a snapshot from one file system to another file system in your account.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Now go build and configure your first on-demand replication today ! -- seb', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='007600ca-2f1f-42e0-ace6-d619956184f1', embedding=None, metadata={'title': 'Introducing shared VPC support for Amazon FSx for NetApp ONTAP', 'summary': 'With this highly requested feature, you can now create Multi-AZ FSx for ONTAP file systems in VPCs that have been shared with you by other accounts in the same AWS Organization.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='You can now create Multi-AZ FSx for ONTAP file systems in VPCs that have been shared with you by other accounts in the same AWS Organization. This highly requested feature enables a clean separation of duties between network administrators and storage administrators, and makes it possible to create storage that’s durable, highly available, and accessible from multiple VPCs. Shared VPC support Before today’s launch, you had the ability to create Single-AZ FSx for ONTAP file systems in subnets that were shared with you by another AWS account, as well as both Single – and Multi-AZ file systems in subnets that you own. With today’s launch you can now do the same for file systems in multiple Availability Zones. Multi-AZ FSx for ONTAP file systems offer even higher availability than Single-AZ file systems, and are a great way to address and support large-scale enterprise storage needs. This new support for shared VPCs gives enterprises, many of which make use of multiple VPCs for technical and organizational reasons, to use FSx for ONTAP in Multi-AZ deployments, while allowing network administrators and storage administrators to work independently. This is easy to set up, but you do need to make sure that there are no IP address conflicts between subnets that are not shared between VPCs. I don’t have an AWS Organization set up, so I will hand-wave through part of this process. As a network administrator (the owner account), I use the AWS Resource Access Manager (RAM) to share the appropriate subnets of my VPC with the desired participant accounts in my Organization: Then I (or the administrators for those accounts) accept the resource shares. Next, I use the new FSx for ONTAP Settings to enable route table updates from participant accounts, and click Submit (this gives the FSx ONTAP service permission to modify route table entries in shared subnets on behalf of participant accounts): At this point, the storage administrators for the participant accounts can create Multi-AZ FSx for ONTAP file systems in the subnets that have been shared with them by the owner accounts. There is no additional charge for this feature and it is available in all AWS Regions where FSx for ONTAP is supported. — Jeff ;', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='55b553c1-2532-4905-b4ab-16219ab8554a', embedding=None, metadata={'title': 'New – Scale-out file systems for Amazon FSx for NetApp ONTAP', 'summary': 'You can now create Amazon FSx for NetApp ONTAP file systems that are up to 9x faster than even before.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='You can now create Amazon FSx for NetApp ONTAP file systems that are up to 9x faster than even before. As is already the case with this service, the file systems are fully managed, with latency to primary storage at the sub-millisecond level and latency to the capacity pool in the tens of milliseconds. This new level of performance will allow you to bring even more of your most demanding electronic design automation (EDA), visual effects (VFX), and statistical computing workloads (to name just a few) to the cloud. Existing FSx for ONTAP scale-up file systems are powered by a single pair of servers in an active/passive high availability (HA) configuration, and can support a maximum of 4 GBps of throughput and 192 TiB of SSD storage. The server pair can be deployed across distinct fault domains of a single Availability Zone, or in two separate Availability Zones to provide continuous availability even when an Availability Zone is unavailable. With today’s launch you can now create scale-out FSx for ONTAP file systems that are powered by two to six HA pairs. Here are the specs for the scale-up and scale-out file systems (these are all listed as “up to” since you can specify your desired values for each one when you create your file system): Deployment Type Read Throughput Write Throughput SSD IOPS SSD Storage Availability Zones Scale-up Up to 4 GBps Up to 1.8 GBps Up to 160K Up to 192 TiB Single or Multiple Scale-out Up to 36 GBps Up to 6.6 GBps Up to 1.2M Up to 1 PiB Single The amount of throughput that you specify for your file system will determine the actual server configuration as follows: Specified Throughput Deployment Type HA Pairs Throughput (Per Server) SSD Storage (Per Server) SSD IOPS (Per Server) 4 GBps or less Scale-up Single Up to 4 GBps Read Up to 1.1 GBps Write (Single-AZ) Up to 1.8 GBps Write (Multi-AZ) 1-192 TiB Up to 160K More than 4 GBps Scale-out Up to 6 Up to 6 GBps Read Up to 1.1 GBps Write 1-512 TiB Up to 200K To learn more about your choices, visit Amazon FSx for NetApp ONTAP performance .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='563f4c7a-2987-4445-b646-da5d451c0d1a', embedding=None, metadata={'title': 'New – Scale-out file systems for Amazon FSx for NetApp ONTAP', 'summary': 'You can now create Amazon FSx for NetApp ONTAP file systems that are up to 9x faster than even before.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Creating a Scale-Out File System I can create a scale-out file system using the AWS Management Console , AWS Command Line Interface (AWS CLI) , or by writing code that calls the Amazon FSx CreateFileSystem function. I will use the console, and start by choosing Amazon FSx for NetApp ONTAP : I choose Standard create , enter a name, select a Single-AZ deployment, and enter my desired SSD storage capacity. I can accepted the recommended throughput capacity, choose a value from the dropdown, or enter a value and see my options (more on that in a sec): The dropdown has a helpful magical feature. If I type my desired throughput capacity it will show me one or more options: The console will sometimes show me several options for the same amount of desired throughput capacity. Here are some guidelines to help you make a choice that will be a good fit for your workload: Low Throughput – If you choose an option that provides 4 GBps or less, you will be running on a single HA pair. This is the simplest option to choose if you don’t need a high degree of throughput. High Throughput and/or High Storage – Maximum throughput scales with the number of HA pairs that you provision. Also, choosing an option with more pairs will maximize your headroom for future growth in provisioned storage. I make selections and enter values for the remaining options as usual, and click Next to review my settings. I check to make sure that I have made good choices for any of the attributes that can’t be edited after creation, and click Create file system . I take a break to see what’s happening upstairs, feed my dog, and when I return my new 2 TB file system is ready to go: I can increase the storage capacity and I can change provisioned IOPS as frequently as every 6 hours: At the moment, the provisioned throughput capacity cannot be changed after creation if the file system uses more than one HA pair. Available Now Scale-out file systems are available in the US East (Ohio, N. Virginia), US West (Oregon), Asia Pacific (Sydney), and Europe (Ireland) Regions and you can start to create and use them today. Learn more Amazon FSx for NetApp ONTAP Amazon FSx for NetApp ONTAP performance — Jeff ;', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='810c4db5-26c2-41f3-9753-1f1a3362bffd', embedding=None, metadata={'title': 'FlexGroup Volume Management for Amazon FSx for NetApp ONTAP is now available', 'summary': 'You can now create, manage, and back up your Amazon FSx for NetApp ONTAP FlexGroup volumes using the AWS Management Console, the Amazon FSx CLI, and the AWS SDK.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='You can now create, manage, and back up your Amazon FSx for NetApp ONTAP FlexGroup volumes using the AWS Management Console, the Amazon FSx CLI, and the AWS SDK. FlexGroups can be as large as 20 petabytes and offer greater performance for demanding workloads. Before this launch you could only create them using the ONTAP CLI and the ONTAP REST API (these options remain available). Also new to this launch is the ability to create Amazon FSx backups of your FlexGroup volumes. FlexVol and FlexGroup FSx for ONTAP supports two volume styles: FlexVol – Support for up to 300 TiB of storage, making these volumes a good fit for general-purpose workloads. FlexGroup – Support for up to 20 PiB of storage and billions of files per volume, making these volumes a good fit for more demanding electronic design automation (EDA), seismic analysis, and software build/test workloads. Using FlexGroups I will use the AWS Management Console to create a new file system. I select Amazon FSx for NetApp ONTAP , and click Next : I select Standard create , enter a name for my file system ( FS-Jeff-1 ), and select Single-AZ as the deployment type: I can use the recommended throughput capacity, or I can specify it explicitly: As you can surmise from the values above, the throughput is determined by the number of high availability (HA) pairs that will be used to host your file system. A single-AZ file system can be hosted on up to 6 such pairs; a multi-AZ file system must reside on a single pair. To learn more about these options visit New – Scale-out file systems for Amazon FSx for NetApp ONTAP . After making my selections for Network & security , Encryption , and Default storage virtual machine configuration , I select the FlexGroup volume style, assign a name to the initial volume, and either accept the recommended number of constituents or specify it myself: On the next page I review my choices and click Create file system : The creation process is a good time for a lunch break. When I return the initial volume ( Vol1 ) of my file system is ready to use.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='36b845c3-6f9d-462f-9ea4-52d598c32283', embedding=None, metadata={'title': 'FlexGroup Volume Management for Amazon FSx for NetApp ONTAP is now available', 'summary': 'You can now create, manage, and back up your Amazon FSx for NetApp ONTAP FlexGroup volumes using the AWS Management Console, the Amazon FSx CLI, and the AWS SDK.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='I can create additional FlexVol or FlexGroup volumes as needed: Things to Know Here are a couple of things to keep in mind about FlexGroup volumes: Constituents – Although each FlexGroup volume can have as many as 200 constituents, we recommend 8 per HA pair. Given the 300 TiB per-constituent size limit, this allows you to create volumes with up to 2.4 PiB of storage per HA pair. ONTAP will balance your files across constituents automatically. File Counts – If you are using NFSv3 and expect to store many billions of files on a FlexGroup volume, be sure to enable 64-bit identifiers on the storage virtual machine associated with the file system. Backups – Starting today you can also create backups of FlexGroup volumes, giving you the same fully-managed built-in options that you already have for FlexVol volumes. NetApp System Manager – You can use the ONTAP CLI and the browser-based NetApp System Manager to perform advanced operations on your ONTAP file systems, storage virtual machines, and volumes. The management endpoint and administrator credentials are available on the File system details page: Regions – Both volume styles are available in all AWS Regions where Amazon FSx for NetApp ONTAP is supported. Pricing – You pay for the SSD storage, SSD IOPS, and throughput capacity that you provision, with separate charges for capacity pool usage, backups, and SnapLock licensing; see the Amazon FSx for NetApp ONTAP Pricing page to learn more. — Jeff ;', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e55b4020-51ff-4473-81e8-05de65804064', embedding=None, metadata={'title': 'Reserve quantum computers, get guidance and cutting-edge capabilities with Amazon Braket Direct', 'summary': 'This program gets you dedicated, private access to the full capacity of various quantum processing units (QPUs) without any queues or wait times, and more.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Today, we are announcing the availability of Braket Direct , a new Amazon Braket program that helps quantum researchers dive deeper into quantum computing. This program lets you get dedicated, private access to the full capacity of various quantum processing units (QPUs) without any queues or wait times, connect with quantum computing specialists to receive expert guidance for your workloads, and get early access to features and devices with limited availability to conduct cutting-edge research on today’s noisy quantum devices. Since its launch in 2020 , Amazon Braket has democratized access to quantum computing by offering on-demand access to various QPUs using shared, public availability windows, where you only pay for the duration of your reservation. You can now use Braket Direct to reserve the entire dedicated machine for a period of time on IonQ Aria, QuEra Aquila, and Rigetti Aspen-M-3 devices for running your most complex, long-running, time-sensitive workloads, or conducting live events such as training workshops and hackathons, where you pay only for what you reserve. To further your research, you can now engage directly with Braket’s experts through free office hours or one-on-one, hands-on reservation prep sessions. For deeper research collaborations, you can connect with specialists from quantum hardware providers such as IonQ , Oxford Quantum Circuits , QuEra , Rigetti , or Amazon Quantum Solutions Lab , our dedicated professional services team. Finally, to truly push the boundaries, you can gain access to experimental capabilities that have limited or reduced availability starting with IonQ’s highest fidelity, 30-qubit Forte device . Braket Direct expands on our commitment to accelerate research and innovation in quantum computing without requiring any upfront fees or long-term commitments. Getting started with Braket Direct To get started, go to the Amazon Braket console and choose Braket Direct in the left pane. You can see new features such as quantum hardware reservation, expert advice and get access to next-generation quantum hardware and features. 1. Request a quantum hardware reservation To create a reservation, choose Reserve device and select the Device that you would like to reserve. Provide your contact information, including your name and email address, any details about the workload that you would like to execute using your reservation, such as desired reservation length, relevant constraints, and desired schedule. Braket Direct assures that you have the full capacity of the QPU during your reservation and the predictability that your workloads will execute when your reservation begins.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='b9ed108d-3b34-4313-babf-09da6e53a475', embedding=None, metadata={'title': 'Reserve quantum computers, get guidance and cutting-edge capabilities with Amazon Braket Direct', 'summary': 'This program gets you dedicated, private access to the full capacity of various quantum processing units (QPUs) without any queues or wait times, and more.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='If you are interested in connecting with a Braket expert for a one-on-one reservation prep session after your reservation is confirmed, you can select that option at no additional cost. Choose Submit to complete your reservation request. A Braket team member will email you within 2–3 business days, pending request verification. To make the most of your reservation, you can choose to pre-create your tasks and jobs prior to a reservation to maximize use of the time. To learn more about your quantum tasks and hybrid jobs to execute in a device reservation, see Get started with Braket Direct in the AWS documentation. 2. Get support from quantum computing experts You can get in touch with quantum experts and get advice about your workload. With Braket office hours, Braket experts can help you go from ideation to execution faster at no additional cost. Explore your device to fit your use case, identify options to make best use of Braket for your algorithm, and get recommendations on how to use certain Braket features like Hybrid Jobs , Braket Pulse , or Analog Hamiltonian Simulation . To book an upcoming Braket office hours slot, choose Sign up and fill out your contact information, workload details, and any desired discussion topics. You will receive a calendar invitation to the next available slot by email. To take advantage of experts from quantum hardware providers, choose Connect and browse their professional services listings on AWS Marketplace . The Amazon Quantum Solutions Lab is a collaborative research and professional services team staffed with quantum computing experts who can assist you in more effectively exploring quantum computing, engaging in quantum research, and assessing the current performance of this technology. To contact the Quantum Solutions Lab, select Connect and fill out contact information and use case details. The team will email you with next steps. 3. Access to cutting-edge capabilities To move your research quicker, you can get early access to innovative new capabilities. With Braket Direct, you can easily request access to cutting-edge capabilities, such as new quantum devices with limited availability, directly in the Braket console. Today, you can get reservation-only access to IonQ’s highest-fidelity Forte QPU. Due to its limited availability, this device is currently only available through Braket Direct reservations. Now available Braket Direct is now generally available in all AWS Regions where Amazon Braket is available. To learn more, see the Braket Direct page and pricing page .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='da485aa5-fefe-4d15-81ee-1097bb289b61', embedding=None, metadata={'title': 'Reserve quantum computers, get guidance and cutting-edge capabilities with Amazon Braket Direct', 'summary': 'This program gets you dedicated, private access to the full capacity of various quantum processing units (QPUs) without any queues or wait times, and more.'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Give it a try and send feedback to AWS re:Post for Amazon Braket , Quantum Computing Stack Exchange , or through your usual AWS Support contacts. — Channy', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "documents = []\n",
    "\n",
    "for announcement in announcements:\n",
    "    for chunk in announcement[\"chunks\"]:\n",
    "        documents.append(Document(\n",
    "            text=chunk,\n",
    "            metadata={\"title\": announcement[\"title\"], \"summary\": announcement[\"summary\"]},\n",
    "        ))\n",
    "\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# create client and a new collection\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "chroma_collection = chroma_client.create_collection(\n",
    "    name=\"awsreinvent2023_announcements_collection\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"} # l2 is the default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 235/235 [00:00<00:00, 751.97it/s]\n",
      "Generating embeddings: 100%|██████████| 235/235 [01:14<00:00,  3.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# define embedding function\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# set up ChromaVectorStore and load in data\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, show_progress=True, embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8621403134099135 {'title': 'Amazon DynamoDB zero-ETL integration with Amazon OpenSearch Service is now available', 'summary': 'This capability lets you perform a search on your DynamoDB data by automatically replicating and transforming it without custom code or infrastructure.'} To learn more, see DynamoDB zero-ETL integration with Amazon OpenSearch Service and Using an OpenSearch Ingestion pipeline with Amazon DynamoDB in the AWS documentation. Give it a try and send feedback to AWS re:Post for Amazon OpenSearch Service or through your usual AWS Support contacts. — Channy\n",
      "0.8579389615031549 {'title': 'Amazon DynamoDB zero-ETL integration with Amazon OpenSearch Service is now available', 'summary': 'This capability lets you perform a search on your DynamoDB data by automatically replicating and transforming it without custom code or infrastructure.'} Today, we are announcing the general availability of Amazon DynamoDB zero-ETL integration with Amazon OpenSearch Service , which lets you perform a search on your DynamoDB data by automatically replicating and transforming it without custom code or infrastructure. This zero-ETL integration reduces the operational burden and cost involved in writing code for a data pipeline architecture, keeping the data in sync, and updating code with frequent application changes, enabling you to focus on your application. With this zero-ETL integration, Amazon DynamoDB customers can now use the powerful search features of Amazon OpenSearch Service , such as full-text search, fuzzy search, auto-complete, and vector search for machine learning (ML) capabilities to offer new experiences that boost user engagement and improve satisfaction with their applications. This zero-ETL integration uses Amazon OpenSearch Ingestion to synchronize the data between Amazon DynamoDB and Amazon OpenSearch Service. You choose the DynamoDB table whose data needs to be synchronized and Amazon OpenSearch Ingestion synchronizes the data to an Amazon OpenSearch managed cluster or serverless collection within seconds of it being available. You can also specify index mapping templates to ensure that your Amazon DynamoDB fields are mapped to the correct fields in your Amazon OpenSearch Service indexes. Also, you can synchronize data from multiple DynamoDB tables into one Amazon OpenSearch Service managed cluster or serverless collection to oﬀer holistic insights across several applications. Getting started with this zero-ETL integration With a few clicks, you can synchronize data from DynamoDB to OpenSearch Service. To create an integration between DynamoDB and OpenSearch Service, choose the Integrations menu in the left pane of the DynamoDB console and the DynamoDB table whose data you want to synchronize. You must turn on point-in-time recovery (PITR) and the DynamoDB Streams feature. This feature allows you to capture item-level changes in your table and push the changes to a stream. Choose Turn on for PITR and enable DynamoDB Streams in the Exports and streams tab. After turning on PITR and DynamoDB Stream, choose Create to set up an OpenSearch Ingestion pipeline in your account that replicates the data to an OpenSearch Service managed domain. In the first step, enter a unique pipeline name and set up pipeline capacity and compute resources to automatically scale your pipeline based on the current ingestion workload. Now you can configure the pre-defined pipeline configuration in YAML file format.\n",
      "0.8566071576728159 {'title': 'Announcing Amazon DynamoDB zero-ETL integration with Amazon Redshift (Preview)', 'summary': 'This enables customers to run high performance analytics on their DynamoDB data.'} AWS announces Amazon DynamoDB zero-ETL integration with Amazon Redshift Posted On: Nov 28, 2023 Amazon DynamoDB now supports zero-ETL integration with Amazon Redshift, enabling customers to run high performance analytics on their DynamoDB data. This zero-ETL integration has no impact on production workloads running on DynamoDB. As data is written into a DynamoDB table, it is seamlessly made available in Amazon Redshift, eliminating the need for customers to build and maintain complex data pipelines for performing extract, transform, and load (ETL) operations. The DynamoDB zero-ETL integration with Amazon Redshift helps you derive holistic insights across many applications, break data silos in your organization, and gain significant cost savings and operational efficiencies. Now you can run enhanced analysis on your DynamoDB data with the rich capabilities of Amazon Redshift, such as high performance SQL, built-in ML and Spark integrations, materialized views, data sharing, and ability to join data across multiple data stores and data lakes. Amazon DynamoDB zero-ETL integration with Amazon Redshift is now available in limited preview in the US East (Ohio) region. To request access to the limited preview, visit the Preview Page . To learn more about each service, visit the Amazon DynamoDB or Amazon Redshift webpages. »\n"
     ]
    }
   ],
   "source": [
    "# Query Data\n",
    "retriever = index.as_retriever(similarity_top_k=3)\n",
    "nodes = retriever.retrieve(\"List some announcements about dynamodb\")\n",
    "for node in nodes:\n",
    "    print(node.get_score(), node.metadata, node.get_text())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
